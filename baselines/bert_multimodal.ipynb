{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHwHkm4rGCLK"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFg6mNX7Gdxg"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQU-d0knGyhl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import warnings\n",
        "import logging\n",
        "import gc\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import ast\n",
        "from tqdm import tqdm\n",
        "from typing import Optional\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import jaccard_score, f1_score, accuracy_score, recall_score, precision_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suRa-hBHG_21"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_Ovlo42G4Kh"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "# from rouge_score.rouge_scorer import RougeScorer\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from transformers import (\n",
        "    BartTokenizerFast,\n",
        "    AdamW\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "    print(\"Using GPU\")\n",
        "\n",
        "else:\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "foldNum = 0\n",
        "\n",
        "\n",
        "\n",
        "SOURCE_MAX_LEN = 500\n",
        "# TARGET_MAX_LEN = 50\n",
        "# MAX_UTTERANCES = 25\n",
        "\n",
        "ACOUSTIC_DIM = 768\n",
        "ACOUSTIC_MAX_LEN = 1000\n",
        "\n",
        "\n",
        "\n",
        "VISUAL_DIM = 2048\n",
        "VISUAL_MAX_LEN = 480\n",
        "\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "LEARNING_RATE = 1e-4\n",
        "# LEARNING_RATE = 1e-5\n",
        "# LEARNING_RATE = 1e-3\n",
        "\n",
        "\n",
        "# VALID_LEN = 69\n",
        "\n",
        "# BASE_LEARNING_RATE = 5e-6\n",
        "# NEW_LEARNING_RATE = 5e-5\n",
        "# WEIGHT_DECAY = 1e-4\n",
        "\n",
        "# NUM_BEAMS = 5\n",
        "# EARLY_STOPPING = True\n",
        "# NO_REPEAT_NGRAM_SIZE = 3\n",
        "\n",
        "# EARLY_STOPPING_THRESHOLD = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMdRaTfAHF7Q"
      },
      "outputs": [],
      "source": [
        "def set_random_seed(seed: int):\n",
        "    print(\"Seed : {}\".format(seed))\n",
        "\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.enabled = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# set_random_seed(42)\n",
        "# set_random_seed(123)\n",
        "# set_random_seed(12345)\n",
        "set_random_seed(994)\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "\n",
        "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\n",
        "\n",
        "from transformers.modeling_utils import PreTrainedModel, unwrap_model\n",
        "\n",
        "# from transformers.models.roberta.configuration_roberta import RobertaConfig\n",
        "# # from transformers.models.roberta.tokenization_roberta import RobertaTokenizer\n",
        "# from transformers.models.roberta.modeling_roberta import RobertaLayer, RobertaEmbeddings, RobertaPooler, RobertaPreTrainedModel\n",
        "# # from transformers.models.roberta.modeling_roberta import RobertaEmbeddings, RobertaPreTrainedModel\n",
        "# from transformers import RobertaTokenizer, RobertaModel\n",
        "# from transformers.modeling_outputs import (\n",
        "#     BaseModelOutputWithPastAndCrossAttentions,\n",
        "#     BaseModelOutputWithPoolingAndCrossAttentions\n",
        "# )\n",
        "\n",
        "from transformers.models.bert.configuration_bert import BertConfig\n",
        "\n",
        "from transformers.models.bert.modeling_bert import BertLayer, BertEmbeddings, BertPooler, BertPreTrainedModel\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "from transformers.modeling_outputs import (\n",
        "    BaseModelOutputWithPastAndCrossAttentions,\n",
        "    BaseModelOutputWithPoolingAndCrossAttentions\n",
        ")\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers.pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n",
        "from transformers.activations import ACT2FN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMtoMDKiVk0r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuNh3gFEHQhY"
      },
      "outputs": [],
      "source": [
        "# from transformer_encoder import TransformerEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ML9b3x4axdj"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtwuoBrcwzUN"
      },
      "outputs": [],
      "source": [
        "# bert_tokenizer  = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "# bert_model = BertModel.from_pretrained(\"bert-base-cased\")\n",
        "# # bert_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1XR3BlgHOY8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEDR_7N6HWsu"
      },
      "outputs": [],
      "source": [
        "# class ContextAwareAttention(nn.Module):\n",
        "\n",
        "#     def __init__(self,\n",
        "#                  dim_model : int,\n",
        "#                  dim_context : int,\n",
        "#                  dropout_rate : Optional[float] = 0.0 ):\n",
        "\n",
        "#         super(ContextAwareAttention, self).__init__()\n",
        "\n",
        "#         self.dim_model = dim_model\n",
        "#         self.dim_context = dim_context\n",
        "#         self.dropout_rate = dropout_rate\n",
        "#         self.attention_layer = nn.MultiheadAttention(embed_dim=self.dim_model,\n",
        "#                                                      num_heads = 1,\n",
        "#                                                      dropout = self.dropout_rate,\n",
        "#                                                      bias = True,\n",
        "#                                                     add_zero_attn=False,\n",
        "#                                                     batch_first=True,\n",
        "#                                                     device=DEVICE\n",
        "#         )\n",
        "\n",
        "#         self.u_k = nn.Linear(self.dim_context, self.dim_model, bias = False)\n",
        "#         self.w1_k = nn.Linear(self.dim_model, 1, bias=False)\n",
        "#         self.w2_k = nn.Linear(self.dim_model, 1, bias=False)\n",
        "\n",
        "#         self.u_v = nn.Linear(self.dim_context, self.dim_model, bias=False)\n",
        "#         self.w1_v = nn.Linear(self.dim_model, 1, bias = False)\n",
        "#         self.w2_v = nn.Linear(self.dim_model, 1, bias = False)\n",
        "\n",
        "#     def forward(self, q, k, v, context):\n",
        "\n",
        "#         # print(\"Context shape : \", context.shape)\n",
        "#         # print(\"Dim context : \", self.dim_context, \" : Dim model : \", self.dim_model)\n",
        "#         key_context = self.u_k(context)\n",
        "#         # print(\"Context shape below key context : \", key_context.shape)\n",
        "#         value_context = self.u_v(context)\n",
        "\n",
        "#         lambda_k = F.sigmoid(self.w1_k(k) + self.w2_k(key_context))\n",
        "#         lambda_v = F.sigmoid(self.w1_v(v) + self.w2_v(value_context))\n",
        "\n",
        "#         k_cap = (1-lambda_k) * k + (lambda_k) * key_context\n",
        "#         v_cap = (1-lambda_v) * v + (lambda_v) * value_context\n",
        "\n",
        "#         attention_output, _ = self.attention_layer(query = q,\n",
        "#                                                    key = k_cap,\n",
        "#                                                    value = v_cap)\n",
        "\n",
        "#         return attention_output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZPn4U3Weq9I"
      },
      "outputs": [],
      "source": [
        "# class MAF_acoustic(nn.Module):\n",
        "#     def __init__(self,\n",
        "#                 dim_model,\n",
        "#                 dropout_rate):\n",
        "#         super(MAF_acoustic, self).__init__()\n",
        "#         self.dropout_rate = dropout_rate\n",
        "\n",
        "#         self.acoustic_context_transform = nn.Linear(ACOUSTIC_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "#         # self.visual_context_transform = nn.Linear(VISUAL_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "\n",
        "#         self.acoustic_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "#                                                                 dim_context=ACOUSTIC_DIM,\n",
        "#                                                                 dropout_rate=dropout_rate)\n",
        "\n",
        "#         # self.visual_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "#         #                                                     dim_context=VISUAL_DIM,\n",
        "#         #                                                     dropout_rate=dropout_rate)\n",
        "\n",
        "#         self.acoustic_gate = nn.Linear(2*dim_model, dim_model)\n",
        "#         # self.visual_gate = nn.Linear(2*dim_model, dim_model)\n",
        "#         self.dropout_layer = nn.Dropout(dropout_rate)\n",
        "#         self.final_layer_norm = nn.LayerNorm(dim_model)\n",
        "\n",
        "#     def forward(self,\n",
        "#                 text_input,\n",
        "#                 acoustic_context):\n",
        "\n",
        "#         # print(\"Acoustic context shape (A) : \", acoustic_context.shape)\n",
        "\n",
        "#         acoustic_context = acoustic_context.permute(0,2,1)\n",
        "#         acoustic_context = self.acoustic_context_transform(acoustic_context.float())\n",
        "#         acoustic_context = acoustic_context.permute(0,2,1)\n",
        "\n",
        "#         audio_out = self.acoustic_context_attention(q=text_input,\n",
        "#                                                     k=text_input,\n",
        "#                                                     v=text_input,\n",
        "#                                                     context=acoustic_context)\n",
        "#         # print(\"Audio out (A) : \", audio_out.shape)\n",
        "\n",
        "#         # print(\"Visual context shape : \", visual_context.shape)\n",
        "#         # visual_context = visual_context.permute(0,2,1)\n",
        "#         # visual_context = self.visual_context_transform(visual_context.float())\n",
        "#         # visual_context = visual_context.permute(0,2,1)\n",
        "\n",
        "#         # video_out = self.visual_context_attention(q=text_input,\n",
        "#         #                                             k=text_input,\n",
        "#         #                                             v=text_input,\n",
        "#         #                                             context=visual_context)\n",
        "\n",
        "#         # print(\"Video out shape : \", video_out.shape)\n",
        "#         # print(\"Text input shape : \", text_input.shape)\n",
        "#         weight_a = F.sigmoid(self.acoustic_gate(torch.cat([text_input, audio_out], dim=-1)))\n",
        "#         # weight_v = F.sigmoid(self.visual_gate(torch.cat([text_input, video_out], dim=-1)))\n",
        "\n",
        "#         # output = self.final_layer_norm(text_input + weight_a * audio_out + weight_v * video_out)\n",
        "\n",
        "#         output = self.final_layer_norm(text_input + weight_a * audio_out)\n",
        "\n",
        "#         return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeZdIlcker1V"
      },
      "outputs": [],
      "source": [
        "# class MAF_visual(nn.Module):\n",
        "#     def __init__(self,\n",
        "#                 dim_model,\n",
        "#                 dropout_rate):\n",
        "#         super(MAF_visual, self).__init__()\n",
        "#         self.dropout_rate = dropout_rate\n",
        "\n",
        "#         # self.acoustic_context_transform = nn.Linear(ACOUSTIC_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "#         self.visual_context_transform = nn.Linear(VISUAL_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "\n",
        "#         # self.acoustic_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "#         #                                                         dim_context=ACOUSTIC_DIM,\n",
        "#         #                                                         dropout_rate=dropout_rate)\n",
        "\n",
        "#         self.visual_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "#                                                             dim_context=VISUAL_DIM,\n",
        "#                                                             dropout_rate=dropout_rate)\n",
        "\n",
        "#         # self.acoustic_gate = nn.Linear(2*dim_model, dim_model)\n",
        "#         self.visual_gate = nn.Linear(2*dim_model, dim_model)\n",
        "#         self.dropout_layer = nn.Dropout(dropout_rate)\n",
        "#         self.final_layer_norm = nn.LayerNorm(dim_model)\n",
        "\n",
        "#     def forward(self,\n",
        "#                 text_input,\n",
        "#                 visual_context):\n",
        "\n",
        "#         # print(\"Acoustic context shape (A) : \", acoustic_context.shape)\n",
        "\n",
        "#         # acoustic_context = acoustic_context.permute(0,2,1)\n",
        "#         # acoustic_context = self.acoustic_context_transform(acoustic_context.float())\n",
        "#         # acoustic_context = acoustic_context.permute(0,2,1)\n",
        "\n",
        "#         # audio_out = self.acoustic_context_attention(q=text_input,\n",
        "#         #                                             k=text_input,\n",
        "#         #                                             v=text_input,\n",
        "#         #                                             context=acoustic_context)\n",
        "#         # print(\"Audio out (A) : \", audio_out.shape)\n",
        "\n",
        "#         # print(\"Visual context shape : \", visual_context.shape)\n",
        "#         visual_context = visual_context.permute(0,2,1)\n",
        "#         visual_context = self.visual_context_transform(visual_context.float())\n",
        "#         visual_context = visual_context.permute(0,2,1)\n",
        "\n",
        "#         video_out = self.visual_context_attention(q=text_input,\n",
        "#                                                     k=text_input,\n",
        "#                                                     v=text_input,\n",
        "#                                                     context=visual_context)\n",
        "\n",
        "#         # print(\"Video out shape : \", video_out.shape)\n",
        "#         # print(\"Text input shape : \", text_input.shape)\n",
        "#         # weight_a = F.sigmoid(self.acoustic_gate(torch.cat([text_input, audio_out], dim=-1)))\n",
        "#         weight_v = F.sigmoid(self.visual_gate(torch.cat([text_input, video_out], dim=-1)))\n",
        "\n",
        "#         # output = self.final_layer_norm(text_input + weight_a * audio_out + weight_v * video_out)\n",
        "\n",
        "#         # q3 = weight_v * video_out\n",
        "#         # print('weight_v shape : ', weight_v.shape)\n",
        "#         # print('weight_v * video_out shape : ', q3.shape)\n",
        "\n",
        "#         output = self.final_layer_norm(text_input  + weight_v * video_out)\n",
        "\n",
        "#         return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s88m19VoHbj5"
      },
      "outputs": [],
      "source": [
        "class MultimodalBertEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "        self.gradient_checkpointing = False\n",
        "\n",
        "        self.fusion_at_layer9 = [8]\n",
        "        # self.fusion_at_layer10 = [11]\n",
        "\n",
        "        # self.fusion_at_layer9 = [19]\n",
        "        # self.fusion_at_layer10 = [20]\n",
        "\n",
        "\n",
        "        self.acoustic_context_transform = nn.Linear(ACOUSTIC_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "        self.visual_context_transform = nn.Linear(VISUAL_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "\n",
        "        self.acoustic_dim = nn.Linear(ACOUSTIC_DIM, 768, bias = False)\n",
        "        self.visual_dim = nn.Linear(VISUAL_DIM, 768, bias = False)\n",
        "\n",
        "        self.concat_linear = nn.Linear(3*768, 768, bias = False)\n",
        "\n",
        "        # self.MAF_layer9 = MAF_acoustic(dim_model=self.config.hidden_size,\n",
        "        #                      dropout_rate=0.2)\n",
        "\n",
        "        # self.MAF_layer10 = MAF_visual(dim_model=self.config.hidden_size,\n",
        "        #                      dropout_rate=0.2)\n",
        "\n",
        "    def forward(self,\n",
        "               hidden_states : torch.Tensor,\n",
        "               attention_mask: Optional[torch.FloatTensor] = None,\n",
        "               acoustic_input: Optional[torch.FloatTensor] = None,\n",
        "               visual_input: Optional[torch.FloatTensor] = None,\n",
        "               head_mask: Optional[torch.FloatTensor] = None,\n",
        "               encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
        "               encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "               past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "               use_cache: Optional[bool] = False,\n",
        "               output_attentions: Optional[bool] = False,\n",
        "               output_hidden_states: Optional[bool] = False,\n",
        "               return_dict: Optional[bool] = True\n",
        "               ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n",
        "\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
        "\n",
        "        next_decoder_cache = () if use_cache else None\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            # print(\"i : \", i)\n",
        "            if i in self.fusion_at_layer9:\n",
        "                    # print(\"Inside layer 9\")\n",
        "                    # print(\"Acoustic input shape (B) : \", acoustic_input)\n",
        "                    # acoustic_input = self.acoustic_transformer(acoustic_input)[-1]\n",
        "                    # print(\"Acoustic input shape (C) : \", acoustic_input)\n",
        "\n",
        "                    # visual_input = self.visual_transformer(visual_input)[-1]\n",
        "                    # print(\"====Idx inside fusion at layer :\", idx)\n",
        "\n",
        "                      acoustic_input = acoustic_input.permute(0,2,1)\n",
        "                      acoustic_input = self.acoustic_context_transform(acoustic_input.float())\n",
        "                      acoustic_input = acoustic_input.permute(0,2,1)\n",
        "\n",
        "                      acoustic_input = self.acoustic_dim(acoustic_input)\n",
        "\n",
        "                      # print(\"acoustic_input shape : \", acoustic_input.shape)\n",
        "\n",
        "                      visual_input = visual_input.permute(0,2,1)\n",
        "                      visual_input = self.visual_context_transform(visual_input.float())\n",
        "                      visual_input = visual_input.permute(0,2,1)\n",
        "\n",
        "                      visual_input = self.visual_dim(visual_input)\n",
        "\n",
        "                      # print(\"visual input shape : \", visual_input.shape)\n",
        "                      concat = torch.concat([hidden_states, acoustic_input, visual_input], dim = -1)\n",
        "\n",
        "                      # print(\"concat shape : \", concat.shape)\n",
        "\n",
        "                      hidden_states = self.concat_linear(concat)\n",
        "\n",
        "                      # print('hidden states shape : ', hidden_states.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # if i in self.fusion_at_layer10:\n",
        "            #         # print(\"Acoustic input shape (B) : \", acoustic_input)\n",
        "            #         # acoustic_input = self.acoustic_transformer(acoustic_input)[-1]\n",
        "            #         # print(\"Acoustic input shape (C) : \", acoustic_input)\n",
        "\n",
        "            #         # visual_input = self.visual_transformer(visual_input)[-1]\n",
        "            #         # print(\"====Idx inside fusion at layer :\", idx)\n",
        "            #         hidden_states = self.MAF_layer10(text_input = hidden_states,\n",
        "            #                                        visual_context = visual_input)\n",
        "\n",
        "\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
        "\n",
        "            if self.gradient_checkpointing and self.training:\n",
        "\n",
        "                if use_cache:\n",
        "                    # logger.warning(\n",
        "                    #     \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
        "                    # )\n",
        "                    use_cache = False\n",
        "\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        return module(*inputs, past_key_value, output_attentions)\n",
        "\n",
        "                    return custom_forward\n",
        "\n",
        "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(layer_module),\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                # print(\"hidden states shape : \", hidden_states.shape)\n",
        "                # print('attention_mask shape : ', attention_mask.shape)\n",
        "                layer_outputs = layer_module(\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                    past_key_value,\n",
        "                    output_attentions,\n",
        "\n",
        "\n",
        "                )\n",
        "\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if use_cache:\n",
        "                next_decoder_cache += (layer_outputs[-1],)\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "                if self.config.add_cross_attention:\n",
        "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(\n",
        "                v\n",
        "                for v in [\n",
        "                    hidden_states,\n",
        "                    next_decoder_cache,\n",
        "                    all_hidden_states,\n",
        "                    all_self_attentions,\n",
        "                    all_cross_attentions,\n",
        "\n",
        "                ]\n",
        "                if v is not None\n",
        "            )\n",
        "\n",
        "        return BaseModelOutputWithPastAndCrossAttentions(\n",
        "            last_hidden_state = hidden_states,\n",
        "            past_key_values = next_decoder_cache,\n",
        "            hidden_states = all_hidden_states,\n",
        "            attentions = all_self_attentions,\n",
        "            cross_attentions = all_cross_attentions\n",
        "\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6F_nfly0HgKR"
      },
      "outputs": [],
      "source": [
        "class MultiModalBertClassification(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(input_dim, num_classes)\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = torch.relu(hidden_states)\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        return hidden_states\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es1L0jZkUFTj"
      },
      "outputs": [],
      "source": [
        "class MultiModalBertModel(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        self.embeddings = BertEmbeddings(config)\n",
        "        self.encoder = MultimodalBertEncoder(config)\n",
        "        # self.encoder = BertEncoder(config)\n",
        "\n",
        "        self.output = MultiModalBertClassification(config.hidden_size, num_classes = 2)\n",
        "\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embeddings.word_embeddings\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embeddings.word_embeddings = value\n",
        "\n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "\n",
        "        for layer, heads in heads_to_prune.items():\n",
        "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        acoustic_input: Optional[torch.Tensor] = None,\n",
        "        visual_input: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        token_type_ids: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.Tensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
        "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None\n",
        "    )   -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.output_hidden_states\n",
        "\n",
        "        if self.config.is_decoder:\n",
        "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        else:\n",
        "            use_cache = False\n",
        "\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError('You can not  specify both input_ids and input_embeds at the same time')\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError('You have to specify either input_ids or inputs_embeds')\n",
        "\n",
        "        batch_size, seq_length = input_shape\n",
        "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "\n",
        "        past_key_value_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(((batch_size, seq_length + past_key_value_length)), device = device)\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self.embeddings, 'token_type_ids'):\n",
        "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(input_shape, dtype = torch.long, device = device)\n",
        "\n",
        "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n",
        "\n",
        "        # print(\"attention mask shape : \", attention_mask.shape)\n",
        "        # print(\"extended attention mask shape : \", extended_attention_mask.shape)\n",
        "\n",
        "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
        "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
        "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
        "            if encoder_attention_mask is None:\n",
        "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device = device)\n",
        "\n",
        "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
        "\n",
        "        else:\n",
        "            encoder_extended_attention_mask = None\n",
        "\n",
        "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
        "\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids = input_ids,\n",
        "            position_ids = position_ids,\n",
        "            token_type_ids = token_type_ids,\n",
        "            inputs_embeds = inputs_embeds,\n",
        "            past_key_values_length = past_key_value_length\n",
        "        )\n",
        "\n",
        "        # print(\"attention mask shape 2 : \", attention_mask.shape)\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            attention_mask = extended_attention_mask,\n",
        "            acoustic_input = acoustic_input,\n",
        "            visual_input = visual_input,\n",
        "            head_mask = head_mask,\n",
        "            encoder_hidden_states = encoder_hidden_states,\n",
        "            encoder_attention_mask = encoder_extended_attention_mask,\n",
        "            past_key_values = past_key_values,\n",
        "            use_cache = use_cache,\n",
        "            output_attentions = output_attentions,\n",
        "            output_hidden_states = output_hidden_states,\n",
        "            return_dict = return_dict\n",
        "        )\n",
        "\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.output(sequence_output)\n",
        "\n",
        "        loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        # print(\"pooled output shape : \", pooled_output.shape)\n",
        "        # print(\"labels shape : \", labels.shape)\n",
        "        pooled_output = pooled_output[:, 0, :]\n",
        "        loss = loss_fn(pooled_output, labels)\n",
        "\n",
        "        temp_dict = {}\n",
        "\n",
        "        temp_dict['logits'] = pooled_output\n",
        "        temp_dict['loss'] = loss\n",
        "\n",
        "        return temp_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4lcNczUHpOH"
      },
      "outputs": [],
      "source": [
        "# def audio_video_broadcast(x):\n",
        "# #     z = torch.empty()\n",
        "#     temp_all = torch.Tensor()\n",
        "#     for j in range(x.shape[0]):\n",
        "#         print(\"j : \", j)\n",
        "#         temp_x = x[j,:]\n",
        "# #         print(\"Temp x shape : \", temp_x.shape)\n",
        "#         temp_x = torch.tensor(temp_x, dtype=torch.float)\n",
        "#         temp_x = torch.broadcast_to(temp_x, (SOURCE_MAX_LEN, temp_x.shape[0]))\n",
        "# #         print(\"Temp x shape : \", temp_x.shape)\n",
        "#         temp_x = temp_x.unsqueeze(0)\n",
        "# #         print(\"Temp x shape : \", temp_x.shape)\n",
        "\n",
        "#         if(j==0):\n",
        "#             temp_all = temp_x\n",
        "#         else:\n",
        "#             temp_all = torch.cat([temp_all, temp_x], dim = 0)\n",
        "\n",
        "\n",
        "#     return temp_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inWKX8K6GV50"
      },
      "outputs": [],
      "source": [
        "foldNum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RunFr5b0Ni1u"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/32/train_audio_fold_'+str(foldNum)+'.p', 'rb') as f:\n",
        "  train_audio_data_utterance1 = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGFk4HCwOo-A"
      },
      "outputs": [],
      "source": [
        "type(train_audio_data_utterance1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sK4GbT8ZOq2W"
      },
      "outputs": [],
      "source": [
        "len(train_audio_data_utterance1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X73Hp5HxOsEl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yM1zXLalOifj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvBQbRyFN5zu"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/32/test_audio_fold_'+str(foldNum)+'.p', 'rb') as f:\n",
        "  test_audio_data_utterance1 = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZctmFGmNQsMN"
      },
      "outputs": [],
      "source": [
        "len(test_audio_data_utterance1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz2xdgV7PzbN"
      },
      "outputs": [],
      "source": [
        "len(test_audio_data_utterance1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yw0sc0aaNzOO"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/32/train_video_fold_'+str(foldNum)+'.p', 'rb') as f:\n",
        "  train_image_data_utterance1 = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yT8QM77QvJS"
      },
      "outputs": [],
      "source": [
        "len(train_image_data_utterance1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUEvmMBNOPFE"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/32/test_video_fold_'+str(foldNum)+'.p', 'rb') as f:\n",
        "  test_image_data_utterance1 = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm6Z6Fc0QxSW"
      },
      "outputs": [],
      "source": [
        "len(test_image_data_utterance1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIEMeHJ6RoXS"
      },
      "outputs": [],
      "source": [
        "tp = torch.ones(4,5)\n",
        "tp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8vsmq4FS48j"
      },
      "outputs": [],
      "source": [
        "torch.zeros(5 - tp.shape[0], 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CNdKCeQRrN8"
      },
      "outputs": [],
      "source": [
        "torch.cat([tp, torch.zeros(5 - tp.shape[0], 5)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO9elV8EQAi1"
      },
      "outputs": [],
      "source": [
        "def pad_seq(tensor, dim, max_len):\n",
        "  if max_len > tensor.shape[0] :\n",
        "    return torch.cat([tensor, torch.zeros(max_len - tensor.shape[0], dim)])\n",
        "  else:\n",
        "    return tensor[:max_len]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmBt6xZ1TElw"
      },
      "outputs": [],
      "source": [
        "ACOUSTIC_DIM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cUPe2LxQ04S"
      },
      "outputs": [],
      "source": [
        "train_audio_data_utterance1 = torch.stack([pad_seq(torch.tensor(a, dtype = torch.float),\n",
        "                                                   dim = ACOUSTIC_DIM,\n",
        "                                                   max_len = ACOUSTIC_MAX_LEN)\n",
        "                                                  for a in train_audio_data_utterance1], 0)\n",
        "train_audio_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6-frFIpTzrW"
      },
      "outputs": [],
      "source": [
        "test_audio_data_utterance1 = torch.stack([pad_seq(torch.tensor(a, dtype = torch.float),\n",
        "                                                   dim = ACOUSTIC_DIM,\n",
        "                                                   max_len = ACOUSTIC_MAX_LEN)\n",
        "                                                  for a in test_audio_data_utterance1], 0)\n",
        "test_audio_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEDVn1TaUicC"
      },
      "outputs": [],
      "source": [
        "VISUAL_DIM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ArX7k9VUkWO"
      },
      "outputs": [],
      "source": [
        "VISUAL_MAX_LEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PO2OLh5yUTF2"
      },
      "outputs": [],
      "source": [
        "train_image_data_utterance1 = torch.stack([pad_seq(torch.tensor(a, dtype = torch.float),\n",
        "                                                   dim = VISUAL_DIM,\n",
        "                                                   max_len = VISUAL_MAX_LEN)\n",
        "                                                  for a in train_image_data_utterance1], 0)\n",
        "train_image_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BAAQoCcUv34"
      },
      "outputs": [],
      "source": [
        "test_image_data_utterance1 = torch.stack([pad_seq(torch.tensor(a, dtype = torch.float),\n",
        "                                                   dim = VISUAL_DIM,\n",
        "                                                   max_len = VISUAL_MAX_LEN)\n",
        "                                                  for a in test_image_data_utterance1], 0)\n",
        "test_image_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTMNyPhSUdBs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3V8UO6HH0xE"
      },
      "outputs": [],
      "source": [
        "# path = \"/content/drive/MyDrive/Colab Notebooks/32/datasetTrue_original/sarcasmDataset_speaker_dependent_True.npz\"\n",
        "# data2 = np.load(path, mmap_mode=True)\n",
        "\n",
        "# train_audio_data_utterance1 = data2['feautesUA_train'][foldNum]\n",
        "# train_image_data_utterance1 = data2['feautesUV_train'][foldNum]\n",
        "\n",
        "# test_audio_data_utterance1 = data2['feautesUA_test'][foldNum]\n",
        "# test_image_data_utterance1 = data2['feautesUV_test'][foldNum]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdf5hGlVH5Zw"
      },
      "outputs": [],
      "source": [
        "# model =  MultiModalRobertaModel.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment-latest')\n",
        "# print(model)\n",
        "\n",
        "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "# print(tokenizer)\n",
        "\n",
        "# num_param = sum(p.numel() for p in model.parameters())\n",
        "# print(\"Total parameters : \", num_param/1e6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYjq5Tz4ceZa"
      },
      "outputs": [],
      "source": [
        "model = MultiModalBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "\n",
        "print(model)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "print(tokenizer)\n",
        "\n",
        "num_param = sum(p.numel() for p in model.parameters())\n",
        "print(\"Total trainanable parameters : \", num_param/1e6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7t2C-xFIQXla"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_N4HcMQ9hXX"
      },
      "outputs": [],
      "source": [
        "cnt = 0\n",
        "for name, param in model.named_parameters():\n",
        "    print(\"Count : \", cnt, \" name : \", name)\n",
        "    cnt+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feA-7jSwIBQO"
      },
      "outputs": [],
      "source": [
        "cnt = 0\n",
        "for name, param in model.named_parameters():\n",
        "\n",
        "    if(cnt>=197):\n",
        "    # if(cnt>=389):\n",
        "\n",
        "        param.requires_grad = True\n",
        "        print(\"Count : \", cnt, \" name : \", name)\n",
        "\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "    cnt+=1\n",
        "\n",
        "\n",
        "num_param = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Total trainanable parameters : \", num_param/1e6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amBD8a4_rhUM"
      },
      "outputs": [],
      "source": [
        "foldNum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1NGerbQcymT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46OM_4HZjKNp"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/Colab Notebooks/32/json_file_fold.p\", \"rb\") as f:\n",
        "    text_file = pickle.load(f)\n",
        "\n",
        "train_text = text_file[\"json_file_list_\" + str(foldNum) +\"_train\"]\n",
        "test_text =  text_file[\"json_file_list_\" + str(foldNum) +\"_test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaH0ocF_zD0O"
      },
      "outputs": [],
      "source": [
        "# Trainlen = 483"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sL8tkEu3kCrJ"
      },
      "outputs": [],
      "source": [
        "print(type(train_audio_data_utterance1))\n",
        "print(train_audio_data_utterance1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRRQRZeBNYum"
      },
      "outputs": [],
      "source": [
        "# combined_train_data = []\n",
        "# for j in range(len(train_text)):\n",
        "#   temp_text = train_text[j]\n",
        "#   temp_audio = train_audio_data_utterance1[j]\n",
        "#   temp_image = train_image_data_utterance1[j]\n",
        "\n",
        "#   temp_list = []\n",
        "#   temp_list.append(temp_text)\n",
        "#   temp_list.append(temp_audio)\n",
        "#   temp_list.append(temp_image)\n",
        "\n",
        "#   combined_train_data.append(temp_list)\n",
        "\n",
        "# random.shuffle(combined_train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGx2mZfPOWjo"
      },
      "outputs": [],
      "source": [
        "# train_text2 = []\n",
        "# train_audio_utterance2 = []\n",
        "# train_image_utterance2 = []\n",
        "\n",
        "# for j in combined_train_data:\n",
        "#   train_text2.append(j[0])\n",
        "#   train_audio_utterance2.append(j[1])\n",
        "#   train_image_utterance2.append(j[2])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qe5xfQYURWrs"
      },
      "outputs": [],
      "source": [
        "# print(len(train_text2))\n",
        "# print(len(train_audio_utterance2))\n",
        "# print(len(train_image_utterance2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPa_FWppRfpn"
      },
      "outputs": [],
      "source": [
        "# train_audio_data_utterance2 = torch.tensor(train_audio_utterance2)\n",
        "# train_audio_data_utterance2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuHaeA7JiRQz"
      },
      "outputs": [],
      "source": [
        "# train_image_data_utterance2 = torch.tensor(train_image_utterance2)\n",
        "# train_image_data_utterance2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQCW6oZTifhd"
      },
      "outputs": [],
      "source": [
        "# train_audio_data_utterance = torch.tensor(train_audio_data_utterance2)[:Trainlen]\n",
        "# # train_audio_data_utterance = train_audio_data_utterance.unsqueeze(dim = 1)\n",
        "# train_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "984ZSUXVWQfj"
      },
      "outputs": [],
      "source": [
        "train_audio_data_utterance = torch.tensor(train_audio_data_utterance1)\n",
        "# train_audio_data_utterance = train_audio_data_utterance.unsqueeze(dim = 1)\n",
        "train_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYe20QnlkJwg"
      },
      "outputs": [],
      "source": [
        "# train_audio_data_utterance = torch.tensor(train_audio_data_utterance1)[:Trainlen]\n",
        "# # train_audio_data_utterance = train_audio_data_utterance.unsqueeze(dim = 1)\n",
        "# train_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLqtw_6-irl3"
      },
      "outputs": [],
      "source": [
        "# train_image_data_utterance = torch.tensor(train_image_data_utterance2)[:Trainlen]\n",
        "# # train_image_data_utterance = train_image_data_utterance.unsqueeze(dim = 1)\n",
        "# train_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AleXnjQNWTKL"
      },
      "outputs": [],
      "source": [
        "train_image_data_utterance = torch.tensor(train_image_data_utterance1)\n",
        "# train_image_data_utterance = train_image_data_utterance.unsqueeze(dim = 1)\n",
        "train_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1NXg6uikX5P"
      },
      "outputs": [],
      "source": [
        "# train_image_data_utterance = torch.tensor(train_image_data_utterance1)[:Trainlen]\n",
        "# # train_image_data_utterance = train_image_data_utterance.unsqueeze(dim = 1)\n",
        "# train_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkC6buI-iu39"
      },
      "outputs": [],
      "source": [
        "# valid_audio_data_utterance = torch.tensor(train_audio_data_utterance2)[Trainlen:]\n",
        "# valid_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU5wt_vv998E"
      },
      "outputs": [],
      "source": [
        "len(test_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4yP2ap7Pl27"
      },
      "outputs": [],
      "source": [
        "test_image_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5rke66XPvU3"
      },
      "outputs": [],
      "source": [
        "test_audio_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyi5s9iS9jej"
      },
      "outputs": [],
      "source": [
        "# combined_test_data = []\n",
        "\n",
        "# for j in range(len(test_text)):\n",
        "#   temp_text = test_text[j]\n",
        "#   temp_audio = test_audio_data_utterance1[j]\n",
        "#   temp_image = test_image_data_utterance1[j]\n",
        "\n",
        "#   temp_list = []\n",
        "#   temp_list.append(temp_text)\n",
        "#   temp_list.append(temp_audio)\n",
        "#   temp_list.append(temp_image)\n",
        "\n",
        "#   combined_test_data.append(temp_list)\n",
        "\n",
        "# random.shuffle(combined_test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvgGLs8S-cW9"
      },
      "outputs": [],
      "source": [
        "# test_text2 = []\n",
        "# test_audio_utterance2 = []\n",
        "# test_image_utterance2 = []\n",
        "\n",
        "# for j in combined_test_data:\n",
        "#   test_text2.append(j[0])\n",
        "#   test_audio_utterance2.append(j[1])\n",
        "#   test_image_utterance2.append(j[2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgCracq6--vg"
      },
      "outputs": [],
      "source": [
        "# VALIDLEN = 69"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UarPmcdo_uaU"
      },
      "outputs": [],
      "source": [
        "# len(test_audio_utterance2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgOQVaRV_-iV"
      },
      "outputs": [],
      "source": [
        "# test_audio_utterance2[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsLJeYkvAdoK"
      },
      "outputs": [],
      "source": [
        "# torch.stack(test_audio_utterance2).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDznHLJbzAbl"
      },
      "outputs": [],
      "source": [
        "# valid_audio_data_utterance = test_audio_data_utterance1[:VALIDLEN]\n",
        "# valid_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XNjZNpBAYd_"
      },
      "outputs": [],
      "source": [
        "# test_audio_data_utterance = test_audio_data_utterance1[VALIDLEN:]\n",
        "# test_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-kqcb9tiyB8"
      },
      "outputs": [],
      "source": [
        "# valid_image_data_utterance = torch.tensor(train_image_data_utterance2)[Trainlen:]\n",
        "# valid_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8KNyWBm2Ie4"
      },
      "outputs": [],
      "source": [
        "# valid_image_data_utterance = test_image_data_utterance1[:VALIDLEN]\n",
        "# valid_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5rPLXq6At1F"
      },
      "outputs": [],
      "source": [
        "# test_image_data_utterance = test_image_data_utterance1[VALIDLEN:]\n",
        "# test_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCxFywiVko_v"
      },
      "outputs": [],
      "source": [
        "# test_audio_data_utterance = torch.tensor(test_audio_data_utterance1)\n",
        "# # test_audio_data_utterance = test_audio_data_utterance.unsqueeze(dim = 1)\n",
        "# test_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxqP1M_2k2Y_"
      },
      "outputs": [],
      "source": [
        "# test_image_data_utterance = torch.tensor(test_image_data_utterance1)\n",
        "# # test_image_data_utterance = test_image_data_utterance.unsqueeze(dim = 1)\n",
        "# test_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKt4UobTIJQY"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # print(len(text_file[train_text]))\n",
        "# # print(train_text)\n",
        "# # print(len(text_file[test_text]))\n",
        "\n",
        "# train_audio_broadcast_utterance = audio_video_broadcast(train_audio_data_utterance)\n",
        "\n",
        "# print(\"train_audio_broadcast_utterance complete : \", train_audio_broadcast_utterance.shape)\n",
        "# train_image_broadcast_utterance = audio_video_broadcast(train_image_data_utterance)\n",
        "# print(\"train_image_broadcast_utterance complete : \",train_image_broadcast_utterance.shape)\n",
        "\n",
        "# valid_audio_broadcast_utterance = audio_video_broadcast(valid_audio_data_utterance)\n",
        "# print(\"valid_audio_broadcast_utterance complete : \", valid_audio_broadcast_utterance.shape)\n",
        "\n",
        "# valid_image_broadcast_utterance = audio_video_broadcast(valid_image_data_utterance)\n",
        "# print('valid_image_broadcast_utterance complete : ', valid_image_broadcast_utterance.shape)\n",
        "\n",
        "# test_audio_broadcast_utterance = audio_video_broadcast(test_audio_data_utterance)\n",
        "# print(\"test_audio_broadcast_utterance complete : \",test_audio_broadcast_utterance.shape)\n",
        "# test_image_broadcast_utterance = audio_video_broadcast(test_image_data_utterance)\n",
        "# print(\"test_image_broadcast_utterance complete : \",test_image_broadcast_utterance.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ik_vBo7x6Rm"
      },
      "outputs": [],
      "source": [
        "# tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0y5DYklxuUT"
      },
      "outputs": [],
      "source": [
        "# p = {\n",
        "#         'additional_special_tokens' : ['[CONTEXT]', '[UTTERANCE]']\n",
        "#     }\n",
        "\n",
        "# tokenizer.add_special_tokens(p)\n",
        "\n",
        "# tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1Ctmwokx-T2"
      },
      "outputs": [],
      "source": [
        "# model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uExwN4zRe0Wq"
      },
      "outputs": [],
      "source": [
        "p = {\n",
        "        'additional_special_tokens' : ['[CONTEXT]', '[UTTERANCE]']\n",
        "    }\n",
        "\n",
        "tokenizer.add_special_tokens(p)\n",
        "\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRsujb2Nmwks"
      },
      "outputs": [],
      "source": [
        "# tokenizer2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_h3zgFvfrGo"
      },
      "outputs": [],
      "source": [
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg1kq4a5e3Cp"
      },
      "outputs": [],
      "source": [
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7mKmBRp2XqP"
      },
      "outputs": [],
      "source": [
        "# def prepare_dataset_context(text_data):\n",
        "\n",
        "\n",
        "#             context = []\n",
        "#             # labels = []\n",
        "#             for i in range(len(text_data)):\n",
        "#                 data_point = text_data[i]\n",
        "\n",
        "#                 # example_speaker = data_point['speaker']\n",
        "#                 # example_utterance = data_point['utterance']\n",
        "#                 # temp_label = int(data_point['sarcasm'])\n",
        "\n",
        "#                 # example_context = '[CONTEXT] '\n",
        "#                 example_context = ''\n",
        "\n",
        "#                 temp_len = len(data_point['context_speakers'])\n",
        "#                 cnt = 0\n",
        "#                 print(\"Temp len : \", temp_len)\n",
        "#                 for speaker, utterance in list(zip(data_point['context_speakers'], data_point['context'])):\n",
        "#                     print(\"count : \", cnt)\n",
        "#                     if(cnt == temp_len - 1):\n",
        "#                       example_context = example_context + speaker.upper() + \" : \" + utterance\n",
        "#                     else:\n",
        "#                       example_context = example_context + speaker.upper() + \" : \" + utterance + \" , \"\n",
        "#                     cnt+=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#                 # print(example_dialog)\n",
        "#                 example_context = re.sub(' +', ' ', example_context)\n",
        "\n",
        "#                 context.append(example_context)\n",
        "#                 # labels.append(temp_label)\n",
        "\n",
        "#             # df = pd.DataFrame(dialog, columns=['dialog'])\n",
        "\n",
        "#             # labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "#             enc = bert_tokenizer(context, max_length = SOURCE_MAX_LEN, padding = 'max_length', truncation = True)\n",
        "\n",
        "#             # df['audio_features'] = acoustic_data\n",
        "#             # df['visual_features'] = visual_data\n",
        "\n",
        "#             return torch.tensor(enc['input_ids'], dtype=torch.long), torch.tensor(enc['attention_mask'], dtype=torch.bool)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiN-BMSgISy7"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(text_data):\n",
        "\n",
        "\n",
        "            dialog = []\n",
        "            labels = []\n",
        "            for i in range(len(text_data)):\n",
        "                data_point = text_data[i]\n",
        "\n",
        "                example_speaker = data_point['speaker']\n",
        "                example_utterance = data_point['utterance']\n",
        "                temp_label = int(data_point['sarcasm'])\n",
        "\n",
        "                # example_dialog = '[CONTEXT] '\n",
        "                # example_dialog = '[TARGET] '\n",
        "                example_dialog = '[CONTEXT] '\n",
        "\n",
        "\n",
        "                for speaker, utterance in list(zip(data_point['context_speakers'], data_point['context'])):\n",
        "                    example_dialog = example_dialog + speaker.upper() + \" : \" + utterance + \" | \"\n",
        "\n",
        "                example_dialog = example_dialog + ' [UTTERANCE] ' + example_speaker + \" : \" + example_utterance + \" | \"\n",
        "                # example_dialog = example_dialog + example_speaker + \" : \" + example_utterance\n",
        "                # example_dialog = example_dialog + example_speaker + \" : \" + example_utterance\n",
        "                # print(example_dialog)\n",
        "                example_dialog = re.sub(' +', ' ', example_dialog)\n",
        "\n",
        "                dialog.append(example_dialog)\n",
        "                labels.append(temp_label)\n",
        "\n",
        "            # df = pd.DataFrame(dialog, columns=['dialog'])\n",
        "\n",
        "            labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "            # enc = tokenizer(dialog, max_length = SOURCE_MAX_LEN, padding = 'max_length', truncation = True)\n",
        "            enc = tokenizer(dialog, max_length = SOURCE_MAX_LEN, padding = 'max_length', truncation = True)\n",
        "\n",
        "            # df['audio_features'] = acoustic_data\n",
        "            # df['visual_features'] = visual_data\n",
        "\n",
        "            return torch.tensor(enc['input_ids'], dtype=torch.long), torch.tensor(enc['attention_mask'], dtype=torch.bool), labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2dU7MfiZaDq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7VlrC_WI9ey"
      },
      "outputs": [],
      "source": [
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgoPnVD4qfO_"
      },
      "outputs": [],
      "source": [
        "len(train_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7p229zVGBUG"
      },
      "outputs": [],
      "source": [
        "train_text_input_ids1, train_text_attention_mask1, train_ground_truth1 = prepare_dataset(train_text)\n",
        "train_ground_truth1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdGsoVBDJxLw"
      },
      "outputs": [],
      "source": [
        "# train_text_input_ids1, train_text_attention_mask1, train_ground_truth1 = prepare_dataset_utterance(train_text)\n",
        "# train_ground_truth1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQdk9Tk8WYzW"
      },
      "outputs": [],
      "source": [
        "train_text_input_ids = train_text_input_ids1\n",
        "train_text_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rqaEYpWZMVx"
      },
      "outputs": [],
      "source": [
        "# train_text_input_ids = train_text_input_ids1[:Trainlen]\n",
        "# train_text_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIYgUkhfWcL8"
      },
      "outputs": [],
      "source": [
        "train_text_attention_mask = train_text_attention_mask1\n",
        "train_text_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRRAuyVaaOun"
      },
      "outputs": [],
      "source": [
        "# train_text_attention_mask = train_text_attention_mask1[:Trainlen]\n",
        "# train_text_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q819IVudYtDG"
      },
      "outputs": [],
      "source": [
        "train_ground_truth = train_ground_truth1\n",
        "train_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8iuUm6vWeN-"
      },
      "outputs": [],
      "source": [
        "# train_ground_truth = train_ground_truth1[:Trainlen]\n",
        "# train_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKXPrrh_jR_O"
      },
      "outputs": [],
      "source": [
        "# train_ground_truth = train_ground_truth1[:Trainlen]\n",
        "# train_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAcvsTpeaeyB"
      },
      "outputs": [],
      "source": [
        "# train_ground_truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bctWHziiWqR0"
      },
      "outputs": [],
      "source": [
        "# context_input_ids, context_attention_mask = prepare_dataset_context(train_text)\n",
        "# print(context_input_ids.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScKZf52FX6AD"
      },
      "outputs": [],
      "source": [
        "# context_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0C9PnF9JypC"
      },
      "outputs": [],
      "source": [
        "print(\"TYPE : train_text_input_ids : \", type(train_text_input_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzsrAIieWgvT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# train_context_input_ids = context_input_ids\n",
        "# train_context_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "815UHYi_juEa"
      },
      "outputs": [],
      "source": [
        "\n",
        "# train_context_input_ids = context_input_ids[:Trainlen]\n",
        "# train_context_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPScSL-aWiyC"
      },
      "outputs": [],
      "source": [
        "# train_context_attention_mask = context_attention_mask\n",
        "# train_context_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEYf1vyblLxm"
      },
      "outputs": [],
      "source": [
        "# train_context_attention_mask = context_attention_mask[:Trainlen]\n",
        "# train_context_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZECJ_D6YwSF"
      },
      "outputs": [],
      "source": [
        "# valid_text_input_ids = train_text_input_ids1[Trainlen:]\n",
        "# valid_text_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Iqag88ipliS"
      },
      "outputs": [],
      "source": [
        "\n",
        "# valid_text_attention_mask = train_text_attention_mask1[Trainlen:]\n",
        "# valid_text_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbjcYjztp523"
      },
      "outputs": [],
      "source": [
        "# valid_ground_truth = train_ground_truth1[Trainlen:]\n",
        "# valid_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDIcnT9t-G1l"
      },
      "outputs": [],
      "source": [
        "# valid_text_input_ids, valid_text_attention_mask, valid_ground_truth = prepare_dataset_utterance(valid_text)\n",
        "# valid_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkfN5cn6YYoz"
      },
      "outputs": [],
      "source": [
        "# valid_context_input_ids = context_input_ids[Trainlen:]\n",
        "# valid_context_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ogVRlSWrgJf"
      },
      "outputs": [],
      "source": [
        "# valid_context_attention_mask = context_attention_mask[Trainlen:]\n",
        "# valid_context_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuNLAd5CIWLd"
      },
      "outputs": [],
      "source": [
        "# test_text_input_ids, test_text_attention_mask, test_ground_truth = prepare_dataset_utterance(test_text)\n",
        "# test_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vB39rShGL4K"
      },
      "outputs": [],
      "source": [
        "test_text_input_ids, test_text_attention_mask, test_ground_truth = prepare_dataset(test_text)\n",
        "test_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dIyrYyhGDFH"
      },
      "outputs": [],
      "source": [
        "# valid_id = test_text_input_ids[:VALID_LEN]\n",
        "# valid_id.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmfvmZTEGbiQ"
      },
      "outputs": [],
      "source": [
        "# test_id = test_text_input_ids[VALIDLEN:]\n",
        "# test_id.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Wi2Kyr5Gm52"
      },
      "outputs": [],
      "source": [
        "# valid_mask = test_text_attention_mask[:VALIDLEN]\n",
        "# valid_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7V0wmGiHAB_"
      },
      "outputs": [],
      "source": [
        "# test_mask = test_text_attention_mask[VALIDLEN:]\n",
        "# test_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFZQBCCTHFzt"
      },
      "outputs": [],
      "source": [
        "# valid_truth = test_ground_truth[:VALIDLEN]\n",
        "# valid_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wiu7X_lHWFi"
      },
      "outputs": [],
      "source": [
        "# test_truth = test_ground_truth[VALIDLEN:]\n",
        "# test_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-ovTep1tWmK"
      },
      "outputs": [],
      "source": [
        "# test_context_input_ids, test_context_attention_mask = prepare_dataset_context(test_text)\n",
        "# test_context_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMO8hPfyHkja"
      },
      "outputs": [],
      "source": [
        "# valid_context_id = test_context_input_ids[:VALIDLEN]\n",
        "# valid_context_id.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IG3A9ASzHwW3"
      },
      "outputs": [],
      "source": [
        "# test_context_id = test_context_input_ids[VALIDLEN:]\n",
        "# test_context_id.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1iZshp8tld_"
      },
      "outputs": [],
      "source": [
        "# test_context_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doTkSyhKIMOt"
      },
      "outputs": [],
      "source": [
        "# valid_context_mask = test_context_attention_mask[:VALIDLEN]\n",
        "# valid_context_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGa0EsBVIpzX"
      },
      "outputs": [],
      "source": [
        "# test_context_mask = test_context_attention_mask[VALIDLEN:]\n",
        "# test_context_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0kqUoFDKPkS"
      },
      "outputs": [],
      "source": [
        "# tokenizer.add_tokens(['[CONTEXT]', '[TARGET]'], special_tokens = True)\n",
        "# model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EEHQlvrRM3j"
      },
      "outputs": [],
      "source": [
        "test_audio_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PdHAFkTRsyK"
      },
      "outputs": [],
      "source": [
        "test_image_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FilVGy_RxCw"
      },
      "outputs": [],
      "source": [
        "print(test_text_input_ids.shape)\n",
        "print(test_text_attention_mask.shape)\n",
        "print(test_ground_truth.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Icba89ykR_5g"
      },
      "outputs": [],
      "source": [
        "# print(test_context_input_ids.shape)\n",
        "# print(test_context_attention_mask.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXqJ0fcjSJ4I"
      },
      "outputs": [],
      "source": [
        "# test_input_data = []\n",
        "\n",
        "\n",
        "# for j in range(test_ground_truth.shape[0]):\n",
        "#   temp_list = []\n",
        "#   temp_list.append(test_text_input_ids[j])\n",
        "#   temp_list.append(test_text_attention_mask[j])\n",
        "#   temp_list.append(test_context_input_ids[j])\n",
        "#   temp_list.append(test_context_attention_mask[j])\n",
        "#   temp_list.append(test_audio_data_utterance1[j])\n",
        "#   temp_list.append(test_image_data_utterance1[j])\n",
        "\n",
        "#   test_input_data.append(temp_list)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9swXKn_TQsW"
      },
      "outputs": [],
      "source": [
        "# print(type(test_input_data))\n",
        "# print(len(test_input_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjn1S99ZS_SX"
      },
      "outputs": [],
      "source": [
        "# test_output_data = test_ground_truth.tolist()\n",
        "# print(type(test_output_data))\n",
        "# print(len(test_output_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecXLmWNsT6h-"
      },
      "outputs": [],
      "source": [
        "# X_valid, X_test, Y_valid, Y_test = train_test_split(\n",
        "#     test_input_data, test_output_data, test_size = 0.5, stratify = test_output_data\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsXIaU_jUUwH"
      },
      "outputs": [],
      "source": [
        "# len(X_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6lG18S9UcWx"
      },
      "outputs": [],
      "source": [
        "# len(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfleSSo6UfWk"
      },
      "outputs": [],
      "source": [
        "# len(Y_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vi0Vpd2lUhR7"
      },
      "outputs": [],
      "source": [
        "# len(Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnzHh4kBUk6f"
      },
      "outputs": [],
      "source": [
        "# valid_text_input_ids = []\n",
        "# valid_text_attention_mask = []\n",
        "# valid_context_input_ids = []\n",
        "# valid_context_attention_mask = []\n",
        "\n",
        "# valid_audio_data = []\n",
        "# valid_image_data = []\n",
        "\n",
        "# for j in range(len(X_valid)):\n",
        "#   valid_text_input_ids.append(X_valid[j][0])\n",
        "#   valid_text_attention_mask.append(X_valid[j][1])\n",
        "\n",
        "#   valid_context_input_ids.append(X_valid[j][2])\n",
        "#   valid_context_attention_mask.append(X_valid[j][3])\n",
        "\n",
        "#   valid_audio_data.append(X_valid[j][4])\n",
        "\n",
        "#   valid_image_data.append(X_valid[j][5])\n",
        "\n",
        "# print(len(valid_text_input_ids))\n",
        "# print(len(valid_text_attention_mask))\n",
        "# print(len(valid_context_input_ids))\n",
        "# print(len(valid_context_attention_mask))\n",
        "# print(len(valid_audio_data))\n",
        "# print(len(valid_image_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyqnoPspY06u"
      },
      "outputs": [],
      "source": [
        "# valid_text_input_ids[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoyGWRN9V6C9"
      },
      "outputs": [],
      "source": [
        "# valid_text_input_ids = torch.stack(valid_text_input_ids)\n",
        "# print(valid_text_input_ids.shape)\n",
        "# valid_text_attention_mask = torch.stack(valid_text_attention_mask)\n",
        "# print(valid_text_attention_mask.shape)\n",
        "# valid_context_input_ids = torch.stack(valid_context_input_ids)\n",
        "# print(valid_context_input_ids.shape)\n",
        "# valid_context_attention_mask = torch.stack(valid_context_attention_mask)\n",
        "# print(valid_context_attention_mask.shape)\n",
        "# valid_audio_data = torch.stack(valid_audio_data)\n",
        "# print(valid_audio_data.shape)\n",
        "# valid_image_data = torch.stack(valid_image_data)\n",
        "# print(valid_image_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipNEkx0wV2Wz"
      },
      "outputs": [],
      "source": [
        "# valid_ground_truth = torch.tensor(Y_valid)\n",
        "# valid_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FM6TlZIatoC"
      },
      "outputs": [],
      "source": [
        "# test_text_id = []\n",
        "# test_text_mask = []\n",
        "\n",
        "# test_context_id = []\n",
        "# test_context_mask = []\n",
        "\n",
        "# test_audio_data = []\n",
        "# test_image_data = []\n",
        "\n",
        "# for j in range(len(X_test)):\n",
        "#   test_text_id.append(X_test[j][0])\n",
        "#   test_text_mask.append(X_test[j][1])\n",
        "\n",
        "#   test_context_id.append(X_test[j][2])\n",
        "#   test_context_mask.append(X_test[j][3])\n",
        "\n",
        "#   test_audio_data.append(X_test[j][4])\n",
        "\n",
        "#   test_image_data.append(X_test[j][5])\n",
        "\n",
        "# test_text_id = torch.stack(test_text_id)\n",
        "# print(test_text_id.shape)\n",
        "# test_text_mask = torch.stack(test_text_mask)\n",
        "# print(test_text_mask.shape)\n",
        "# test_context_id = torch.stack(test_context_id)\n",
        "# print(test_context_id.shape)\n",
        "# test_context_mask = torch.stack(test_context_mask)\n",
        "# print(test_context_mask.shape)\n",
        "# test_audio_data = torch.stack(test_audio_data)\n",
        "# print(test_audio_data.shape)\n",
        "# test_image_data = torch.stack(test_image_data)\n",
        "# print(test_image_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiJSqVpFckAT"
      },
      "outputs": [],
      "source": [
        "# test_ground_truth = torch.tensor(Y_test)\n",
        "# print(test_ground_truth.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMN_GJV1Jjps"
      },
      "outputs": [],
      "source": [
        "\n",
        "# tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SY3KgbQUKoFd"
      },
      "outputs": [],
      "source": [
        "tokenizer.all_special_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7uPY9VkLRcr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGASW03jLFtd"
      },
      "outputs": [],
      "source": [
        "# tokenizer.all_special_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVqVQYAULBg5"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.all_special_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNcRXEYZLK7w"
      },
      "outputs": [],
      "source": [
        "# train_audio_broadcast_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eP1Y_my0-btZ"
      },
      "outputs": [],
      "source": [
        "# valid_audio_data_utterance = test_audio_data_utterance[:VALID_LEN, :, :]\n",
        "# valid_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4c_xiCD-tn0"
      },
      "outputs": [],
      "source": [
        "# valid_image_data_utterance = test_image_data_utterance[:VALID_LEN, :, :]\n",
        "# valid_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwLEXwtB-6KN"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# test_audio_data_utterance = test_audio_data_utterance[VALID_LEN:, :, :]\n",
        "# test_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fF_ErwW5_EsF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# test_image_data_utterance = test_image_data_utterance[VALID_LEN:, :, :]\n",
        "# test_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-7eDQZmEhNZ"
      },
      "outputs": [],
      "source": [
        "test_input_data = []\n",
        "\n",
        "\n",
        "for j in range(test_ground_truth.shape[0]):\n",
        "  temp_list = []\n",
        "  temp_list.append(test_text_input_ids[j])\n",
        "  temp_list.append(test_text_attention_mask[j])\n",
        "\n",
        "  temp_list.append(test_audio_data_utterance1[j])\n",
        "  temp_list.append(test_image_data_utterance1[j])\n",
        "\n",
        "  test_input_data.append(temp_list)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5UZkOdvEl6f"
      },
      "outputs": [],
      "source": [
        "print(type(test_input_data))\n",
        "print(len(test_input_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnW2xGe6Enfw"
      },
      "outputs": [],
      "source": [
        "test_output_data = test_ground_truth.tolist()\n",
        "print(type(test_output_data))\n",
        "print(len(test_output_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jO18wKiHEoto"
      },
      "outputs": [],
      "source": [
        "X_valid, X_test, Y_valid, Y_test = train_test_split(\n",
        "    test_input_data, test_output_data, test_size = 0.5, stratify = test_output_data\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFOS59YEKznY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fegJT6LBEsM8"
      },
      "outputs": [],
      "source": [
        "len(X_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRaVQfSEEtmp"
      },
      "outputs": [],
      "source": [
        "len(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_Q2A4vkEu2w"
      },
      "outputs": [],
      "source": [
        "len(Y_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCjoibAAEwTD"
      },
      "outputs": [],
      "source": [
        "len(Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bccK__bzExmv"
      },
      "outputs": [],
      "source": [
        "valid_text_input_ids = []\n",
        "valid_text_attention_mask = []\n",
        "valid_context_input_ids = []\n",
        "valid_context_attention_mask = []\n",
        "\n",
        "valid_audio_data = []\n",
        "valid_image_data = []\n",
        "\n",
        "for j in range(len(X_valid)):\n",
        "  valid_text_input_ids.append(X_valid[j][0])\n",
        "  valid_text_attention_mask.append(X_valid[j][1])\n",
        "\n",
        "\n",
        "  valid_audio_data.append(X_valid[j][2])\n",
        "\n",
        "  valid_image_data.append(X_valid[j][3])\n",
        "\n",
        "print(len(valid_text_input_ids))\n",
        "print(len(valid_text_attention_mask))\n",
        "\n",
        "print(len(valid_audio_data))\n",
        "print(len(valid_image_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pK8TO60JEy_x"
      },
      "outputs": [],
      "source": [
        "valid_text_input_ids[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XECiB9_qE0KB"
      },
      "outputs": [],
      "source": [
        "valid_text_input_ids = torch.stack(valid_text_input_ids)\n",
        "print(valid_text_input_ids.shape)\n",
        "valid_text_attention_mask = torch.stack(valid_text_attention_mask)\n",
        "print(valid_text_attention_mask.shape)\n",
        "\n",
        "valid_audio_data = torch.stack(valid_audio_data)\n",
        "print(valid_audio_data.shape)\n",
        "valid_image_data = torch.stack(valid_image_data)\n",
        "print(valid_image_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqE-5ALLE1en"
      },
      "outputs": [],
      "source": [
        "valid_ground_truth = torch.tensor(Y_valid)\n",
        "valid_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTV-5u5IE2zS"
      },
      "outputs": [],
      "source": [
        "test_text_id = []\n",
        "test_text_mask = []\n",
        "\n",
        "test_context_id = []\n",
        "test_context_mask = []\n",
        "\n",
        "test_audio_data = []\n",
        "test_image_data = []\n",
        "\n",
        "for j in range(len(X_test)):\n",
        "  test_text_id.append(X_test[j][0])\n",
        "  test_text_mask.append(X_test[j][1])\n",
        "\n",
        "\n",
        "  test_audio_data.append(X_test[j][2])\n",
        "\n",
        "  test_image_data.append(X_test[j][3])\n",
        "\n",
        "test_text_id = torch.stack(test_text_id)\n",
        "print(test_text_id.shape)\n",
        "test_text_mask = torch.stack(test_text_mask)\n",
        "print(test_text_mask.shape)\n",
        "\n",
        "test_audio_data = torch.stack(test_audio_data)\n",
        "print(test_audio_data.shape)\n",
        "test_image_data = torch.stack(test_image_data)\n",
        "print(test_image_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s73tO--wE4aD"
      },
      "outputs": [],
      "source": [
        "test_ground_truth = torch.tensor(Y_test)\n",
        "print(test_ground_truth.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrligkveIb4f"
      },
      "outputs": [],
      "source": [
        "class MultimodalSarcasmDataset(Dataset):\n",
        "    # def __init__(self, utterance_input_ids, utterance_attention_mask, context_input_ids, context_attention_mask, acoustic_data, visual_data, labels):\n",
        "    def __init__(self, utterance_input_ids, utterance_attention_mask, acoustic_data, visual_data, labels):\n",
        "\n",
        "        self.utterance_input_ids = utterance_input_ids\n",
        "        self.utterance_attention_mask = utterance_attention_mask\n",
        "        # self.context_input_ids = context_input_ids\n",
        "        # self.context_attention_mask = context_attention_mask\n",
        "        # self.context_attention_mask\n",
        "        self.acoustic_data = acoustic_data\n",
        "        self.visual_data = visual_data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.utterance_input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # return self.utterance_input_ids[idx], self.utterance_attention_mask[idx], self.context_input_ids[idx], self.context_attention_mask[idx], self.acoustic_data[idx], self.visual_data[idx], self.labels[idx]\n",
        "        return self.utterance_input_ids[idx], self.utterance_attention_mask[idx],  self.acoustic_data[idx], self.visual_data[idx], self.labels[idx]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IhvAU6M_0mh"
      },
      "outputs": [],
      "source": [
        "# train_text_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UJo5LQ3RjCx"
      },
      "outputs": [],
      "source": [
        "# train_context_attention_mask.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oQY9qKjXbPU"
      },
      "outputs": [],
      "source": [
        "# train_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1wRtyIFYS2m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILQZRYToYYDd"
      },
      "outputs": [],
      "source": [
        "# train_text_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3SQyApZYbH7"
      },
      "outputs": [],
      "source": [
        "# train_text_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JFdrEk1Yd-J"
      },
      "outputs": [],
      "source": [
        "# train_context_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYiUBLjTYgdj"
      },
      "outputs": [],
      "source": [
        "# train_context_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRpLc5f7Yiyz"
      },
      "outputs": [],
      "source": [
        "# train_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c8Wsp6wYlcV"
      },
      "outputs": [],
      "source": [
        "# train_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBLinpQcYo_4"
      },
      "outputs": [],
      "source": [
        "# train_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gya_VngQJFlx"
      },
      "outputs": [],
      "source": [
        "# valid_context_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvLOlLI1JJmE"
      },
      "outputs": [],
      "source": [
        "# valid_context_id.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOwejwPbJPwB"
      },
      "outputs": [],
      "source": [
        "# valid_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KcNwn5qJcGa"
      },
      "outputs": [],
      "source": [
        "# test_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_5HoO4YdY0U"
      },
      "outputs": [],
      "source": [
        "# test_context_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYWmjXQZ5y2W"
      },
      "outputs": [],
      "source": [
        "# test_audio_data_utterance1.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOHMQ9HIFRnh"
      },
      "outputs": [],
      "source": [
        "test_image_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0C6e9caDFSy_"
      },
      "outputs": [],
      "source": [
        "test_audio_data.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEIpohRVFUO4"
      },
      "outputs": [],
      "source": [
        "test_text_id.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jEKs32xFVsZ"
      },
      "outputs": [],
      "source": [
        "test_text_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBTmMfh6FXAx"
      },
      "outputs": [],
      "source": [
        "valid_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ux-4pOaOFYYk"
      },
      "outputs": [],
      "source": [
        "test_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5j_r6ZqtIeD9"
      },
      "outputs": [],
      "source": [
        "# train_loader = DataLoader(MultimodalSarcasmDataset(train_text_input_ids, train_text_attention_mask, train_context_input_ids, train_context_attention_mask,train_audio_broadcast_utterance, train_image_broadcast_utterance, train_ground_truth), batch_size=32, shuffle = True)\n",
        "# valid_loader = DataLoader(MultimodalSarcasmDataset(valid_text_input_ids, valid_text_attention_mask, valid_context_input_ids, valid_context_attention_mask, valid_audio_broadcast_utterance, valid_image_broadcast_utterance, valid_ground_truth), batch_size = 32, shuffle = False)\n",
        "# test_loader = DataLoader(MultimodalSarcasmDataset(test_text_input_ids, test_text_attention_mask, test_context_input_ids, test_context_attention_mask, test_audio_broadcast_utterance, test_image_broadcast_utterance, test_ground_truth), batch_size=32, shuffle = False)\n",
        "\n",
        "# train_loader = DataLoader(MultimodalSarcasmDataset(train_text_input_ids, train_text_attention_mask, train_context_input_ids, train_context_attention_mask, train_audio_data_utterance, train_image_data_utterance, train_ground_truth), batch_size=32, shuffle = True)\n",
        "# # valid_loader = DataLoader(MultimodalSarcasmDataset(valid_text_input_ids, valid_text_attention_mask, valid_context_input_ids, valid_context_attention_mask,   valid_audio_data, valid_image_data, valid_ground_truth), batch_size = 32, shuffle = False)\n",
        "# test_loader = DataLoader(MultimodalSarcasmDataset(test_text_input_ids, test_text_attention_mask, test_context_input_ids, test_context_attention_mask,  test_audio_data_utterance1, test_image_data_utterance1, test_ground_truth), batch_size=32, shuffle = False)\n",
        "\n",
        "# train_loader = DataLoader(MultimodalSarcasmDataset(train_text_input_ids, train_text_attention_mask,  train_audio_data_utterance, train_image_data_utterance, train_ground_truth), batch_size=32, shuffle = True)\n",
        "# # valid_loader = DataLoader(MultimodalSarcasmDataset(valid_text_input_ids, valid_text_attention_mask, valid_context_input_ids, valid_context_attention_mask,   valid_audio_data, valid_image_data, valid_ground_truth), batch_size = 32, shuffle = False)\n",
        "# test_loader = DataLoader(MultimodalSarcasmDataset(test_text_input_ids, test_text_attention_mask,   test_audio_data_utterance1, test_image_data_utterance1, test_ground_truth), batch_size=32, shuffle = False)\n",
        "\n",
        "# train_loader = DataLoader(MultimodalSarcasmDataset(train_text_input_ids, train_text_attention_mask,  train_audio_data_utterance, train_image_data_utterance, train_ground_truth), batch_size=4, shuffle = True)\n",
        "# # valid_loader = DataLoader(MultimodalSarcasmDataset(valid_text_input_ids, valid_text_attention_mask, valid_context_input_ids, valid_context_attention_mask,   valid_audio_data, valid_image_data, valid_ground_truth), batch_size = 32, shuffle = False)\n",
        "# test_loader = DataLoader(MultimodalSarcasmDataset(test_text_input_ids, test_text_attention_mask,   test_audio_data_utterance1, test_image_data_utterance1, test_ground_truth), batch_size=4, shuffle = False)\n",
        "\n",
        "train_loader = DataLoader(MultimodalSarcasmDataset(train_text_input_ids, train_text_attention_mask,  train_audio_data_utterance, train_image_data_utterance, train_ground_truth), batch_size=32, shuffle = True)\n",
        "valid_loader = DataLoader(MultimodalSarcasmDataset(valid_text_input_ids, valid_text_attention_mask,    valid_audio_data, valid_image_data, valid_ground_truth), batch_size = 32, shuffle = False)\n",
        "test_loader = DataLoader(MultimodalSarcasmDataset(test_text_id, test_text_mask,   test_audio_data, test_image_data, test_ground_truth), batch_size=32, shuffle = False)\n",
        "\n",
        "\n",
        "print(test_loader)\n",
        "\n",
        "# print(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJYdQp5vwRim"
      },
      "outputs": [],
      "source": [
        "len(train_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYX5DVe_wYtG"
      },
      "outputs": [],
      "source": [
        "len(valid_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPTKQs5uwdsY"
      },
      "outputs": [],
      "source": [
        "len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44TjmFmSIh9D"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1EC_JPPUpq_"
      },
      "outputs": [],
      "source": [
        "model.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbrSjHApAMiQ"
      },
      "outputs": [],
      "source": [
        "DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TF3U8SkdZJ9P"
      },
      "outputs": [],
      "source": [
        "model = model.to(DEVICE)\n",
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5MxPWWyIotE"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, data_loader):\n",
        "      model.train()\n",
        "      epoch_train_loss = 0.0\n",
        "\n",
        "\n",
        "      for step, batch in enumerate(tqdm(data_loader, desc = 'Training Iteration')):\n",
        "        # for i, t in enumerate(batch):\n",
        "        #     print(\"Inside hello\")\n",
        "        #     print(i, \" : \", type(t))\n",
        "        batch = tuple(t.to(DEVICE) for t in batch)\n",
        "        # input_ids, attention_mask, context_input_ids, context_attention_mask, acoustic_input, visual_input, labels = batch\n",
        "        input_ids, attention_mask,  acoustic_input, visual_input, labels = batch\n",
        "        # print('\\ninput ids shape : ', input_ids.shape)\n",
        "        # print(\"attention mask shape : \", attention_mask.shape)\n",
        "        # print('acoustic_input shape : ', acoustic_input.shape)\n",
        "        # print('visual_input shape : ', visual_input.shape)\n",
        "        optimizer.zero_grad()\n",
        "        # print(\"Input ids shape : \", input_ids.shape)\n",
        "        # print(\"Input ids shape : \", input_ids.shape)\n",
        "        outputs = model(input_ids = input_ids,\n",
        "                        attention_mask = attention_mask,\n",
        "\n",
        "                        # context_input_ids = context_input_ids,\n",
        "                        # context_attention_mask = context_attention_mask,\n",
        "                        acoustic_input = acoustic_input,\n",
        "                        visual_input = visual_input,\n",
        "                        labels = labels)\n",
        "\n",
        "        # outputs = model2(input_ids = input_ids,\n",
        "        #                 attention_mask = attention_mask,\n",
        "        #                 )\n",
        "        # last_hidden_state = outputs['last_hidde,n_state']\n",
        "        # print('last hidden_state shape : ', last_hidden_state.shape)\n",
        "        loss = outputs['loss']\n",
        "        epoch_train_loss += loss.item()\n",
        "\n",
        "        # print(\"Batch wise loss : \", epoch_train_loss)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "      print(\"Epoch train loss : \", epoch_train_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0H6Hk2w62xu"
      },
      "outputs": [],
      "source": [
        "def valid_epoch(model, data_loader):\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  gold = []\n",
        "\n",
        "  valid_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for step, batch in enumerate(tqdm(data_loader)):\n",
        "      batch = tuple(t.to(DEVICE) for t in batch)\n",
        "      # input_ids, attention_mask, context_input_ids, context_attention_mask, acoustic_input, visual_input, labels = batch\n",
        "      input_ids, attention_mask,  acoustic_input, visual_input, labels = batch\n",
        "\n",
        "\n",
        "      outputs = model(input_ids = input_ids,\n",
        "                            attention_mask = attention_mask,\n",
        "                            # context_input_ids = context_input_ids,\n",
        "                            # context_attention_mask = context_attention_mask,\n",
        "                            acoustic_input = acoustic_input,\n",
        "                            visual_input = visual_input,\n",
        "                            labels = labels)\n",
        "\n",
        "      logits = outputs['logits']\n",
        "      loss = outputs['loss']\n",
        "\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "\n",
        "\n",
        "      pred = logits.argmax(dim = -1)\n",
        "\n",
        "      predictions.extend(pred.tolist())\n",
        "      gold.extend(labels.tolist())\n",
        "\n",
        "  return valid_loss, predictions, gold\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93nGID9-Imr-"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def test_epoch(model, data_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    gold = []\n",
        "\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(tqdm(data_loader)):\n",
        "            batch = tuple(t.to(DEVICE) for t in batch)\n",
        "            # input_ids, attention_mask, context_input_ids, context_attention_mask, acoustic_input, visual_input, labels = batch\n",
        "            input_ids, attention_mask,  acoustic_input, visual_input, labels = batch\n",
        "\n",
        "            # print(\"attention mask shape : \", attention_mask.shape)\n",
        "\n",
        "            outputs = model(input_ids = input_ids,\n",
        "                            attention_mask = attention_mask,\n",
        "                            # context_input_ids = context_input_ids,\n",
        "                            # context_attention_mask = context_attention_mask,\n",
        "\n",
        "                            acoustic_input = acoustic_input,\n",
        "                            visual_input = visual_input,\n",
        "                            labels = labels)\n",
        "\n",
        "            logits = outputs['logits']\n",
        "\n",
        "            pred = logits.argmax(dim = -1)\n",
        "\n",
        "            predictions.extend(pred.tolist())\n",
        "\n",
        "            gold.extend(labels.tolist())\n",
        "\n",
        "            correct += int((pred == labels).sum())\n",
        "\n",
        "    return correct/len(data_loader.dataset), predictions, gold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5QauBgKBtYS"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "  def __init__(self, patience, min_delta):\n",
        "    self.patience = patience\n",
        "    self.min_delta = min_delta\n",
        "    self.counter = 0\n",
        "    self.min_validation = np.inf\n",
        "\n",
        "  def early_stop(self, valid_loss):\n",
        "    if valid_loss < self.min_validation:\n",
        "      self.min_validation = valid_loss\n",
        "      self.counter = 0\n",
        "    elif valid_loss > (self.min_validation + self.min_delta):\n",
        "      self.counter += 1\n",
        "      if self.counter >= self.patience:\n",
        "        return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dHmyfM2Cc9b"
      },
      "outputs": [],
      "source": [
        "early_stopper = EarlyStopping(patience = 15, min_delta = 0.2)\n",
        "early_stopper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0HB4PIc6ixU"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_and_validation(model, train_loader, valid_loader):\n",
        "  # lowest_loss = 1e6\n",
        "  best_f1 = 0.0\n",
        "  for epoch in range(30):\n",
        "    print(\"\\n=============Epoch : \", epoch)\n",
        "    train_epoch(model, train_loader)\n",
        "    valid_loss, valid_pred, valid_gold = valid_epoch(model, valid_loader)\n",
        "\n",
        "    if early_stopper.early_stop(valid_loss):\n",
        "      break\n",
        "\n",
        "    print(\"Length of predictions : \", len(valid_pred))\n",
        "    print(\"Length of gold : \", len(valid_gold))\n",
        "    print(\"Valid loss : \", valid_loss)\n",
        "    print(\"\\n Valid Accuracy : \", accuracy_score(valid_gold, valid_pred))\n",
        "    print(\"\\n Valid Precision : \", precision_score(valid_gold, valid_pred, average = 'weighted'))\n",
        "    print(\"\\n Valid Recall : \", recall_score(valid_gold, valid_pred, average = 'weighted'))\n",
        "    print(\"\\nValid F1 score : \", f1_score(valid_gold, valid_pred, average = 'weighted'))\n",
        "\n",
        "\n",
        "    curr_f1 = f1_score(valid_gold, valid_pred, average = 'weighted')\n",
        "\n",
        "    curr_loss = valid_loss\n",
        "    # if((curr_f1 > best_f1) and (epoch>=4)):\n",
        "    if(curr_f1 > best_f1):\n",
        "    # if(curr_loss < lowest_loss):\n",
        "      best_f1 = curr_f1\n",
        "      # print(\"Valid pred : \", valid_pred)\n",
        "      # print('valid_gold : ', valid_gold)\n",
        "      # torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/roberta/new_seed/best_model_epoch_'+str(epoch)+'_best_f1_'+str(int(best_f1*100))+'_foldNum_'+str(foldNum)+'.pt')\n",
        "      torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/32/saved_model/bert_only/best_model_epoch_'+str(epoch)+'_best_f1_'+str(int(best_f1*100))+'_foldNum_'+str(foldNum)+'.pt')\n",
        "\n",
        "      print(\"model saved\\n\")\n",
        "      # print(\"best model\\n\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMJL4HbkEBh2"
      },
      "outputs": [],
      "source": [
        "# train_and_validation(model, train_loader, test_loader)\n",
        "train_and_validation(model, train_loader, valid_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3o5p6PC96ZJQ"
      },
      "outputs": [],
      "source": [
        "# best_model_epoch_12_best_f1_72_foldNum_0.pt\n",
        "# best_model_epoch_9_best_f1_69_foldNum_1.pt\n",
        "# best_model_epoch_9_best_f1_79_foldNum_2.pt\n",
        "# best_model_epoch_9_best_f1_73_foldNum_3.pt\n",
        "# best_model_epoch_7_best_f1_75_foldNum_4.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DO0Qsq9ufvhi"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/Colab Notebooks/32/saved_model/bert_only/best_model_epoch_19_best_f1_81_foldNum_0.pt'\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAZqaFeUAiMW"
      },
      "outputs": [],
      "source": [
        "# PATH_0 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_12_f1_76.pt'\n",
        "# PATH_1 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_13_f1_78_foldNum_1.pt'\n",
        "# PATH_2 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_9_f1_64_foldNum_2.pt'\n",
        "# PATH_3 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_5_f1_73_foldNum_3.pt'\n",
        "# PATH_4 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_4_f1_74_foldNum_4.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU9qavVgEwJ0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# PATH_0 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_6_f1_50_foldNum_0.pt'\n",
        "# PATH_1 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_4_f1_70_foldNum_1.pt'\n",
        "# PATH_2 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_11_f1_63_foldNum_2.pt'\n",
        "# PATH_3 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_4_f1_44_foldNum_3.pt'\n",
        "# PATH_4 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_4_f1_62_foldNum_4.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PMNVLL3BM-Y"
      },
      "outputs": [],
      "source": [
        "foldNum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_J_qDFj5_ie3"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11LQROttIrMQ"
      },
      "outputs": [],
      "source": [
        "# 2*3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgEpzAt-_mSQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "acc, test_pred, test_gold = test_epoch(model, test_loader)\n",
        "\n",
        "print(acc)\n",
        "\n",
        "print(\"\\nAccuracy : \", accuracy_score(test_gold, test_pred))\n",
        "print(\"\\nPrecision : \", precision_score(test_gold, test_pred, average = 'weighted'))\n",
        "print(\"\\nRecall : \", recall_score(test_gold, test_pred, average = 'weighted'))\n",
        "print(\"\\nF1 score : \", f1_score(test_gold, test_pred, average = 'weighted'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50ctOavcBUOt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTd5vAY4HOp6"
      },
      "outputs": [],
      "source": [
        "# valid_loss, valid_pred, valid_gold = valid_epoch(model, valid_loader)\n",
        "\n",
        "# # print(acc)\n",
        "\n",
        "# print(\"\\nAccuracy : \", accuracy_score(valid_gold, valid_pred))\n",
        "# print(\"\\nPrecision : \", precision_score(valid_gold, valid_pred, average = 'weighted'))\n",
        "# print(\"\\nRecall : \", recall_score(valid_gold, valid_pred, average = 'weighted'))\n",
        "# print(\"\\nF1 score : \", f1_score(valid_gold, valid_pred, average = 'weighted'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eD8Mc-spvSz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3K5movSJPIf"
      },
      "outputs": [],
      "source": [
        "# valid_loss, valid_pred, valid_gold = valid_epoch(model, valid_loader)\n",
        "\n",
        "# # print(acc)\n",
        "\n",
        "# print(\"\\nAccuracy : \", accuracy_score(valid_gold, valid_pred))\n",
        "# print(\"\\nPrecision : \", precision_score(valid_gold, valid_pred))\n",
        "# print(\"\\nRecall : \", recall_score(valid_gold, valid_pred))\n",
        "# print(\"\\nF1 score : \", f1_score(valid_gold, valid_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWNb08EkJPu0"
      },
      "outputs": [],
      "source": [
        "# valid_loss, valid_pred, valid_gold = valid_epoch(model, valid_loader)\n",
        "\n",
        "# # print(acc)\n",
        "\n",
        "# print(\"\\nAccuracy : \", accuracy_score(valid_gold, valid_pred))\n",
        "# print(\"\\nPrecision : \", precision_score(valid_gold, valid_pred, average = 'micro'))\n",
        "# print(\"\\nRecall : \", recall_score(valid_gold, valid_pred, average = 'micro'))\n",
        "# print(\"\\nF1 score : \", f1_score(valid_gold, valid_pred, average = 'micro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ixy_k2L9GgEV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3XQZOkxJ4eP"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# acc, test_pred, test_gold = test_epoch(model, test_loader)\n",
        "\n",
        "# # print(acc)\n",
        "\n",
        "# print(\"\\nAccuracy : \", accuracy_score(test_gold, test_pred))\n",
        "# print(\"\\nPrecision : \", precision_score(test_gold, test_pred))\n",
        "# print(\"\\nRecall : \", recall_score(test_gold, test_pred))\n",
        "# print(\"\\nF1 score : \", f1_score(test_gold, test_pred))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jryD1kV-J5MG"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# acc, test_pred, test_gold = test_epoch(model, test_loader)\n",
        "\n",
        "# # print(acc)\n",
        "\n",
        "# print(\"\\nAccuracy : \", accuracy_score(test_gold, test_pred))\n",
        "# print(\"\\nPrecision : \", precision_score(test_gold, test_pred, average = 'micro'))\n",
        "# print(\"\\nRecall : \", recall_score(test_gold, test_pred, average = 'micro'))\n",
        "# print(\"\\nF1 score : \", f1_score(test_gold, test_pred, average = 'micro'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "993-qFHhZxPp"
      },
      "outputs": [],
      "source": [
        "# test_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPaJ3qk0isrD"
      },
      "outputs": [],
      "source": [
        "# test_gold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94i6uFJtivrG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "gpuClass": "premium"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}