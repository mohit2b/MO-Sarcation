{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHwHkm4rGCLK"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFg6mNX7Gdxg"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.26.1"
      ],
      "metadata": {
        "id": "g3fAxtYIcEwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch -y"
      ],
      "metadata": {
        "id": "5eZP-MmncGe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.13.1"
      ],
      "metadata": {
        "id": "v2zP-uJocH_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQU-d0knGyhl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import warnings\n",
        "import logging\n",
        "import gc\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import ast\n",
        "from tqdm import tqdm\n",
        "from typing import Optional\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import jaccard_score, f1_score, accuracy_score, recall_score, precision_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suRa-hBHG_21"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "THe-J3aMatJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qrnNZdPxXQ3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_Ovlo42G4Kh"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "# from rouge_score.rouge_scorer import RougeScorer\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from transformers import (\n",
        "    BartTokenizerFast,\n",
        "    AdamW\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "    print(\"Using GPU\")\n",
        "\n",
        "else:\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "foldNum = 0\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "SOURCE_MAX_LEN = 500\n",
        "# TARGET_MAX_LEN = 50\n",
        "# MAX_UTTERANCES = 25\n",
        "\n",
        "ACOUSTIC_DIM = 768\n",
        "ACOUSTIC_MAX_LEN = 1000\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "VISUAL_DIM = 2048\n",
        "VISUAL_MAX_LEN = 480\n",
        "\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "\n",
        "VALID_LEN = 69\n",
        "\n",
        "# BASE_LEARNING_RATE = 5e-6\n",
        "# NEW_LEARNING_RATE = 5e-5\n",
        "# WEIGHT_DECAY = 1e-4\n",
        "\n",
        "# NUM_BEAMS = 5\n",
        "# EARLY_STOPPING = True\n",
        "# NO_REPEAT_NGRAM_SIZE = 3\n",
        "\n",
        "# EARLY_STOPPING_THRESHOLD = 5"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fd4fgjzSaL2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMdRaTfAHF7Q"
      },
      "outputs": [],
      "source": [
        "def set_random_seed(seed: int):\n",
        "    print(\"Seed : {}\".format(seed))\n",
        "\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.enabled = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# set_random_seed(42)\n",
        "# a = np.random.randint(0, 1000)\n",
        "\n",
        "# set_random_seed(123)\n",
        "set_random_seed(994)\n",
        "# set_random_seed(12345)\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "\n",
        "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\n",
        "\n",
        "from transformers.modeling_utils import PreTrainedModel, unwrap_model\n",
        "\n",
        "from transformers import (\n",
        "    BartTokenizer,\n",
        "    AdamW\n",
        ")\n",
        "\n",
        "from transformers.models.bart.configuration_bart import BartConfig\n",
        "\n",
        "from transformers.models.bart.modeling_bart import (\n",
        "    BartPretrainedModel,\n",
        "    BartDecoder,\n",
        "    BartModel,\n",
        "    BartLearnedPositionalEmbedding,\n",
        "    BartEncoderLayer,\n",
        "    shift_tokens_right,\n",
        "    _make_causal_mask,\n",
        "    _expand_mask\n",
        ")\n",
        "\n",
        "from transformers.modeling_outputs import (\n",
        "    BaseModelOutput,\n",
        "    Seq2SeqLMOutput,\n",
        "    Seq2SeqModelOutput,\n",
        "    Seq2SeqSequenceClassifierOutput\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers"
      ],
      "metadata": {
        "id": "c9haaIGeWspc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(transformers.__version__)"
      ],
      "metadata": {
        "id": "LTJglj3WWuqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)"
      ],
      "metadata": {
        "id": "PUEQ-CMVWMll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuNh3gFEHQhY"
      },
      "outputs": [],
      "source": [
        "# from transformer_encoder import TransformerEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ML9b3x4axdj"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtwuoBrcwzUN"
      },
      "outputs": [],
      "source": [
        "# bert_tokenizer  = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "# bert_model = BertModel.from_pretrained(\"bert-base-cased\")\n",
        "# # bert_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1XR3BlgHOY8"
      },
      "outputs": [],
      "source": [
        "# BART_INPUTS_DOCSTRING = r\"\"\"\n",
        "#     Args:\n",
        "#         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
        "#             Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n",
        "#             it.\n",
        "#             Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "#             [`PreTrainedTokenizer.__call__`] for details.\n",
        "#             [What are input IDs?](../glossary#input-ids)\n",
        "#         attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "#             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
        "#             - 1 for tokens that are **not masked**,\n",
        "#             - 0 for tokens that are **masked**.\n",
        "#             [What are attention masks?](../glossary#attention-mask)\n",
        "#         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n",
        "#             Indices of decoder input sequence tokens in the vocabulary.\n",
        "#             Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "#             [`PreTrainedTokenizer.__call__`] for details.\n",
        "#             [What are decoder input IDs?](../glossary#decoder-input-ids)\n",
        "#             Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`\n",
        "#             is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).\n",
        "#             For translation and summarization training, `decoder_input_ids` should be provided. If no\n",
        "#             `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right\n",
        "#             for denoising pre-training following the paper.\n",
        "#         decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n",
        "#             Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n",
        "#             be used by default.\n",
        "#             If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]\n",
        "#             and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n",
        "#             information on the default strategy.\n",
        "#         head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n",
        "#             Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:\n",
        "#             - 1 indicates the head is **not masked**,\n",
        "#             - 0 indicates the head is **masked**.\n",
        "#         decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
        "#             Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n",
        "#             - 1 indicates the head is **not masked**,\n",
        "#             - 0 indicates the head is **masked**.\n",
        "#         cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
        "#             Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n",
        "#             1]`:\n",
        "#             - 1 indicates the head is **not masked**,\n",
        "#             - 0 indicates the head is **masked**.\n",
        "#         encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n",
        "#             Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n",
        "#             `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n",
        "#             hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n",
        "#         past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
        "#             Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
        "#             `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n",
        "#             `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
        "#             Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
        "#             blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
        "#             If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
        "#             don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
        "#             `decoder_input_ids` of shape `(batch_size, sequence_length)`. inputs_embeds (`torch.FloatTensor` of shape\n",
        "#             `(batch_size, sequence_length, hidden_size)`, *optional*): Optionally, instead of passing `input_ids` you\n",
        "#             can choose to directly pass an embedded representation. This is useful if you want more control over how to\n",
        "#             convert `input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n",
        "#         decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):\n",
        "#             Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded\n",
        "#             representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be\n",
        "#             input (see `past_key_values`). This is useful if you want more control over how to convert\n",
        "#             `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n",
        "#             If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value\n",
        "#             of `inputs_embeds`.\n",
        "#         use_cache (`bool`, *optional*):\n",
        "#             If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
        "#             `past_key_values`).\n",
        "#         output_attentions (`bool`, *optional*):\n",
        "#             Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
        "#             tensors for more detail.\n",
        "#         output_hidden_states (`bool`, *optional*):\n",
        "#             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
        "#             more detail.\n",
        "#         return_dict (`bool`, *optional*):\n",
        "#             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
        "# \"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEDR_7N6HWsu"
      },
      "outputs": [],
      "source": [
        "# class ContextAwareAttention(nn.Module):\n",
        "\n",
        "#     def __init__(self,\n",
        "#                  dim_model : int,\n",
        "#                  dim_context : int,\n",
        "#                  dropout_rate : Optional[float] = 0.0 ):\n",
        "\n",
        "#         super(ContextAwareAttention, self).__init__()\n",
        "\n",
        "#         self.dim_model = dim_model\n",
        "#         self.dim_context = dim_context\n",
        "#         self.dropout_rate = dropout_rate\n",
        "#         self.attention_layer = nn.MultiheadAttention(embed_dim=self.dim_model,\n",
        "#                                                      num_heads = 1,\n",
        "#                                                      dropout = self.dropout_rate,\n",
        "#                                                      bias = True,\n",
        "#                                                     add_zero_attn=False,\n",
        "#                                                     batch_first=True,\n",
        "#                                                     device=DEVICE\n",
        "#         )\n",
        "\n",
        "#         self.u_k = nn.Linear(self.dim_context, self.dim_model, bias = False)\n",
        "#         self.w1_k = nn.Linear(self.dim_model, 1, bias=False)\n",
        "#         self.w2_k = nn.Linear(self.dim_model, 1, bias=False)\n",
        "\n",
        "#         self.u_v = nn.Linear(self.dim_context, self.dim_model, bias=False)\n",
        "#         self.w1_v = nn.Linear(self.dim_model, 1, bias = False)\n",
        "#         self.w2_v = nn.Linear(self.dim_model, 1, bias = False)\n",
        "\n",
        "#     def forward(self, q, k, v, context):\n",
        "\n",
        "#         # print(\"Context shape : \", context.shape)\n",
        "#         # print(\"Dim context : \", self.dim_context, \" : Dim model : \", self.dim_model)\n",
        "#         key_context = self.u_k(context)\n",
        "#         # print(\"Context shape below key context : \", key_context.shape)\n",
        "#         value_context = self.u_v(context)\n",
        "\n",
        "#         lambda_k = F.sigmoid(self.w1_k(k) + self.w2_k(key_context))\n",
        "#         lambda_v = F.sigmoid(self.w1_v(v) + self.w2_v(value_context))\n",
        "\n",
        "#         k_cap = (1-lambda_k) * k + (lambda_k) * key_context\n",
        "#         v_cap = (1-lambda_v) * v + (lambda_v) * value_context\n",
        "\n",
        "#         attention_output, _ = self.attention_layer(query = q,\n",
        "#                                                    key = k_cap,\n",
        "#                                                    value = v_cap)\n",
        "\n",
        "#         return attention_output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZPn4U3Weq9I"
      },
      "outputs": [],
      "source": [
        "# class MAF_acoustic(nn.Module):\n",
        "#     def __init__(self,\n",
        "#                 dim_model,\n",
        "#                 dropout_rate):\n",
        "#         super(MAF_acoustic, self).__init__()\n",
        "#         self.dropout_rate = dropout_rate\n",
        "\n",
        "#         self.acoustic_context_transform = nn.Linear(ACOUSTIC_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "#         # self.visual_context_transform = nn.Linear(VISUAL_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "\n",
        "#         self.acoustic_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "#                                                                 dim_context=ACOUSTIC_DIM,\n",
        "#                                                                 dropout_rate=dropout_rate)\n",
        "\n",
        "#         # self.visual_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "#         #                                                     dim_context=VISUAL_DIM,\n",
        "#         #                                                     dropout_rate=dropout_rate)\n",
        "\n",
        "#         self.acoustic_gate = nn.Linear(2*dim_model, dim_model)\n",
        "#         # self.visual_gate = nn.Linear(2*dim_model, dim_model)\n",
        "#         self.dropout_layer = nn.Dropout(dropout_rate)\n",
        "#         self.final_layer_norm = nn.LayerNorm(dim_model)\n",
        "\n",
        "#     def forward(self,\n",
        "#                 text_input,\n",
        "#                 acoustic_context):\n",
        "\n",
        "#         # print(\"Acoustic context shape (A) : \", acoustic_context.shape)\n",
        "\n",
        "#         acoustic_context = acoustic_context.permute(0,2,1)\n",
        "#         acoustic_context = self.acoustic_context_transform(acoustic_context.float())\n",
        "#         acoustic_context = acoustic_context.permute(0,2,1)\n",
        "\n",
        "#         audio_out = self.acoustic_context_attention(q=text_input,\n",
        "#                                                     k=text_input,\n",
        "#                                                     v=text_input,\n",
        "#                                                     context=acoustic_context)\n",
        "#         # print(\"Audio out (A) : \", audio_out.shape)\n",
        "\n",
        "#         # print(\"Visual context shape : \", visual_context.shape)\n",
        "#         # visual_context = visual_context.permute(0,2,1)\n",
        "#         # visual_context = self.visual_context_transform(visual_context.float())\n",
        "#         # visual_context = visual_context.permute(0,2,1)\n",
        "\n",
        "#         # video_out = self.visual_context_attention(q=text_input,\n",
        "#         #                                             k=text_input,\n",
        "#         #                                             v=text_input,\n",
        "#         #                                             context=visual_context)\n",
        "\n",
        "#         # print(\"Video out shape : \", video_out.shape)\n",
        "#         # print(\"Text input shape : \", text_input.shape)\n",
        "#         weight_a = F.sigmoid(self.acoustic_gate(torch.cat([text_input, audio_out], dim=-1)))\n",
        "#         # weight_v = F.sigmoid(self.visual_gate(torch.cat([text_input, video_out], dim=-1)))\n",
        "\n",
        "#         # output = self.final_layer_norm(text_input + weight_a * audio_out + weight_v * video_out)\n",
        "\n",
        "#         output = self.final_layer_norm(text_input + weight_a * audio_out)\n",
        "\n",
        "#         return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeZdIlcker1V"
      },
      "outputs": [],
      "source": [
        "# class MAF_visual(nn.Module):\n",
        "#     def __init__(self,\n",
        "#                 dim_model,\n",
        "#                 dropout_rate):\n",
        "#         super(MAF_visual, self).__init__()\n",
        "#         self.dropout_rate = dropout_rate\n",
        "\n",
        "#         # self.acoustic_context_transform = nn.Linear(ACOUSTIC_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "#         self.visual_context_transform = nn.Linear(VISUAL_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "\n",
        "#         # self.acoustic_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "#         #                                                         dim_context=ACOUSTIC_DIM,\n",
        "#         #                                                         dropout_rate=dropout_rate)\n",
        "\n",
        "#         self.visual_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "#                                                             dim_context=VISUAL_DIM,\n",
        "#                                                             dropout_rate=dropout_rate)\n",
        "\n",
        "#         # self.acoustic_gate = nn.Linear(2*dim_model, dim_model)\n",
        "#         self.visual_gate = nn.Linear(2*dim_model, dim_model)\n",
        "#         self.dropout_layer = nn.Dropout(dropout_rate)\n",
        "#         self.final_layer_norm = nn.LayerNorm(dim_model)\n",
        "\n",
        "#     def forward(self,\n",
        "#                 text_input,\n",
        "#                 visual_context):\n",
        "\n",
        "#         # print(\"Acoustic context shape (A) : \", acoustic_context.shape)\n",
        "\n",
        "#         # acoustic_context = acoustic_context.permute(0,2,1)\n",
        "#         # acoustic_context = self.acoustic_context_transform(acoustic_context.float())\n",
        "#         # acoustic_context = acoustic_context.permute(0,2,1)\n",
        "\n",
        "#         # audio_out = self.acoustic_context_attention(q=text_input,\n",
        "#         #                                             k=text_input,\n",
        "#         #                                             v=text_input,\n",
        "#         #                                             context=acoustic_context)\n",
        "#         # print(\"Audio out (A) : \", audio_out.shape)\n",
        "\n",
        "#         # print(\"Visual context shape : \", visual_context.shape)\n",
        "#         visual_context = visual_context.permute(0,2,1)\n",
        "#         visual_context = self.visual_context_transform(visual_context.float())\n",
        "#         visual_context = visual_context.permute(0,2,1)\n",
        "\n",
        "#         video_out = self.visual_context_attention(q=text_input,\n",
        "#                                                     k=text_input,\n",
        "#                                                     v=text_input,\n",
        "#                                                     context=visual_context)\n",
        "\n",
        "#         # print(\"Video out shape : \", video_out.shape)\n",
        "#         # print(\"Text input shape : \", text_input.shape)\n",
        "#         # weight_a = F.sigmoid(self.acoustic_gate(torch.cat([text_input, audio_out], dim=-1)))\n",
        "#         weight_v = F.sigmoid(self.visual_gate(torch.cat([text_input, video_out], dim=-1)))\n",
        "\n",
        "#         # output = self.final_layer_norm(text_input + weight_a * audio_out + weight_v * video_out)\n",
        "\n",
        "#         output = self.final_layer_norm(text_input  + weight_v * video_out)\n",
        "\n",
        "#         return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVX99ZEfRebx"
      },
      "outputs": [],
      "source": [
        "# class CustomTransformerEncoder(torch.nn.Module):\n",
        "#   def __init__(self, vocab_size, hidden_dim, expand_factor):\n",
        "#     super(CustomTransformerEncoder, self).__init__()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmfR5TqF20m4"
      },
      "outputs": [],
      "source": [
        "# class MultiHeadSelfAttention(torch.nn.Module):\n",
        "#   def __init__(self, config):\n",
        "#     super(MultiHeadSelfAttention, self).__init__()\n",
        "\n",
        "#     self.num_attention_heads = 12\n",
        "#     self.attention_head_size = config.d_model//12\n",
        "#     self.embed_dim = config.d_model\n",
        "#     self.dropout = config.dropout\n",
        "\n",
        "#     self.query = nn.Linear(config.d_model, config.d_model)\n",
        "#     self.key = nn.Linear(config.d_model, config.d_model)\n",
        "#     self.value = nn.Linear(config.d_model, config.d_model)\n",
        "#     self.out_proj = nn.Linear(config.d_model, config.d_model)\n",
        "\n",
        "#   def forward(self, hidden_states, attention_mask):\n",
        "#     query_states = self.query(hidden_states)\n",
        "#     key_states = self.key(hidden_states)\n",
        "#     value_states = self.value(hidden_states)\n",
        "\n",
        "#     batch_size = hidden_states.shape[0]\n",
        "#     seq_len = hidden_states.shape[1]\n",
        "\n",
        "#     query_states =  query_states.view(batch_size, seq_len, self.num_attention_heads, self.attention_head_size).transpose(1,2).contiguous()\n",
        "#     key_states =  key_states.view(batch_size, seq_len, self.num_attention_heads, self.attention_head_size).transpose(1,2).contiguous()\n",
        "#     value_states = value_states.view(batch_size, seq_len, self.num_attention_heads, self.attention_head_size).transpose(1,2).contiguous()\n",
        "\n",
        "#     query_states = query_states.view(batch_size*self.num_attention_heads, -1, self.attention_head_size)\n",
        "#     key_states  = key_states.view(batch_size*self.num_attention_heads, -1, self.attention_head_size)\n",
        "#     value_states = value_states.view(batch_size*self.num_attention_heads, -1, self.attention_head_size)\n",
        "\n",
        "#     attn_weights = torch.bmm(query_states, key_states.transpose(1,2))\n",
        "\n",
        "#     attn_weights = attn_weights.view(batch_size, self.num_attention_heads, attn_weights.shape[1], attn_weights.shape[2])\n",
        "#     attn_weights = attn_weights/torch.sqrt(torch.tensor(self.attention_head_size))\n",
        "\n",
        "#     attn_weights_masked = attn_weights.masked_fill(\n",
        "#         attention_mask[:, None, None, :] == False, -1e16\n",
        "#     )\n",
        "\n",
        "#     attn_weights = torch.nn.functional.softmax(attn_weights_masked, dim = -1)\n",
        "\n",
        "#     attn_weights = torch.nn.functional.dropout(attn_weights, p = self.dropout, training = self.training)\n",
        "\n",
        "#     attn_weights = attn_weights.view(batch_size*self.num_attention_heads, seq_len, seq_len)\n",
        "\n",
        "\n",
        "\n",
        "#     attn_output = torch.bmm(attn_weights, value_states)\n",
        "\n",
        "#     attn_output = attn_output.view(batch_size, self.num_attention_heads, seq_len, self.attention_head_size)\n",
        "#     attn_output = attn_output.transpose(1,2)\n",
        "\n",
        "#     attn_output = attn_output.reshape(batch_size, seq_len, self.embed_dim)\n",
        "\n",
        "#     attn_out = self.out_proj(attn_output)\n",
        "\n",
        "#     return attn_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0mlADX3NWj-"
      },
      "outputs": [],
      "source": [
        "# def gelu(input):\n",
        "#   input = torch.tensor(input)\n",
        "#   return input * 0.5 * (1.0 + torch.erf(input/torch.sqrt(torch.tensor(2.0))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCJvPt5hFS3X"
      },
      "outputs": [],
      "source": [
        "# class CustomEncoder(torch.nn.Module):\n",
        "#   def __init__(self, config):\n",
        "#     super(CustomEncoder, self).__init__()\n",
        "#     self.embed_dim = config.d_model\n",
        "#     self.self_attn = MultiHeadSelfAttention(config)\n",
        "#     self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "#     self.dropout = config.dropout\n",
        "#     # self.activation_fn = torch.nn.ReLU()\n",
        "#     self.activation_dropout = config.activation_dropout\n",
        "#     self.fc1 = nn.Linear(self.embed_dim, 2*self.embed_dim)\n",
        "#     self.fc2 = nn.Linear(2*self.embed_dim, self.embed_dim)\n",
        "#     self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "\n",
        "#   def forward(self, hidden_states, attention_mask):\n",
        "#    residual = hidden_states\n",
        "#    hidden_states =  self.self_attn(hidden_states = hidden_states, attention_mask = attention_mask)\n",
        "#    hidden_states = nn.functional.dropout(hidden_states, p = self.dropout, training = self.training)\n",
        "#    hidden_states = residual + hidden_states\n",
        "#    hidden_states = self.self_attn_layer_norm(hidden_states)\n",
        "\n",
        "#    residual = hidden_states\n",
        "#    hidden_states = gelu(self.fc1(hidden_states))\n",
        "#    hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training = self.training)\n",
        "#    hidden_states = self.fc2(hidden_states)\n",
        "#    hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training = self.training)\n",
        "#    hidden_states = residual + hidden_states\n",
        "#    hidden_states = self.final_layer_norm(hidden_states)\n",
        "\n",
        "#    if hidden_states.dtype == torch.float16 and (\n",
        "#        torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n",
        "#    ):\n",
        "#       clamp_value = torch.finfo(hidden_states.dtype).max = -1000\n",
        "#       hidden_states = torch.clamp(hidden_states, min = -clamp_value, max = clamp_value)\n",
        "\n",
        "#    return hidden_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEWqLUTHbj0t"
      },
      "outputs": [],
      "source": [
        "# class ContextEncoder(nn.Module):\n",
        "#   def __init__(self, config, num_layers):\n",
        "#     super(ContextEncoder, self).__init__()\n",
        "\n",
        "#     self.word_embeddings = nn.Embedding(config.vocab_size, config.d_model, padding_idx = config.pad_token_id)\n",
        "#     self.position_embedding = nn.Embedding(500, config.d_model)\n",
        "#     self.layer_norm = nn.LayerNorm(config.d_model)\n",
        "#     self.context_encoder = torch.nn.ModuleList([CustomEncoder(config) for _ in range(num_layers)])\n",
        "#     self.context_gate = nn.Linear(2*config.d_model, config.d_model)\n",
        "\n",
        "\n",
        "#   def forward(self, hidden_states, context_input_ids, context_attention_mask) :\n",
        "\n",
        "#     seq_len = context_input_ids.shape[1]\n",
        "#     pos = torch.arange(seq_len, dtype = torch.long)\n",
        "#     pos = pos.unsqueeze(0).expand_as(context_input_ids)\n",
        "#     pos = pos.to(device)\n",
        "#     context_state = self.word_embeddings(context_input_ids) + self.position_embedding(pos)\n",
        "\n",
        "#     for layer in self.context_encoder:\n",
        "#             context_state = layer(hidden_states = context_state, attention_mask  = context_attention_mask)\n",
        "\n",
        "\n",
        "#     weight_c = F.sigmoid(self.context_gate(torch.cat([hidden_states, context_state], dim=-1)))\n",
        "#     output = self.layer_norm(hidden_states + weight_c * context_state)\n",
        "#     # print('transformer context shape :', output.shape)\n",
        "#     return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Oa75YDExmAz"
      },
      "outputs": [],
      "source": [
        "# class ContextEncoder(nn.Module):\n",
        "#   def __init__(self, config):\n",
        "#     super(ContextEncoder, self).__init__()\n",
        "\n",
        "#     self.bert = bert_model\n",
        "#     self.layer_norm = nn.LayerNorm(config.d_model)\n",
        "#     self.context_gate = nn.Linear(2*config.d_model, config.d_model)\n",
        "\n",
        "\n",
        "#   def forward(self, hidden_states, context_input_ids, context_attention_mask) :\n",
        "\n",
        "#     output_encoding = self.bert(context_input_ids, context_attention_mask)\n",
        "\n",
        "#     context_state = output_encoding['last_hidden_state']\n",
        "\n",
        "\n",
        "#     weight_c = F.sigmoid(self.context_gate(torch.cat([hidden_states, context_state], dim=-1)))\n",
        "#     output = self.layer_norm(hidden_states + weight_c * context_state)\n",
        "#     # print('transformer context shape :', output.shape)\n",
        "#     return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oESmDliml6-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s88m19VoHbj5"
      },
      "outputs": [],
      "source": [
        "# class MultiModalBartEncoder(BartPretrainedModel):\n",
        "\n",
        "#     def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = None):\n",
        "#         super().__init__(config)\n",
        "\n",
        "#         self.dropout = config.dropout\n",
        "#         self.layerdrop = config.encoder_layerdrop\n",
        "\n",
        "#         embed_dim = config.d_model\n",
        "#         self.padding_idx = config.pad_token_id\n",
        "#         self.max_source_position = config.max_position_embeddings\n",
        "#         self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n",
        "\n",
        "#         if embed_tokens is not None:\n",
        "#             self.embed_tokens = embed_tokens\n",
        "#         else:\n",
        "#             self.embed_tokens = nn.Embedding(config.vocab_size, embed_dim, self.padding_idx)\n",
        "\n",
        "#         self.embed_positions = BartLearnedPositionalEmbedding(\n",
        "#             config.max_position_embeddings,\n",
        "#             embed_dim\n",
        "#         )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#         self.layers = nn.ModuleList([BartEncoderLayer(config) for _ in range(config.encoder_layers)])\n",
        "\n",
        "#         self.layernorm_embedding = nn.LayerNorm(embed_dim)\n",
        "\n",
        "#         self.init_weights()\n",
        "#         self.gradient_checkpointing = False\n",
        "\n",
        "#         # self.fusion_at_layer = [4]\n",
        "#         # self.fusion_at_layer = [3, 4]\n",
        "#         # self.fusion_at_layer3 = [3]\n",
        "#         self.fusion_at_layer4 = [4]\n",
        "#         self.fusion_at_layer5 = [5]\n",
        "\n",
        "#         # self.fusion_of_context = [3]\n",
        "#         # self.visual_transformer = TransformerEncoder(d_model = VISUAL_DIM,\n",
        "#         #                                              n_layers = 4,\n",
        "#         #                                              n_heads=8,\n",
        "#         #                                              d_ff=VISUAL_DIM\n",
        "#         #                                              )\n",
        "#         # self.acoustic_transformer = TransformerEncoder(d_model = ACOUSTIC_DIM,\n",
        "#         #                                                n_layers=4,\n",
        "#         #                                                n_heads=2,\n",
        "#         #                                                d_ff=ACOUSTIC_DIM)\n",
        "\n",
        "#         # self.MAF_layer3 = MAF(dim_model=embed_dim,\n",
        "#         #                      dropout_rate=0.2)\n",
        "\n",
        "#         self.MAF_layer4 = MAF_acoustic(dim_model=embed_dim,\n",
        "#                              dropout_rate=0.2)\n",
        "\n",
        "#         self.MAF_layer5 = MAF_visual(dim_model=embed_dim,\n",
        "#                              dropout_rate=0.2)\n",
        "\n",
        "#         # self.context_encoder = ContextEncoder(config)\n",
        "\n",
        "#         # self.classification = nn.Linear(embed_dim, 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#     def forward(self,\n",
        "#             input_ids = None,\n",
        "#             attention_mask = None,\n",
        "#             # context_input_ids = None,\n",
        "#             # context_attention_mask = None,\n",
        "#             acoustic_input = None,\n",
        "#             visual_input = None,\n",
        "#             head_mask = None,\n",
        "#             inputs_embeds = None,\n",
        "#             output_attentions = None,\n",
        "#             output_hidden_states  = None,\n",
        "#             return_dict = None):\n",
        "\n",
        "#             # print(\"Input ids shape : \", input_ids.shape)\n",
        "#             output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "\n",
        "#             output_hidden_states = (\n",
        "#                 output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "#             )\n",
        "\n",
        "#             return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "#             if input_ids is not None and inputs_embeds is not None:\n",
        "#                 raise ValueError(\"You can't specify both input_ids and inputs_embeds at the same time\")\n",
        "#             elif input_ids is not None:\n",
        "#                 input_shape = input_ids.size()\n",
        "#                 input_ids = input_ids.view(-1, input_shape[-1])\n",
        "\n",
        "#             elif inputs_embeds is not None:\n",
        "#                 input_shape = inputs_embeds.size()[:-1]\n",
        "#             else:\n",
        "#                 raise ValueError(\"You have to specify either input_ids or input_embeds\")\n",
        "\n",
        "\n",
        "#             if inputs_embeds is None:\n",
        "#                 # print(\"Input ids shape : \", input_ids.shape)\n",
        "#                 inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
        "\n",
        "#             # print(\"Input shape type : \", type(input_shape))\n",
        "#             # print(\"Input shape : \", input_shape)\n",
        "#             input_shape = torch.tensor(input_shape)\n",
        "#             # print(\"Input shape type : \", type(input_shape))\n",
        "#             # print(\"Input shape : \", input_shape)\n",
        "#             # print(\"Input shape : \", input_shape.shape)\n",
        "#             embed_pos = self.embed_positions(input_ids)\n",
        "#             # embed_pos = self.embed_positions(input_shape)\n",
        "\n",
        "\n",
        "#             hidden_states = inputs_embeds + embed_pos\n",
        "#             hidden_states = self.layernorm_embedding(hidden_states)\n",
        "#             hidden_states = F.dropout(hidden_states, p = self.dropout, training=self.training)\n",
        "\n",
        "#             # print(\"attention mask shape 3 : \", attention_mask.shape)\n",
        "#             if attention_mask is not None:\n",
        "#                 attention_mask =  _expand_mask(attention_mask, inputs_embeds.dtype)\n",
        "\n",
        "#             # print(\"attention mask shape 4 : \", attention_mask.shape)\n",
        "#             encoder_states = () if output_hidden_states else None\n",
        "#             all_attentions = () if output_attentions else None\n",
        "\n",
        "#             if head_mask is not None:\n",
        "#                 assert head_mask.size()[0] == (\n",
        "#                     len(self.layers)\n",
        "#                 ), f\"The head mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n",
        "\n",
        "#             for idx, encoder_layer in enumerate(self.layers):\n",
        "#                 # print(\"============Idx : \", idx)\n",
        "\n",
        "#                 # if idx in self.fusion_at_layer3:\n",
        "#                 #     # print(\"Acoustic input shape (B) : \", acoustic_input)\n",
        "#                 #     # acoustic_input = self.acoustic_transformer(acoustic_input)[-1]\n",
        "#                 #     # print(\"Acoustic input shape (C) : \", acoustic_input)\n",
        "\n",
        "#                 #     # visual_input = self.visual_transformer(visual_input)[-1]\n",
        "#                 #     # print(\"====Idx inside fusion at layer :\", idx)\n",
        "#                 #     hidden_states = self.MAF_layer3(text_input = hidden_states,\n",
        "#                 #                                    acoustic_context = acoustic_input,\n",
        "#                 #                                    visual_context = visual_input)\n",
        "\n",
        "#                 # if idx in self.fusion_of_context:\n",
        "\n",
        "#                 #   hidden_states = self.context_encoder(hidden_states = hidden_states, context_input_ids = context_input_ids, context_attention_mask = context_attention_mask)\n",
        "\n",
        "\n",
        "#                 if idx in self.fusion_at_layer4:\n",
        "#                     # print(\"Acoustic input shape (B) : \", acoustic_input)\n",
        "#                     # acoustic_input = self.acoustic_transformer(acoustic_input)[-1]\n",
        "#                     # print(\"Acoustic input shape (C) : \", acoustic_input)\n",
        "\n",
        "#                     # visual_input = self.visual_transformer(visual_input)[-1]\n",
        "#                     # print(\"====Idx inside fusion at layer :\", idx)\n",
        "\n",
        "\n",
        "\n",
        "#                     hidden_states = self.MAF_layer4(text_input = hidden_states,\n",
        "#                                                    acoustic_context = acoustic_input\n",
        "#                                                    )\n",
        "#                 if idx in self.fusion_at_layer5:\n",
        "#                     # print(\"Acoustic input shape (B) : \", acoustic_input)\n",
        "#                     # acoustic_input = self.acoustic_transformer(acoustic_input)[-1]\n",
        "#                     # print(\"Acoustic input shape (C) : \", acoustic_input)\n",
        "\n",
        "#                     # visual_input = self.visual_transformer(visual_input)[-1]\n",
        "#                     # print(\"====Idx inside fusion at layer :\", idx)\n",
        "\n",
        "\n",
        "\n",
        "#                     hidden_states = self.MAF_layer5(text_input = hidden_states,\n",
        "#                                                    visual_context = visual_input)\n",
        "\n",
        "#                 if output_hidden_states:\n",
        "#                     encoder_states = encoder_states + (hidden_states,)\n",
        "\n",
        "#                 dropout_probability = random.uniform(0,1)\n",
        "\n",
        "#                 if self.training and (dropout_probability < self.layerdrop):\n",
        "#                     layer_outputs = (None, None)\n",
        "\n",
        "#                 else:\n",
        "#                     if self.gradient_checkpointing and self.training:\n",
        "\n",
        "#                         def create_custom_forward(module):\n",
        "#                             def custom_forward(*inputs):\n",
        "#                                 return module(*inputs, output_attentions)\n",
        "\n",
        "#                             return custom_forward\n",
        "\n",
        "#                         layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "#                             create_custom_forward(encoder_layer),\n",
        "#                             hidden_states,\n",
        "#                             attention_mask,\n",
        "#                             (head_mask[idx] if head_mask is not None else None),\n",
        "#                         )\n",
        "\n",
        "#                     else:\n",
        "#                         # print(\"Checking Attention mask shape : \", attention_mask.shape)\n",
        "#                         layer_outputs = encoder_layer(\n",
        "#                             hidden_states,\n",
        "#                             attention_mask,\n",
        "#                             layer_head_mask = (head_mask[idx] if head_mask is not None else None),\n",
        "#                             output_attentions = output_attentions\n",
        "#                         )\n",
        "\n",
        "#                     hidden_states = layer_outputs[0]\n",
        "\n",
        "#                 if output_attentions:\n",
        "#                     all_attentions  = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "#             if output_hidden_states:\n",
        "#                 encoder_states = encoder_states + (hidden_states,)\n",
        "\n",
        "#             if not return_dict:\n",
        "#                 return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n",
        "\n",
        "#             return BaseModelOutput(\n",
        "#                 last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n",
        "#             )\n",
        "\n",
        "#             # print(\"Hidden states shape : \", hidden_states)\n",
        "\n",
        "#             # cls = hidden_states.permute(1,0,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6F_nfly0HgKR"
      },
      "outputs": [],
      "source": [
        "# class MultimodalBartModel(BartPretrainedModel):\n",
        "#     def __init__(self, config: BartConfig):\n",
        "#         super().__init__(config)\n",
        "\n",
        "#         padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n",
        "#         self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n",
        "\n",
        "#         self.encoder = MultiModalBartEncoder(config, self.shared)\n",
        "#         self.decoder = BartDecoder(config, self.shared)\n",
        "\n",
        "#         self.init_weights()\n",
        "\n",
        "#     def get_input_embeddings(self):\n",
        "#         return self.shared\n",
        "\n",
        "#     def set_input_embeddings(self, value):\n",
        "#         self.shared = value\n",
        "#         self.encoder.embed_tokens = self.shared\n",
        "#         self.decoder.embed_tokens = self.shared\n",
        "\n",
        "#     def get_encoder(self):\n",
        "#         return self.encoder\n",
        "\n",
        "\n",
        "#     def get_decoder(self):\n",
        "#         return self.decoder\n",
        "\n",
        "#     def forward(\n",
        "#         self,\n",
        "#         input_ids = None,\n",
        "#         attention_mask = None,\n",
        "#         # context_input_ids = None,\n",
        "#         # context_attention_mask = None,\n",
        "#         acoustic_input = None,\n",
        "#         visual_input = None,\n",
        "#         decoder_input_ids = None,\n",
        "#         decoder_attention_mask = None,\n",
        "#         head_mask = None,\n",
        "#         decoder_head_mask = None,\n",
        "#         cross_attn_head_mask = None,\n",
        "#         encoder_outputs = None,\n",
        "#         past_key_values = None,\n",
        "#         inputs_embeds = None,\n",
        "#         decoder_inputs_embeds = None,\n",
        "#         use_cache = None,\n",
        "#         output_attentions = None,\n",
        "#         output_hidden_states = None,\n",
        "#         return_dict = None\n",
        "#     ):\n",
        "\n",
        "#         if decoder_input_ids is None and decoder_inputs_embeds is None:\n",
        "#             decoder_input_ids = shift_tokens_right(\n",
        "#                 input_ids, self.config.pad_token_id, self.config.decoder_start_token_id\n",
        "\n",
        "#             )\n",
        "\n",
        "#         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "#         output_hidden_states = (\n",
        "#             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "#         )\n",
        "#         use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "#         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "#         # print(\"attention mask shape 2 : \", attention_mask.shape)\n",
        "\n",
        "#         if encoder_outputs is None:\n",
        "#             encoder_outputs = self.encoder(\n",
        "#                 input_ids = input_ids,\n",
        "#                 attention_mask = attention_mask,\n",
        "#                 # context_input_ids = context_input_ids,\n",
        "#                 # context_attention_mask = context_attention_mask,\n",
        "#                 acoustic_input = acoustic_input,\n",
        "#                 visual_input = visual_input,\n",
        "#                 head_mask = head_mask,\n",
        "#                 inputs_embeds = inputs_embeds,\n",
        "#                 output_attentions = output_attentions,\n",
        "#                 output_hidden_states = output_hidden_states,\n",
        "#                 return_dict = return_dict\n",
        "#             )\n",
        "\n",
        "#         elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
        "#             encoder_outputs = BaseModelOutput(\n",
        "#                 last_hidden_state=encoder_outputs[0],\n",
        "#                 hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
        "#                 attentions = encoder_outputs[2] if len(encoder_outputs) > 2 else None\n",
        "#             )\n",
        "\n",
        "#         decoder_outputs = self.decoder(\n",
        "#             input_ids = decoder_input_ids,\n",
        "#             attention_mask = decoder_attention_mask,\n",
        "#             encoder_hidden_states = encoder_outputs[0],\n",
        "#             encoder_attention_mask = attention_mask,\n",
        "#             head_mask = decoder_head_mask,\n",
        "#             cross_attn_head_mask = cross_attn_head_mask,\n",
        "#             past_key_values = past_key_values,\n",
        "#             inputs_embeds = decoder_inputs_embeds,\n",
        "#             use_cache = use_cache,\n",
        "#             output_attentions = output_attentions,\n",
        "#             output_hidden_states = output_hidden_states,\n",
        "#             return_dict = return_dict\n",
        "#         )\n",
        "\n",
        "#         if not return_dict:\n",
        "#             return decoder_outputs + encoder_outputs\n",
        "\n",
        "#         return Seq2SeqModelOutput(\n",
        "#             last_hidden_state=decoder_outputs.last_hidden_state,\n",
        "#             past_key_values=decoder_outputs.past_key_values,\n",
        "#             decoder_hidden_states=decoder_outputs.hidden_states,\n",
        "#             decoder_attentions=decoder_outputs.attentions,\n",
        "#             cross_attentions=decoder_outputs.cross_attentions,\n",
        "#             encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
        "#             encoder_hidden_states=encoder_outputs.hidden_states,\n",
        "#             encoder_attentions=encoder_outputs.attentions\n",
        "\n",
        "#         )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VO1KdKseHlkn"
      },
      "outputs": [],
      "source": [
        "# class MultimodalBartClassification(nn.Module):\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         input_dim: int,\n",
        "#         inned_dim: int,\n",
        "#         num_classes: int,\n",
        "#         pooler_dropout: float\n",
        "#     ):\n",
        "#         super().__init__()\n",
        "#         self.dense = nn.Linear(input_dim, inned_dim)\n",
        "#         self.dropout = nn.Dropout(p = pooler_dropout)\n",
        "#         self.out_proj = nn.Linear(inned_dim, num_classes)\n",
        "\n",
        "#     def forward(self, hidden_states):\n",
        "#         hidden_states = self.dropout(hidden_states)\n",
        "#         hidden_states = self.dense(hidden_states)\n",
        "#         hidden_states = torch.tanh(hidden_states)\n",
        "#         hidden_states = self.dropout(hidden_states)\n",
        "#         hidden_states = self.out_proj(hidden_states)\n",
        "#         return hidden_states\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# class MultimodalBartForSequenceClassification(BartPretrainedModel):\n",
        "#     _keys_to_ignore_on_load_missing = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n",
        "\n",
        "#     def __init__(self, config: BartConfig, **kwargs):\n",
        "#         super().__init__(config, **kwargs)\n",
        "#         self.model = MultimodalBartModel(config)\n",
        "#         self.classification_head = MultimodalBartClassification(\n",
        "#             config.d_model,\n",
        "#             config.d_model,\n",
        "#             2,\n",
        "#             config.classifier_dropout\n",
        "#         )\n",
        "#         self.model._init_weights(self.classification_head.dense)\n",
        "#         self.model._init_weights(self.classification_head.out_proj)\n",
        "\n",
        "#     def forward(\n",
        "#         self,\n",
        "#         input_ids: torch.LongTensor = None,\n",
        "#         attention_mask: Optional[torch.tensor] = None,\n",
        "#         # context_input_ids : torch.LongTensor = None,\n",
        "#         # context_attention_mask : Optional[torch.tensor] = None,\n",
        "#         acoustic_input = None,\n",
        "#         visual_input = None,\n",
        "#         decoder_input_ids: Optional[torch.LongTensor] = None,\n",
        "#         decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
        "#         head_mask: Optional[torch.Tensor] = None,\n",
        "#         decoder_head_mask: Optional[torch.Tensor] = None,\n",
        "#         cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
        "#         encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n",
        "#         inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "#         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "#         labels: Optional[torch.LongTensor] = None,\n",
        "#         use_cache: Optional[bool] = None,\n",
        "#         output_attentions: Optional[bool] = None,\n",
        "#         output_hidden_states: Optional[bool] = None,\n",
        "#         return_dict: Optional[bool] = None\n",
        "#     ) -> Union[Tuple, Seq2SeqSequenceClassifierOutput]:\n",
        "#         r\"\"\"\n",
        "#         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
        "#             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
        "#             config.num_labels - 1]`. If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "#         \"\"\"\n",
        "\n",
        "#         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "#         if labels is not None:\n",
        "#             use_cache = False\n",
        "\n",
        "#         if input_ids is None and inputs_embeds is not None:\n",
        "#             raise NotImplementedError(\n",
        "#                 f\"Passing input embeddings is currently not supported for {self.__class__.__name__}\"\n",
        "#             )\n",
        "\n",
        "#         # print(\"attention mask shape 1 : \", attention_mask.shape )\n",
        "#         outputs = self.model(\n",
        "#             input_ids,\n",
        "#             attention_mask = attention_mask,\n",
        "#             # context_input_ids = context_input_ids,\n",
        "#             # context_attention_mask = context_attention_mask,\n",
        "#             acoustic_input = acoustic_input,\n",
        "#             visual_input = visual_input,\n",
        "#             decoder_input_ids = decoder_input_ids,\n",
        "#             decoder_attention_mask = decoder_attention_mask,\n",
        "#             head_mask = head_mask,\n",
        "#             decoder_head_mask = decoder_head_mask,\n",
        "#             cross_attn_head_mask = cross_attn_head_mask,\n",
        "#             encoder_outputs = encoder_outputs,\n",
        "#             inputs_embeds = inputs_embeds,\n",
        "#             decoder_inputs_embeds = decoder_inputs_embeds,\n",
        "#             use_cache = use_cache,\n",
        "#             output_attentions = output_attentions,\n",
        "#             output_hidden_states = output_hidden_states,\n",
        "#             return_dict = return_dict\n",
        "\n",
        "#         )\n",
        "\n",
        "#         hidden_states = outputs[0]\n",
        "\n",
        "#         eos_mask = input_ids.eq(self.config.eos_token_id).to(hidden_states.device)\n",
        "\n",
        "#         if len(torch.unique_consecutive(eos_mask.sum(1))) > 1:\n",
        "#             raise ValueError(\"All examples must have same number of <eos> tokens\")\n",
        "\n",
        "#         sentence_representation = hidden_states[eos_mask, :].view(hidden_states.size(0), -1, hidden_states.size(-1))[\n",
        "#             :, -1, :\n",
        "#         ]\n",
        "\n",
        "#         logits = self.classification_head(sentence_representation)\n",
        "\n",
        "#         loss = None\n",
        "\n",
        "#         loss_fct = CrossEntropyLoss()\n",
        "#         # print(\"logits shape : \", logits.shape)\n",
        "#         loss = loss_fct(logits.view(-1, 2), labels.view(-1))\n",
        "#         # print(\"Loss : \", loss)\n",
        "#         if not return_dict:\n",
        "#             output = (logits,) + outputs[1:]\n",
        "#             return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "\n",
        "#         return Seq2SeqSequenceClassifierOutput(\n",
        "#             loss=loss,\n",
        "#             logits=logits,\n",
        "#             past_key_values=outputs.past_key_values,\n",
        "#             decoder_hidden_states=outputs.decoder_hidden_states,\n",
        "#             decoder_attentions = outputs.decoder_attentions,\n",
        "#             cross_attentions=outputs.cross_attentions,\n",
        "#             encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n",
        "#             encoder_hidden_states=outputs.encoder_hidden_states,\n",
        "#             encoder_attentions=outputs.encoder_attentions\n",
        "#         )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4lcNczUHpOH"
      },
      "outputs": [],
      "source": [
        "# def audio_video_broadcast(x):\n",
        "# #     z = torch.empty()\n",
        "#     temp_all = torch.Tensor()\n",
        "#     for j in range(x.shape[0]):\n",
        "#         print(\"j : \", j)\n",
        "#         temp_x = x[j,:]\n",
        "# #         print(\"Temp x shape : \", temp_x.shape)\n",
        "#         temp_x = torch.tensor(temp_x, dtype=torch.float)\n",
        "#         temp_x = torch.broadcast_to(temp_x, (SOURCE_MAX_LEN, temp_x.shape[0]))\n",
        "# #         print(\"Temp x shape : \", temp_x.shape)\n",
        "#         temp_x = temp_x.unsqueeze(0)\n",
        "# #         print(\"Temp x shape : \", temp_x.shape)\n",
        "\n",
        "#         if(j==0):\n",
        "#             temp_all = temp_x\n",
        "#         else:\n",
        "#             temp_all = torch.cat([temp_all, temp_x], dim = 0)\n",
        "\n",
        "\n",
        "#     return temp_all"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
        "bart_model = BartModel.from_pretrained('facebook/bart-base')\n",
        "\n",
        "p = {\n",
        "        'additional_special_tokens' : ['[CONTEXT]', '[UTTERANCE]']\n",
        "    }\n",
        "\n",
        "tokenizer.add_special_tokens(p)\n",
        "\n",
        "bart_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "# outputs = model(**inputs)"
      ],
      "metadata": {
        "id": "pshLRQxd2_Pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs.keys()"
      ],
      "metadata": {
        "id": "OvT88kvIF24N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# outputs.keys()"
      ],
      "metadata": {
        "id": "DSkqbNaGFxAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7Z30bGs2_V6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVQvabBN2_V6"
      },
      "outputs": [],
      "source": [
        "# class MAF_one(nn.Module):\n",
        "#     def __init__(self,\n",
        "#                 dim_model,\n",
        "#                 dropout_rate):\n",
        "#         super(MAF_one, self).__init__()\n",
        "#         self.dropout_rate = dropout_rate\n",
        "\n",
        "\n",
        "#         # self.visual_context_transform = nn.Linear(VISUAL_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "\n",
        "#         self.acoustic_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "#                                                                 # dim_context=ACOUSTIC_DIM,\n",
        "#                                                                 dim_context = dim_model,\n",
        "#                                                                 dropout_rate=dropout_rate)\n",
        "\n",
        "#         # self.visual_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "#         #                                                     dim_context=VISUAL_DIM,\n",
        "#         #                                                     dropout_rate=dropout_rate)\n",
        "\n",
        "#         self.acoustic_gate = nn.Linear(2*dim_model, dim_model)\n",
        "#         # self.visual_gate = nn.Linear(2*dim_model, dim_model)\n",
        "#         self.dropout_layer = nn.Dropout(dropout_rate)\n",
        "#         self.final_layer_norm = nn.LayerNorm(dim_model)\n",
        "\n",
        "#     def forward(self,\n",
        "#                 text_input,\n",
        "#                 # acoustic_context\n",
        "#                 acoustic_input):\n",
        "\n",
        "#         # print(\"Acoustic context shape (A) : \", acoustic_context.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#         # audio_out = self.acoustic_context_attention(q=text_input,\n",
        "#         #                                             k=text_input,\n",
        "#         #                                             v=text_input,\n",
        "#         #                                             context=acoustic_context)\n",
        "\n",
        "#         text_out = self.acoustic_context_attention(q=acoustic_input,\n",
        "#                                                     k=acoustic_input,\n",
        "#                                                     v=acoustic_input,\n",
        "#                                                     context=text_input)\n",
        "#         # print(\"Audio out (A) : \", audio_out.shape)\n",
        "\n",
        "#         # print(\"Visual context shape : \", visual_context.shape)\n",
        "#         # visual_context = visual_context.permute(0,2,1)\n",
        "#         # visual_context = self.visual_context_transform(visual_context.float())\n",
        "#         # visual_context = visual_context.permute(0,2,1)\n",
        "\n",
        "#         # video_out = self.visual_context_attention(q=text_input,\n",
        "#         #                                             k=text_input,\n",
        "#         #                                             v=text_input,\n",
        "#         #                                             context=visual_context)\n",
        "\n",
        "#         # print(\"Video out shape : \", video_out.shape)\n",
        "#         # print(\"Text input shape : \", text_input.shape)\n",
        "#         # weight_a = F.sigmoid(self.acoustic_gate(torch.cat([text_input, audio_out], dim=-1)))\n",
        "#         weight_a = F.sigmoid(self.acoustic_gate(torch.cat([text_out, acoustic_input], dim=-1)))\n",
        "#         # weight_v = F.sigmoid(self.visual_gate(torch.cat([text_input, video_out], dim=-1)))\n",
        "\n",
        "#         # output = self.final_layer_norm(text_input + weight_a * audio_out + weight_v * video_out)\n",
        "\n",
        "#         # output = self.final_layer_norm(text_input + weight_a * audio_out)\n",
        "#         output = self.final_layer_norm(weight_a * text_out +  acoustic_input)\n",
        "\n",
        "#         return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdSjcJSD2_V7"
      },
      "outputs": [],
      "source": [
        "# class MAF_two(nn.Module):\n",
        "#     def __init__(self,\n",
        "#                 dim_model,\n",
        "#                 dropout_rate):\n",
        "#         super(MAF_two, self).__init__()\n",
        "#         self.dropout_rate = dropout_rate\n",
        "\n",
        "#         # self.acoustic_context_transform = nn.Linear(ACOUSTIC_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "\n",
        "#         # self.acoustic_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "#         #                                                         dim_context=ACOUSTIC_DIM,\n",
        "#         #                                                         dropout_rate=dropout_rate)\n",
        "\n",
        "#         # self.visual_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "#         #                                                     dim_context=VISUAL_DIM,\n",
        "#         #                                                     dropout_rate=dropout_rate)\n",
        "#         self.visual_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "#                                                             dim_context=dim_model,\n",
        "#                                                             dropout_rate=dropout_rate)\n",
        "\n",
        "#         # self.acoustic_gate = nn.Linear(2*dim_model, dim_model)\n",
        "#         self.visual_gate = nn.Linear(2*dim_model, dim_model)\n",
        "#         self.dropout_layer = nn.Dropout(dropout_rate)\n",
        "#         self.final_layer_norm = nn.LayerNorm(dim_model)\n",
        "\n",
        "#     def forward(self,\n",
        "#                 text_input,\n",
        "#                 # visual_context\n",
        "#                 visual_input):\n",
        "\n",
        "#         # print(\"Acoustic context shape (A) : \", acoustic_context.shape)\n",
        "\n",
        "#         # acoustic_context = acoustic_context.permute(0,2,1)\n",
        "#         # acoustic_context = self.acoustic_context_transform(acoustic_context.float())\n",
        "#         # acoustic_context = acoustic_context.permute(0,2,1)\n",
        "\n",
        "#         # audio_out = self.acoustic_context_attention(q=text_input,\n",
        "#         #                                             k=text_input,\n",
        "#         #                                             v=text_input,\n",
        "#         #                                             context=acoustic_context)\n",
        "#         # print(\"Audio out (A) : \", audio_out.shape)\n",
        "\n",
        "#         # print(\"Visual context shape : \", visual_context.shape)\n",
        "\n",
        "\n",
        "#         text_out = self.visual_context_attention(q=visual_input,\n",
        "#                                                     k=visual_input,\n",
        "#                                                     v=visual_input,\n",
        "#                                                     context=text_input)\n",
        "\n",
        "#         # print(\"Video out shape : \", video_out.shape)\n",
        "#         # print(\"Text input shape : \", text_input.shape)\n",
        "#         # weight_a = F.sigmoid(self.acoustic_gate(torch.cat([text_input, audio_out], dim=-1)))\n",
        "#         # weight_v = F.sigmoid(self.visual_gate(torch.cat([text_input, video_out], dim=-1)))\n",
        "#         weight_v = F.sigmoid(self.visual_gate(torch.cat([text_out, visual_input], dim=-1)))\n",
        "\n",
        "#         # output = self.final_layer_norm(text_input + weight_a * audio_out + weight_v * video_out)\n",
        "\n",
        "#         output = self.final_layer_norm(weight_v * text_out  + visual_input)\n",
        "\n",
        "#         return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ContextAwareAttention(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 dim_model : int,\n",
        "                 dim_context : int,\n",
        "                 dropout_rate : Optional[float] = 0.0 ):\n",
        "\n",
        "        super(ContextAwareAttention, self).__init__()\n",
        "\n",
        "        self.dim_model = dim_model\n",
        "        self.dim_context = dim_context\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.attention_layer = nn.MultiheadAttention(embed_dim=self.dim_model,\n",
        "                                                     num_heads = 1,\n",
        "                                                     dropout = self.dropout_rate,\n",
        "                                                     bias = True,\n",
        "                                                    add_zero_attn=False,\n",
        "                                                    batch_first=True,\n",
        "                                                    device=DEVICE\n",
        "        )\n",
        "\n",
        "        self.u_k = nn.Linear(self.dim_context, self.dim_model, bias = False)\n",
        "        self.w1_k = nn.Linear(self.dim_model, 1, bias=False)\n",
        "        self.w2_k = nn.Linear(self.dim_model, 1, bias=False)\n",
        "\n",
        "        self.u_v = nn.Linear(self.dim_context, self.dim_model, bias=False)\n",
        "        self.w1_v = nn.Linear(self.dim_model, 1, bias = False)\n",
        "        self.w2_v = nn.Linear(self.dim_model, 1, bias = False)\n",
        "\n",
        "    def forward(self, q, k, v, context):\n",
        "\n",
        "        # print(\"Context shape : \", context.shape)\n",
        "        # print(\"Dim context : \", self.dim_context, \" : Dim model : \", self.dim_model)\n",
        "        key_context = self.u_k(context)\n",
        "        # print(\"Context shape below key context : \", key_context.shape)\n",
        "        value_context = self.u_v(context)\n",
        "\n",
        "        lambda_k = F.sigmoid(self.w1_k(k) + self.w2_k(key_context))\n",
        "        lambda_v = F.sigmoid(self.w1_v(v) + self.w2_v(value_context))\n",
        "\n",
        "        k_cap = (1-lambda_k) * k + (lambda_k) * key_context\n",
        "        v_cap = (1-lambda_v) * v + (lambda_v) * value_context\n",
        "\n",
        "        attention_output, _ = self.attention_layer(query = q,\n",
        "                                                   key = k_cap,\n",
        "                                                   value = v_cap)\n",
        "\n",
        "        return attention_output\n",
        "\n"
      ],
      "metadata": {
        "id": "lEwhnknKJF1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MAF_main(nn.Module):\n",
        "    def __init__(self,\n",
        "                dim_model,\n",
        "                dropout_rate):\n",
        "        super(MAF_main, self).__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "\n",
        "        self.context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "                                                            dim_context=dim_model,\n",
        "                                                            dropout_rate=dropout_rate)\n",
        "\n",
        "\n",
        "        self.gate = nn.Linear(2*dim_model, dim_model)\n",
        "        self.dropout_layer = nn.Dropout(dropout_rate)\n",
        "        self.final_layer_norm = nn.LayerNorm(dim_model)\n",
        "\n",
        "    def forward(self,\n",
        "                main_input,\n",
        "                context_input):\n",
        "\n",
        "\n",
        "\n",
        "        mixed_out = self.context_attention(q=main_input,\n",
        "                                                    k=main_input,\n",
        "                                                    v=main_input,\n",
        "                                                    context=context_input)\n",
        "\n",
        "        weight_v = F.sigmoid(self.gate(torch.cat([mixed_out, main_input], dim=-1)))\n",
        "\n",
        "        output = self.final_layer_norm(weight_v * mixed_out  + main_input)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "Rf7mCvxMHlkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalClassification(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        inned_dim: int,\n",
        "        num_classes: int,\n",
        "        pooler_dropout: float\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(input_dim, inned_dim)\n",
        "        self.dropout = nn.Dropout(p = pooler_dropout)\n",
        "        self.out_proj = nn.Linear(inned_dim, num_classes)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = torch.tanh(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.out_proj(hidden_states)\n",
        "        return hidden_states"
      ],
      "metadata": {
        "id": "9ND7fKneCW61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalVideo(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MultimodalVideo, self).__init__()\n",
        "    self.bart_model = bart_model\n",
        "\n",
        "    self.acoustic_context_transform = nn.Linear(ACOUSTIC_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "    self.acoustic_dimension_transform = nn.Linear(ACOUSTIC_DIM, 768, bias = False)\n",
        "\n",
        "    self.visual_context_transform = nn.Linear(VISUAL_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "    self.visual_dimension_transform = nn.Linear(VISUAL_DIM, 768, bias = False)\n",
        "\n",
        "\n",
        "    self.maf_one = MAF_main(dim_model=768,\n",
        "                             dropout_rate=0.2)\n",
        "\n",
        "    self.maf_two = MAF_main(dim_model=768,\n",
        "                             dropout_rate=0.2)\n",
        "\n",
        "    self.classification_head = MultimodalClassification(\n",
        "            768,\n",
        "            768,\n",
        "            2,\n",
        "            0.0\n",
        "        )\n",
        "\n",
        "  def forward(self, input_ids, attention_mask, acoustic_input, visual_input, labels):\n",
        "\n",
        "    acoustic_input = acoustic_input.permute(0,2,1)\n",
        "    acoustic_input = self.acoustic_context_transform(acoustic_input.float())\n",
        "    acoustic_input = acoustic_input.permute(0,2,1)\n",
        "    acoustic_input = self.acoustic_dimension_transform(acoustic_input)\n",
        "\n",
        "\n",
        "    visual_input = visual_input.permute(0,2,1)\n",
        "    visual_input = self.visual_context_transform(visual_input.float())\n",
        "    visual_input = visual_input.permute(0,2,1)\n",
        "    visual_input = self.visual_dimension_transform(visual_input)\n",
        "\n",
        "\n",
        "\n",
        "    bart_output = self.bart_model(input_ids, attention_mask)['last_hidden_state']\n",
        "\n",
        "    output_one = self.maf_one(main_input =  visual_input, context_input = bart_output)\n",
        "\n",
        "    output_two = self.maf_two(main_input = output_one, context_input = acoustic_input)\n",
        "\n",
        "    # output_one = self.maf_one(main_input =  visual_input, context_input = acoustic_input)\n",
        "\n",
        "    # output_two = self.maf_two(main_input = output_one, context_input = bart_output)\n",
        "\n",
        "    final_out = output_two[:, -1, :]\n",
        "    # final_out = output_one[:, -1, :]\n",
        "    # final_out = visual_input[:, -1, :]\n",
        "\n",
        "    final_out = self.classification_head(final_out)\n",
        "\n",
        "    loss_fct = CrossEntropyLoss()\n",
        "\n",
        "    loss = loss_fct(final_out.view(-1, 2), labels.view(-1))\n",
        "\n",
        "    temp_dict = {}\n",
        "\n",
        "    temp_dict['logits'] = final_out\n",
        "    temp_dict['loss'] = loss\n",
        "\n",
        "    return temp_dict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ovNsT7dg2vRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultimodalVideo()\n",
        "model"
      ],
      "metadata": {
        "id": "dkrh6tAdJWDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uc9qK9bGJWCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inWKX8K6GV50"
      },
      "outputs": [],
      "source": [
        "foldNum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RunFr5b0Ni1u"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/32/train_audio_fold_'+str(foldNum)+'.p', 'rb') as f:\n",
        "  train_audio_data_utterance1 = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAMm_6j1mHg-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGFk4HCwOo-A"
      },
      "outputs": [],
      "source": [
        "type(train_audio_data_utterance1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sK4GbT8ZOq2W"
      },
      "outputs": [],
      "source": [
        "len(train_audio_data_utterance1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZRAOA88iYB-"
      },
      "outputs": [],
      "source": [
        "train_audio_data_utterance1[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X73Hp5HxOsEl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yM1zXLalOifj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvBQbRyFN5zu"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/32/test_audio_fold_'+str(foldNum)+'.p', 'rb') as f:\n",
        "  test_audio_data_utterance1 = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9m8Fi_VmOVK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZctmFGmNQsMN"
      },
      "outputs": [],
      "source": [
        "len(test_audio_data_utterance1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz2xdgV7PzbN"
      },
      "outputs": [],
      "source": [
        "len(test_audio_data_utterance1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yw0sc0aaNzOO"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/32/train_video_fold_'+str(foldNum)+'.p', 'rb') as f:\n",
        "  train_image_data_utterance1 = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yT8QM77QvJS"
      },
      "outputs": [],
      "source": [
        "len(train_image_data_utterance1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUEvmMBNOPFE"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/32/test_video_fold_'+str(foldNum)+'.p', 'rb') as f:\n",
        "  test_image_data_utterance1 = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm6Z6Fc0QxSW"
      },
      "outputs": [],
      "source": [
        "len(test_image_data_utterance1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIEMeHJ6RoXS"
      },
      "outputs": [],
      "source": [
        "tp = torch.ones(4,5)\n",
        "tp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8vsmq4FS48j"
      },
      "outputs": [],
      "source": [
        "torch.zeros(5 - tp.shape[0], 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CNdKCeQRrN8"
      },
      "outputs": [],
      "source": [
        "torch.cat([tp, torch.zeros(5 - tp.shape[0], 5)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO9elV8EQAi1"
      },
      "outputs": [],
      "source": [
        "def pad_seq(tensor, dim, max_len):\n",
        "  if max_len > tensor.shape[0] :\n",
        "    return torch.cat([tensor, torch.zeros(max_len - tensor.shape[0], dim)])\n",
        "  else:\n",
        "    return tensor[:max_len]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmBt6xZ1TElw"
      },
      "outputs": [],
      "source": [
        "ACOUSTIC_DIM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cUPe2LxQ04S"
      },
      "outputs": [],
      "source": [
        "train_audio_data_utterance1 = torch.stack([pad_seq(torch.tensor(a, dtype = torch.float),\n",
        "                                                   dim = ACOUSTIC_DIM,\n",
        "                                                   max_len = ACOUSTIC_MAX_LEN)\n",
        "                                                  for a in train_audio_data_utterance1], 0)\n",
        "train_audio_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6-frFIpTzrW"
      },
      "outputs": [],
      "source": [
        "test_audio_data_utterance1 = torch.stack([pad_seq(torch.tensor(a, dtype = torch.float),\n",
        "                                                   dim = ACOUSTIC_DIM,\n",
        "                                                   max_len = ACOUSTIC_MAX_LEN)\n",
        "                                                  for a in test_audio_data_utterance1], 0)\n",
        "test_audio_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEDVn1TaUicC"
      },
      "outputs": [],
      "source": [
        "VISUAL_DIM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ArX7k9VUkWO"
      },
      "outputs": [],
      "source": [
        "VISUAL_MAX_LEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PO2OLh5yUTF2"
      },
      "outputs": [],
      "source": [
        "train_image_data_utterance1 = torch.stack([pad_seq(torch.tensor(a, dtype = torch.float),\n",
        "                                                   dim = VISUAL_DIM,\n",
        "                                                   max_len = VISUAL_MAX_LEN)\n",
        "                                                  for a in train_image_data_utterance1], 0)\n",
        "train_image_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BAAQoCcUv34"
      },
      "outputs": [],
      "source": [
        "test_image_data_utterance1 = torch.stack([pad_seq(torch.tensor(a, dtype = torch.float),\n",
        "                                                   dim = VISUAL_DIM,\n",
        "                                                   max_len = VISUAL_MAX_LEN)\n",
        "                                                  for a in test_image_data_utterance1], 0)\n",
        "test_image_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTMNyPhSUdBs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3V8UO6HH0xE"
      },
      "outputs": [],
      "source": [
        "# path = \"/content/drive/MyDrive/Colab Notebooks/32/datasetTrue_original/sarcasmDataset_speaker_dependent_True.npz\"\n",
        "# data2 = np.load(path, mmap_mode=True)\n",
        "\n",
        "# train_audio_data_utterance1 = data2['feautesUA_train'][foldNum]\n",
        "# train_image_data_utterance1 = data2['feautesUV_train'][foldNum]\n",
        "\n",
        "# test_audio_data_utterance1 = data2['feautesUA_test'][foldNum]\n",
        "# test_image_data_utterance1 = data2['feautesUV_test'][foldNum]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdf5hGlVH5Zw"
      },
      "outputs": [],
      "source": [
        "# model = MultimodalBartForSequenceClassification.from_pretrained(\"facebook/bart-base\")\n",
        "# print(model)\n",
        "\n",
        "# tokenizer = BartTokenizerFast.from_pretrained('facebook/bart-base')\n",
        "# print(\"Tokenizer : \", tokenizer)\n",
        "\n",
        "# num_param = sum(p.numel() for p in model.parameters())\n",
        "# print(\"Total parameters : \", num_param/1e6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_N4HcMQ9hXX"
      },
      "outputs": [],
      "source": [
        "cnt = 0\n",
        "for name, param in model.named_parameters():\n",
        "    print(\"Count : \", cnt, \" name : \", name)\n",
        "    cnt+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feA-7jSwIBQO"
      },
      "outputs": [],
      "source": [
        "cnt = 0\n",
        "for name, param in model.named_parameters():\n",
        "\n",
        "    if(cnt>=259):\n",
        "\n",
        "        param.requires_grad = True\n",
        "        print(\"Count : \", cnt, \" name : \", name)\n",
        "\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "    cnt+=1\n",
        "\n",
        "\n",
        "num_param = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Total trainanable parameters : \", num_param/1e6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amBD8a4_rhUM"
      },
      "outputs": [],
      "source": [
        "foldNum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46OM_4HZjKNp"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/Colab Notebooks/32/json_file_fold.p\", \"rb\") as f:\n",
        "    text_file = pickle.load(f)\n",
        "\n",
        "train_text = text_file[\"json_file_list_\" + str(foldNum) +\"_train\"]\n",
        "test_text =  text_file[\"json_file_list_\" + str(foldNum) +\"_test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaH0ocF_zD0O"
      },
      "outputs": [],
      "source": [
        "# Trainlen = 483"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sL8tkEu3kCrJ"
      },
      "outputs": [],
      "source": [
        "print(type(train_audio_data_utterance1))\n",
        "print(train_audio_data_utterance1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRRQRZeBNYum"
      },
      "outputs": [],
      "source": [
        "# combined_train_data = []\n",
        "# for j in range(len(train_text)):\n",
        "#   temp_text = train_text[j]\n",
        "#   temp_audio = train_audio_data_utterance1[j]\n",
        "#   temp_image = train_image_data_utterance1[j]\n",
        "\n",
        "#   temp_list = []\n",
        "#   temp_list.append(temp_text)\n",
        "#   temp_list.append(temp_audio)\n",
        "#   temp_list.append(temp_image)\n",
        "\n",
        "#   combined_train_data.append(temp_list)\n",
        "\n",
        "# random.shuffle(combined_train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGx2mZfPOWjo"
      },
      "outputs": [],
      "source": [
        "# train_text2 = []\n",
        "# train_audio_utterance2 = []\n",
        "# train_image_utterance2 = []\n",
        "\n",
        "# for j in combined_train_data:\n",
        "#   train_text2.append(j[0])\n",
        "#   train_audio_utterance2.append(j[1])\n",
        "#   train_image_utterance2.append(j[2])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qe5xfQYURWrs"
      },
      "outputs": [],
      "source": [
        "# print(len(train_text2))\n",
        "# print(len(train_audio_utterance2))\n",
        "# print(len(train_image_utterance2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPa_FWppRfpn"
      },
      "outputs": [],
      "source": [
        "# train_audio_data_utterance2 = torch.tensor(train_audio_utterance2)\n",
        "# train_audio_data_utterance2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuHaeA7JiRQz"
      },
      "outputs": [],
      "source": [
        "# train_image_data_utterance2 = torch.tensor(train_image_utterance2)\n",
        "# train_image_data_utterance2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQCW6oZTifhd"
      },
      "outputs": [],
      "source": [
        "# train_audio_data_utterance = torch.tensor(train_audio_data_utterance2)[:Trainlen]\n",
        "# # train_audio_data_utterance = train_audio_data_utterance.unsqueeze(dim = 1)\n",
        "# train_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "984ZSUXVWQfj"
      },
      "outputs": [],
      "source": [
        "train_audio_data_utterance = torch.tensor(train_audio_data_utterance1)\n",
        "# train_audio_data_utterance = train_audio_data_utterance.unsqueeze(dim = 1)\n",
        "train_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYe20QnlkJwg"
      },
      "outputs": [],
      "source": [
        "# train_audio_data_utterance = torch.tensor(train_audio_data_utterance1)[:Trainlen]\n",
        "# # train_audio_data_utterance = train_audio_data_utterance.unsqueeze(dim = 1)\n",
        "# train_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLqtw_6-irl3"
      },
      "outputs": [],
      "source": [
        "# train_image_data_utterance = torch.tensor(train_image_data_utterance2)[:Trainlen]\n",
        "# # train_image_data_utterance = train_image_data_utterance.unsqueeze(dim = 1)\n",
        "# train_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AleXnjQNWTKL"
      },
      "outputs": [],
      "source": [
        "train_image_data_utterance = torch.tensor(train_image_data_utterance1)\n",
        "# train_image_data_utterance = train_image_data_utterance.unsqueeze(dim = 1)\n",
        "train_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1NXg6uikX5P"
      },
      "outputs": [],
      "source": [
        "# train_image_data_utterance = torch.tensor(train_image_data_utterance1)[:Trainlen]\n",
        "# # train_image_data_utterance = train_image_data_utterance.unsqueeze(dim = 1)\n",
        "# train_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkC6buI-iu39"
      },
      "outputs": [],
      "source": [
        "# valid_audio_data_utterance = torch.tensor(train_audio_data_utterance2)[Trainlen:]\n",
        "# valid_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU5wt_vv998E"
      },
      "outputs": [],
      "source": [
        "len(test_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4yP2ap7Pl27"
      },
      "outputs": [],
      "source": [
        "test_image_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5rke66XPvU3"
      },
      "outputs": [],
      "source": [
        "test_audio_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyi5s9iS9jej"
      },
      "outputs": [],
      "source": [
        "# combined_test_data = []\n",
        "\n",
        "# for j in range(len(test_text)):\n",
        "#   temp_text = test_text[j]\n",
        "#   temp_audio = test_audio_data_utterance1[j]\n",
        "#   temp_image = test_image_data_utterance1[j]\n",
        "\n",
        "#   temp_list = []\n",
        "#   temp_list.append(temp_text)\n",
        "#   temp_list.append(temp_audio)\n",
        "#   temp_list.append(temp_image)\n",
        "\n",
        "#   combined_test_data.append(temp_list)\n",
        "\n",
        "# random.shuffle(combined_test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvgGLs8S-cW9"
      },
      "outputs": [],
      "source": [
        "# test_text2 = []\n",
        "# test_audio_utterance2 = []\n",
        "# test_image_utterance2 = []\n",
        "\n",
        "# for j in combined_test_data:\n",
        "#   test_text2.append(j[0])\n",
        "#   test_audio_utterance2.append(j[1])\n",
        "#   test_image_utterance2.append(j[2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgCracq6--vg"
      },
      "outputs": [],
      "source": [
        "# VALIDLEN = 69"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UarPmcdo_uaU"
      },
      "outputs": [],
      "source": [
        "# len(test_audio_utterance2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgOQVaRV_-iV"
      },
      "outputs": [],
      "source": [
        "# test_audio_utterance2[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsLJeYkvAdoK"
      },
      "outputs": [],
      "source": [
        "# torch.stack(test_audio_utterance2).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDznHLJbzAbl"
      },
      "outputs": [],
      "source": [
        "# valid_audio_data_utterance = test_audio_data_utterance1[:VALIDLEN]\n",
        "# valid_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XNjZNpBAYd_"
      },
      "outputs": [],
      "source": [
        "# test_audio_data_utterance = test_audio_data_utterance1[VALIDLEN:]\n",
        "# test_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-kqcb9tiyB8"
      },
      "outputs": [],
      "source": [
        "# valid_image_data_utterance = torch.tensor(train_image_data_utterance2)[Trainlen:]\n",
        "# valid_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8KNyWBm2Ie4"
      },
      "outputs": [],
      "source": [
        "# valid_image_data_utterance = test_image_data_utterance1[:VALIDLEN]\n",
        "# valid_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5rPLXq6At1F"
      },
      "outputs": [],
      "source": [
        "# test_image_data_utterance = test_image_data_utterance1[VALIDLEN:]\n",
        "# test_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCxFywiVko_v"
      },
      "outputs": [],
      "source": [
        "# test_audio_data_utterance = torch.tensor(test_audio_data_utterance1)\n",
        "# # test_audio_data_utterance = test_audio_data_utterance.unsqueeze(dim = 1)\n",
        "# test_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxqP1M_2k2Y_"
      },
      "outputs": [],
      "source": [
        "# test_image_data_utterance = torch.tensor(test_image_data_utterance1)\n",
        "# # test_image_data_utterance = test_image_data_utterance.unsqueeze(dim = 1)\n",
        "# test_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKt4UobTIJQY"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # print(len(text_file[train_text]))\n",
        "# # print(train_text)\n",
        "# # print(len(text_file[test_text]))\n",
        "\n",
        "# train_audio_broadcast_utterance = audio_video_broadcast(train_audio_data_utterance)\n",
        "\n",
        "# print(\"train_audio_broadcast_utterance complete : \", train_audio_broadcast_utterance.shape)\n",
        "# train_image_broadcast_utterance = audio_video_broadcast(train_image_data_utterance)\n",
        "# print(\"train_image_broadcast_utterance complete : \",train_image_broadcast_utterance.shape)\n",
        "\n",
        "# valid_audio_broadcast_utterance = audio_video_broadcast(valid_audio_data_utterance)\n",
        "# print(\"valid_audio_broadcast_utterance complete : \", valid_audio_broadcast_utterance.shape)\n",
        "\n",
        "# valid_image_broadcast_utterance = audio_video_broadcast(valid_image_data_utterance)\n",
        "# print('valid_image_broadcast_utterance complete : ', valid_image_broadcast_utterance.shape)\n",
        "\n",
        "# test_audio_broadcast_utterance = audio_video_broadcast(test_audio_data_utterance)\n",
        "# print(\"test_audio_broadcast_utterance complete : \",test_audio_broadcast_utterance.shape)\n",
        "# test_image_broadcast_utterance = audio_video_broadcast(test_image_data_utterance)\n",
        "# print(\"test_image_broadcast_utterance complete : \",test_image_broadcast_utterance.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ik_vBo7x6Rm"
      },
      "outputs": [],
      "source": [
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0y5DYklxuUT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1Ctmwokx-T2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7mKmBRp2XqP"
      },
      "outputs": [],
      "source": [
        "# def prepare_dataset_context(text_data):\n",
        "\n",
        "\n",
        "#             context = []\n",
        "#             # labels = []\n",
        "#             for i in range(len(text_data)):\n",
        "#                 data_point = text_data[i]\n",
        "\n",
        "#                 # example_speaker = data_point['speaker']\n",
        "#                 # example_utterance = data_point['utterance']\n",
        "#                 # temp_label = int(data_point['sarcasm'])\n",
        "\n",
        "#                 # example_context = '[CONTEXT] '\n",
        "#                 example_context = ''\n",
        "\n",
        "#                 temp_len = len(data_point['context_speakers'])\n",
        "#                 cnt = 0\n",
        "#                 print(\"Temp len : \", temp_len)\n",
        "#                 for speaker, utterance in list(zip(data_point['context_speakers'], data_point['context'])):\n",
        "#                     print(\"count : \", cnt)\n",
        "#                     if(cnt == temp_len - 1):\n",
        "#                       example_context = example_context + speaker.upper() + \" : \" + utterance\n",
        "#                     else:\n",
        "#                       example_context = example_context + speaker.upper() + \" : \" + utterance + \" , \"\n",
        "#                     cnt+=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#                 # print(example_dialog)\n",
        "#                 example_context = re.sub(' +', ' ', example_context)\n",
        "\n",
        "#                 context.append(example_context)\n",
        "#                 # labels.append(temp_label)\n",
        "\n",
        "#             # df = pd.DataFrame(dialog, columns=['dialog'])\n",
        "\n",
        "#             # labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "#             enc = bert_tokenizer(context, max_length = SOURCE_MAX_LEN, padding = 'max_length', truncation = True)\n",
        "\n",
        "#             # df['audio_features'] = acoustic_data\n",
        "#             # df['visual_features'] = visual_data\n",
        "\n",
        "#             return torch.tensor(enc['input_ids'], dtype=torch.long), torch.tensor(enc['attention_mask'], dtype=torch.bool)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiN-BMSgISy7"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(text_data):\n",
        "\n",
        "\n",
        "            dialog = []\n",
        "            labels = []\n",
        "            for i in range(len(text_data)):\n",
        "                data_point = text_data[i]\n",
        "\n",
        "                example_speaker = data_point['speaker']\n",
        "                example_utterance = data_point['utterance']\n",
        "                temp_label = int(data_point['sarcasm'])\n",
        "\n",
        "                # example_dialog = '[CONTEXT] '\n",
        "                # example_dialog = '[TARGET] '\n",
        "                example_dialog = '[CONTEXT] '\n",
        "\n",
        "\n",
        "                for speaker, utterance in list(zip(data_point['context_speakers'], data_point['context'])):\n",
        "                    example_dialog = example_dialog + speaker.upper() + \" : \" + utterance + \" | \"\n",
        "\n",
        "                example_dialog = example_dialog + ' [UTTERANCE] ' + example_speaker + \" : \" + example_utterance + \" | \"\n",
        "                # example_dialog = example_dialog + example_speaker + \" : \" + example_utterance\n",
        "                # example_dialog = example_dialog + example_speaker + \" : \" + example_utterance\n",
        "                # print(example_dialog)\n",
        "                example_dialog = re.sub(' +', ' ', example_dialog)\n",
        "\n",
        "                dialog.append(example_dialog)\n",
        "                labels.append(temp_label)\n",
        "\n",
        "            # df = pd.DataFrame(dialog, columns=['dialog'])\n",
        "\n",
        "            labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "            enc = tokenizer(dialog, max_length = SOURCE_MAX_LEN, padding = 'max_length', truncation = True)\n",
        "\n",
        "            # df['audio_features'] = acoustic_data\n",
        "            # df['visual_features'] = visual_data\n",
        "\n",
        "            return torch.tensor(enc['input_ids'], dtype=torch.long), torch.tensor(enc['attention_mask'], dtype=torch.bool), labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2dU7MfiZaDq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7VlrC_WI9ey"
      },
      "outputs": [],
      "source": [
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgoPnVD4qfO_"
      },
      "outputs": [],
      "source": [
        "len(train_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7p229zVGBUG"
      },
      "outputs": [],
      "source": [
        "train_text_input_ids1, train_text_attention_mask1, train_ground_truth1 = prepare_dataset(train_text)\n",
        "train_ground_truth1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdGsoVBDJxLw"
      },
      "outputs": [],
      "source": [
        "# train_text_input_ids1, train_text_attention_mask1, train_ground_truth1 = prepare_dataset_utterance(train_text)\n",
        "# train_ground_truth1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQdk9Tk8WYzW"
      },
      "outputs": [],
      "source": [
        "train_text_input_ids = train_text_input_ids1\n",
        "train_text_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rqaEYpWZMVx"
      },
      "outputs": [],
      "source": [
        "# train_text_input_ids = train_text_input_ids1[:Trainlen]\n",
        "# train_text_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIYgUkhfWcL8"
      },
      "outputs": [],
      "source": [
        "train_text_attention_mask = train_text_attention_mask1\n",
        "train_text_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRRAuyVaaOun"
      },
      "outputs": [],
      "source": [
        "# train_text_attention_mask = train_text_attention_mask1[:Trainlen]\n",
        "# train_text_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q819IVudYtDG"
      },
      "outputs": [],
      "source": [
        "train_ground_truth = train_ground_truth1\n",
        "train_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8iuUm6vWeN-"
      },
      "outputs": [],
      "source": [
        "# train_ground_truth = train_ground_truth1[:Trainlen]\n",
        "# train_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKXPrrh_jR_O"
      },
      "outputs": [],
      "source": [
        "# train_ground_truth = train_ground_truth1[:Trainlen]\n",
        "# train_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAcvsTpeaeyB"
      },
      "outputs": [],
      "source": [
        "# train_ground_truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bctWHziiWqR0"
      },
      "outputs": [],
      "source": [
        "# context_input_ids, context_attention_mask = prepare_dataset_context(train_text)\n",
        "# print(context_input_ids.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScKZf52FX6AD"
      },
      "outputs": [],
      "source": [
        "# context_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0C9PnF9JypC"
      },
      "outputs": [],
      "source": [
        "print(\"TYPE : train_text_input_ids : \", type(train_text_input_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzsrAIieWgvT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# train_context_input_ids = context_input_ids\n",
        "# train_context_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "815UHYi_juEa"
      },
      "outputs": [],
      "source": [
        "\n",
        "# train_context_input_ids = context_input_ids[:Trainlen]\n",
        "# train_context_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPScSL-aWiyC"
      },
      "outputs": [],
      "source": [
        "# train_context_attention_mask = context_attention_mask\n",
        "# train_context_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEYf1vyblLxm"
      },
      "outputs": [],
      "source": [
        "# train_context_attention_mask = context_attention_mask[:Trainlen]\n",
        "# train_context_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZECJ_D6YwSF"
      },
      "outputs": [],
      "source": [
        "# valid_text_input_ids = train_text_input_ids1[Trainlen:]\n",
        "# valid_text_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Iqag88ipliS"
      },
      "outputs": [],
      "source": [
        "\n",
        "# valid_text_attention_mask = train_text_attention_mask1[Trainlen:]\n",
        "# valid_text_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbjcYjztp523"
      },
      "outputs": [],
      "source": [
        "# valid_ground_truth = train_ground_truth1[Trainlen:]\n",
        "# valid_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDIcnT9t-G1l"
      },
      "outputs": [],
      "source": [
        "# valid_text_input_ids, valid_text_attention_mask, valid_ground_truth = prepare_dataset_utterance(valid_text)\n",
        "# valid_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkfN5cn6YYoz"
      },
      "outputs": [],
      "source": [
        "# valid_context_input_ids = context_input_ids[Trainlen:]\n",
        "# valid_context_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ogVRlSWrgJf"
      },
      "outputs": [],
      "source": [
        "# valid_context_attention_mask = context_attention_mask[Trainlen:]\n",
        "# valid_context_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuNLAd5CIWLd"
      },
      "outputs": [],
      "source": [
        "# test_text_input_ids, test_text_attention_mask, test_ground_truth = prepare_dataset_utterance(test_text)\n",
        "# test_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vB39rShGL4K"
      },
      "outputs": [],
      "source": [
        "test_text_input_ids, test_text_attention_mask, test_ground_truth = prepare_dataset(test_text)\n",
        "test_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dIyrYyhGDFH"
      },
      "outputs": [],
      "source": [
        "# valid_id = test_text_input_ids[:VALID_LEN]\n",
        "# valid_id.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmfvmZTEGbiQ"
      },
      "outputs": [],
      "source": [
        "# test_id = test_text_input_ids[VALIDLEN:]\n",
        "# test_id.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Wi2Kyr5Gm52"
      },
      "outputs": [],
      "source": [
        "# valid_mask = test_text_attention_mask[:VALIDLEN]\n",
        "# valid_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7V0wmGiHAB_"
      },
      "outputs": [],
      "source": [
        "# test_mask = test_text_attention_mask[VALIDLEN:]\n",
        "# test_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFZQBCCTHFzt"
      },
      "outputs": [],
      "source": [
        "# valid_truth = test_ground_truth[:VALIDLEN]\n",
        "# valid_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wiu7X_lHWFi"
      },
      "outputs": [],
      "source": [
        "# test_truth = test_ground_truth[VALIDLEN:]\n",
        "# test_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-ovTep1tWmK"
      },
      "outputs": [],
      "source": [
        "# test_context_input_ids, test_context_attention_mask = prepare_dataset_context(test_text)\n",
        "# test_context_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMO8hPfyHkja"
      },
      "outputs": [],
      "source": [
        "# valid_context_id = test_context_input_ids[:VALIDLEN]\n",
        "# valid_context_id.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IG3A9ASzHwW3"
      },
      "outputs": [],
      "source": [
        "# test_context_id = test_context_input_ids[VALIDLEN:]\n",
        "# test_context_id.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1iZshp8tld_"
      },
      "outputs": [],
      "source": [
        "# test_context_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doTkSyhKIMOt"
      },
      "outputs": [],
      "source": [
        "# valid_context_mask = test_context_attention_mask[:VALIDLEN]\n",
        "# valid_context_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGa0EsBVIpzX"
      },
      "outputs": [],
      "source": [
        "# test_context_mask = test_context_attention_mask[VALIDLEN:]\n",
        "# test_context_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0kqUoFDKPkS"
      },
      "outputs": [],
      "source": [
        "# tokenizer.add_tokens(['[CONTEXT]', '[TARGET]'], special_tokens = True)\n",
        "# model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EEHQlvrRM3j"
      },
      "outputs": [],
      "source": [
        "test_audio_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PdHAFkTRsyK"
      },
      "outputs": [],
      "source": [
        "test_image_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FilVGy_RxCw"
      },
      "outputs": [],
      "source": [
        "print(test_text_input_ids.shape)\n",
        "print(test_text_attention_mask.shape)\n",
        "print(test_ground_truth.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Icba89ykR_5g"
      },
      "outputs": [],
      "source": [
        "# print(test_context_input_ids.shape)\n",
        "# print(test_context_attention_mask.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXqJ0fcjSJ4I"
      },
      "outputs": [],
      "source": [
        "# test_input_data = []\n",
        "\n",
        "\n",
        "# for j in range(test_ground_truth.shape[0]):\n",
        "#   temp_list = []\n",
        "#   temp_list.append(test_text_input_ids[j])\n",
        "#   temp_list.append(test_text_attention_mask[j])\n",
        "#   temp_list.append(test_context_input_ids[j])\n",
        "#   temp_list.append(test_context_attention_mask[j])\n",
        "#   temp_list.append(test_audio_data_utterance1[j])\n",
        "#   temp_list.append(test_image_data_utterance1[j])\n",
        "\n",
        "#   test_input_data.append(temp_list)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9swXKn_TQsW"
      },
      "outputs": [],
      "source": [
        "# print(type(test_input_data))\n",
        "# print(len(test_input_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjn1S99ZS_SX"
      },
      "outputs": [],
      "source": [
        "# test_output_data = test_ground_truth.tolist()\n",
        "# print(type(test_output_data))\n",
        "# print(len(test_output_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecXLmWNsT6h-"
      },
      "outputs": [],
      "source": [
        "# X_valid, X_test, Y_valid, Y_test = train_test_split(\n",
        "#     test_input_data, test_output_data, test_size = 0.5, stratify = test_output_data\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsXIaU_jUUwH"
      },
      "outputs": [],
      "source": [
        "# len(X_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6lG18S9UcWx"
      },
      "outputs": [],
      "source": [
        "# len(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfleSSo6UfWk"
      },
      "outputs": [],
      "source": [
        "# len(Y_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vi0Vpd2lUhR7"
      },
      "outputs": [],
      "source": [
        "# len(Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnzHh4kBUk6f"
      },
      "outputs": [],
      "source": [
        "# valid_text_input_ids = []\n",
        "# valid_text_attention_mask = []\n",
        "# valid_context_input_ids = []\n",
        "# valid_context_attention_mask = []\n",
        "\n",
        "# valid_audio_data = []\n",
        "# valid_image_data = []\n",
        "\n",
        "# for j in range(len(X_valid)):\n",
        "#   valid_text_input_ids.append(X_valid[j][0])\n",
        "#   valid_text_attention_mask.append(X_valid[j][1])\n",
        "\n",
        "#   valid_context_input_ids.append(X_valid[j][2])\n",
        "#   valid_context_attention_mask.append(X_valid[j][3])\n",
        "\n",
        "#   valid_audio_data.append(X_valid[j][4])\n",
        "\n",
        "#   valid_image_data.append(X_valid[j][5])\n",
        "\n",
        "# print(len(valid_text_input_ids))\n",
        "# print(len(valid_text_attention_mask))\n",
        "# print(len(valid_context_input_ids))\n",
        "# print(len(valid_context_attention_mask))\n",
        "# print(len(valid_audio_data))\n",
        "# print(len(valid_image_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyqnoPspY06u"
      },
      "outputs": [],
      "source": [
        "# valid_text_input_ids[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoyGWRN9V6C9"
      },
      "outputs": [],
      "source": [
        "# valid_text_input_ids = torch.stack(valid_text_input_ids)\n",
        "# print(valid_text_input_ids.shape)\n",
        "# valid_text_attention_mask = torch.stack(valid_text_attention_mask)\n",
        "# print(valid_text_attention_mask.shape)\n",
        "# valid_context_input_ids = torch.stack(valid_context_input_ids)\n",
        "# print(valid_context_input_ids.shape)\n",
        "# valid_context_attention_mask = torch.stack(valid_context_attention_mask)\n",
        "# print(valid_context_attention_mask.shape)\n",
        "# valid_audio_data = torch.stack(valid_audio_data)\n",
        "# print(valid_audio_data.shape)\n",
        "# valid_image_data = torch.stack(valid_image_data)\n",
        "# print(valid_image_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipNEkx0wV2Wz"
      },
      "outputs": [],
      "source": [
        "# valid_ground_truth = torch.tensor(Y_valid)\n",
        "# valid_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FM6TlZIatoC"
      },
      "outputs": [],
      "source": [
        "# test_text_id = []\n",
        "# test_text_mask = []\n",
        "\n",
        "# test_context_id = []\n",
        "# test_context_mask = []\n",
        "\n",
        "# test_audio_data = []\n",
        "# test_image_data = []\n",
        "\n",
        "# for j in range(len(X_test)):\n",
        "#   test_text_id.append(X_test[j][0])\n",
        "#   test_text_mask.append(X_test[j][1])\n",
        "\n",
        "#   test_context_id.append(X_test[j][2])\n",
        "#   test_context_mask.append(X_test[j][3])\n",
        "\n",
        "#   test_audio_data.append(X_test[j][4])\n",
        "\n",
        "#   test_image_data.append(X_test[j][5])\n",
        "\n",
        "# test_text_id = torch.stack(test_text_id)\n",
        "# print(test_text_id.shape)\n",
        "# test_text_mask = torch.stack(test_text_mask)\n",
        "# print(test_text_mask.shape)\n",
        "# test_context_id = torch.stack(test_context_id)\n",
        "# print(test_context_id.shape)\n",
        "# test_context_mask = torch.stack(test_context_mask)\n",
        "# print(test_context_mask.shape)\n",
        "# test_audio_data = torch.stack(test_audio_data)\n",
        "# print(test_audio_data.shape)\n",
        "# test_image_data = torch.stack(test_image_data)\n",
        "# print(test_image_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiJSqVpFckAT"
      },
      "outputs": [],
      "source": [
        "# test_ground_truth = torch.tensor(Y_test)\n",
        "# print(test_ground_truth.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMN_GJV1Jjps"
      },
      "outputs": [],
      "source": [
        "\n",
        "# tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SY3KgbQUKoFd"
      },
      "outputs": [],
      "source": [
        "tokenizer.all_special_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7uPY9VkLRcr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGASW03jLFtd"
      },
      "outputs": [],
      "source": [
        "tokenizer.all_special_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVqVQYAULBg5"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.all_special_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNcRXEYZLK7w"
      },
      "outputs": [],
      "source": [
        "# train_audio_broadcast_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eP1Y_my0-btZ"
      },
      "outputs": [],
      "source": [
        "# valid_audio_data_utterance = test_audio_data_utterance[:VALID_LEN, :, :]\n",
        "# valid_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4c_xiCD-tn0"
      },
      "outputs": [],
      "source": [
        "# valid_image_data_utterance = test_image_data_utterance[:VALID_LEN, :, :]\n",
        "# valid_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwLEXwtB-6KN"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# test_audio_data_utterance = test_audio_data_utterance[VALID_LEN:, :, :]\n",
        "# test_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fF_ErwW5_EsF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# test_image_data_utterance = test_image_data_utterance[VALID_LEN:, :, :]\n",
        "# test_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGdaP4uxzX05"
      },
      "outputs": [],
      "source": [
        "test_input_data = []\n",
        "\n",
        "\n",
        "for j in range(test_ground_truth.shape[0]):\n",
        "  temp_list = []\n",
        "  temp_list.append(test_text_input_ids[j])\n",
        "  temp_list.append(test_text_attention_mask[j])\n",
        "\n",
        "  temp_list.append(test_audio_data_utterance1[j])\n",
        "  temp_list.append(test_image_data_utterance1[j])\n",
        "\n",
        "  test_input_data.append(temp_list)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcHY-Z2szays"
      },
      "outputs": [],
      "source": [
        "print(type(test_input_data))\n",
        "print(len(test_input_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Vp5_UG4zm_e"
      },
      "outputs": [],
      "source": [
        "test_output_data = test_ground_truth.tolist()\n",
        "print(type(test_output_data))\n",
        "print(len(test_output_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OYmBDf1zpVv"
      },
      "outputs": [],
      "source": [
        "X_valid, X_test, Y_valid, Y_test = train_test_split(\n",
        "    test_input_data, test_output_data, test_size = 0.5, stratify = test_output_data\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ya-gq91pzs55"
      },
      "outputs": [],
      "source": [
        "len(X_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6HposQMzvVI"
      },
      "outputs": [],
      "source": [
        "len(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37krGL8vzxgX"
      },
      "outputs": [],
      "source": [
        "len(Y_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVCKId-SzzO5"
      },
      "outputs": [],
      "source": [
        "len(Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_TuTdsjz2gx"
      },
      "outputs": [],
      "source": [
        "valid_text_input_ids = []\n",
        "valid_text_attention_mask = []\n",
        "valid_context_input_ids = []\n",
        "valid_context_attention_mask = []\n",
        "\n",
        "valid_audio_data = []\n",
        "valid_image_data = []\n",
        "\n",
        "for j in range(len(X_valid)):\n",
        "  valid_text_input_ids.append(X_valid[j][0])\n",
        "  valid_text_attention_mask.append(X_valid[j][1])\n",
        "\n",
        "\n",
        "  valid_audio_data.append(X_valid[j][2])\n",
        "\n",
        "  valid_image_data.append(X_valid[j][3])\n",
        "\n",
        "print(len(valid_text_input_ids))\n",
        "print(len(valid_text_attention_mask))\n",
        "\n",
        "print(len(valid_audio_data))\n",
        "print(len(valid_image_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mU-7c-w7z9fK"
      },
      "outputs": [],
      "source": [
        "valid_text_input_ids[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_RHsBNLz_d8"
      },
      "outputs": [],
      "source": [
        "valid_text_input_ids = torch.stack(valid_text_input_ids)\n",
        "print(valid_text_input_ids.shape)\n",
        "valid_text_attention_mask = torch.stack(valid_text_attention_mask)\n",
        "print(valid_text_attention_mask.shape)\n",
        "\n",
        "valid_audio_data = torch.stack(valid_audio_data)\n",
        "print(valid_audio_data.shape)\n",
        "valid_image_data = torch.stack(valid_image_data)\n",
        "print(valid_image_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctw1YWnX0DrP"
      },
      "outputs": [],
      "source": [
        "valid_ground_truth = torch.tensor(Y_valid)\n",
        "valid_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ogsa3Gqd0FvA"
      },
      "outputs": [],
      "source": [
        "test_text_id = []\n",
        "test_text_mask = []\n",
        "\n",
        "test_context_id = []\n",
        "test_context_mask = []\n",
        "\n",
        "test_audio_data = []\n",
        "test_image_data = []\n",
        "\n",
        "for j in range(len(X_test)):\n",
        "  test_text_id.append(X_test[j][0])\n",
        "  test_text_mask.append(X_test[j][1])\n",
        "\n",
        "\n",
        "  test_audio_data.append(X_test[j][2])\n",
        "\n",
        "  test_image_data.append(X_test[j][3])\n",
        "\n",
        "test_text_id = torch.stack(test_text_id)\n",
        "print(test_text_id.shape)\n",
        "test_text_mask = torch.stack(test_text_mask)\n",
        "print(test_text_mask.shape)\n",
        "\n",
        "test_audio_data = torch.stack(test_audio_data)\n",
        "print(test_audio_data.shape)\n",
        "test_image_data = torch.stack(test_image_data)\n",
        "print(test_image_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uzCN2Pz0NdD"
      },
      "outputs": [],
      "source": [
        "test_ground_truth = torch.tensor(Y_test)\n",
        "print(test_ground_truth.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yazzz3i0OpP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrligkveIb4f"
      },
      "outputs": [],
      "source": [
        "class MultimodalSarcasmDataset(Dataset):\n",
        "    # def __init__(self, utterance_input_ids, utterance_attention_mask, context_input_ids, context_attention_mask, acoustic_data, visual_data, labels):\n",
        "    def __init__(self, utterance_input_ids, utterance_attention_mask, acoustic_data, visual_data, labels):\n",
        "\n",
        "        self.utterance_input_ids = utterance_input_ids\n",
        "        self.utterance_attention_mask = utterance_attention_mask\n",
        "        # self.context_input_ids = context_input_ids\n",
        "        # self.context_attention_mask = context_attention_mask\n",
        "        # self.context_attention_mask\n",
        "        self.acoustic_data = acoustic_data\n",
        "        self.visual_data = visual_data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.utterance_input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # return self.utterance_input_ids[idx], self.utterance_attention_mask[idx], self.context_input_ids[idx], self.context_attention_mask[idx], self.acoustic_data[idx], self.visual_data[idx], self.labels[idx]\n",
        "        return self.utterance_input_ids[idx], self.utterance_attention_mask[idx],  self.acoustic_data[idx], self.visual_data[idx], self.labels[idx]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IhvAU6M_0mh"
      },
      "outputs": [],
      "source": [
        "# train_text_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UJo5LQ3RjCx"
      },
      "outputs": [],
      "source": [
        "# train_context_attention_mask.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oQY9qKjXbPU"
      },
      "outputs": [],
      "source": [
        "# train_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1wRtyIFYS2m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILQZRYToYYDd"
      },
      "outputs": [],
      "source": [
        "# train_text_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3SQyApZYbH7"
      },
      "outputs": [],
      "source": [
        "# train_text_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JFdrEk1Yd-J"
      },
      "outputs": [],
      "source": [
        "# train_context_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYiUBLjTYgdj"
      },
      "outputs": [],
      "source": [
        "# train_context_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRpLc5f7Yiyz"
      },
      "outputs": [],
      "source": [
        "# train_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c8Wsp6wYlcV"
      },
      "outputs": [],
      "source": [
        "# train_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBLinpQcYo_4"
      },
      "outputs": [],
      "source": [
        "# train_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gya_VngQJFlx"
      },
      "outputs": [],
      "source": [
        "# valid_context_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvLOlLI1JJmE"
      },
      "outputs": [],
      "source": [
        "# valid_context_id.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOwejwPbJPwB"
      },
      "outputs": [],
      "source": [
        "# valid_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KcNwn5qJcGa"
      },
      "outputs": [],
      "source": [
        "# test_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_5HoO4YdY0U"
      },
      "outputs": [],
      "source": [
        "# test_context_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXW_gI7m0pLB"
      },
      "outputs": [],
      "source": [
        "test_image_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYWmjXQZ5y2W"
      },
      "outputs": [],
      "source": [
        "test_audio_data.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0p-j9gl10hAx"
      },
      "outputs": [],
      "source": [
        "test_text_id.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5na9PPNa0tak"
      },
      "outputs": [],
      "source": [
        "test_text_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03WRDNsp0zkZ"
      },
      "outputs": [],
      "source": [
        "valid_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fj1vwA8H02Pt"
      },
      "outputs": [],
      "source": [
        "test_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5j_r6ZqtIeD9"
      },
      "outputs": [],
      "source": [
        "# train_loader = DataLoader(MultimodalSarcasmDataset(train_text_input_ids, train_text_attention_mask, train_context_input_ids, train_context_attention_mask,train_audio_broadcast_utterance, train_image_broadcast_utterance, train_ground_truth), batch_size=32, shuffle = True)\n",
        "# valid_loader = DataLoader(MultimodalSarcasmDataset(valid_text_input_ids, valid_text_attention_mask, valid_context_input_ids, valid_context_attention_mask, valid_audio_broadcast_utterance, valid_image_broadcast_utterance, valid_ground_truth), batch_size = 32, shuffle = False)\n",
        "# test_loader = DataLoader(MultimodalSarcasmDataset(test_text_input_ids, test_text_attention_mask, test_context_input_ids, test_context_attention_mask, test_audio_broadcast_utterance, test_image_broadcast_utterance, test_ground_truth), batch_size=32, shuffle = False)\n",
        "\n",
        "# train_loader = DataLoader(MultimodalSarcasmDataset(train_text_input_ids, train_text_attention_mask, train_context_input_ids, train_context_attention_mask, train_audio_data_utterance, train_image_data_utterance, train_ground_truth), batch_size=32, shuffle = True)\n",
        "# # valid_loader = DataLoader(MultimodalSarcasmDataset(valid_text_input_ids, valid_text_attention_mask, valid_context_input_ids, valid_context_attention_mask,   valid_audio_data, valid_image_data, valid_ground_truth), batch_size = 32, shuffle = False)\n",
        "# test_loader = DataLoader(MultimodalSarcasmDataset(test_text_input_ids, test_text_attention_mask, test_context_input_ids, test_context_attention_mask,  test_audio_data_utterance1, test_image_data_utterance1, test_ground_truth), batch_size=32, shuffle = False)\n",
        "\n",
        "train_loader = DataLoader(MultimodalSarcasmDataset(train_text_input_ids, train_text_attention_mask,  train_audio_data_utterance, train_image_data_utterance, train_ground_truth), batch_size=32, shuffle = True)\n",
        "valid_loader = DataLoader(MultimodalSarcasmDataset(valid_text_input_ids, valid_text_attention_mask,    valid_audio_data, valid_image_data, valid_ground_truth), batch_size = 32, shuffle = False)\n",
        "test_loader = DataLoader(MultimodalSarcasmDataset(test_text_id, test_text_mask,   test_audio_data, test_image_data, test_ground_truth), batch_size=32, shuffle = False)\n",
        "\n",
        "\n",
        "print(test_loader)\n",
        "\n",
        "# print(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJYdQp5vwRim"
      },
      "outputs": [],
      "source": [
        "len(train_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYX5DVe_wYtG"
      },
      "outputs": [],
      "source": [
        "len(valid_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPTKQs5uwdsY"
      },
      "outputs": [],
      "source": [
        "len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxfmB8BbZHbj"
      },
      "outputs": [],
      "source": [
        "# base_params_list = []\n",
        "# new_params_list = []\n",
        "# weight_decay = 1e-4\n",
        "# for name, param in model.named_parameters():\n",
        "#         if \"classification_head\" or \"MAF_layer\" in name:\n",
        "#             new_params_list.append(param)\n",
        "#         else:\n",
        "#             base_params_list.append(param)\n",
        "\n",
        "#         optimizer = torch.optim.AdamW(\n",
        "#         [\n",
        "#             {'params': base_params_list,'lr': 5e-6, 'weight_decay': weight_decay},\n",
        "#             {'params': new_params_list,'lr': LEARNING_RATE, 'weight_decay': weight_decay}\n",
        "#         ],\n",
        "#         lr=5e-6,\n",
        "#         weight_decay=weight_decay\n",
        "#     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44TjmFmSIh9D"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1EC_JPPUpq_"
      },
      "outputs": [],
      "source": [
        "# model.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbrSjHApAMiQ"
      },
      "outputs": [],
      "source": [
        "DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TF3U8SkdZJ9P"
      },
      "outputs": [],
      "source": [
        "model = model.to(DEVICE)\n",
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5MxPWWyIotE"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, data_loader):\n",
        "      model.train()\n",
        "      epoch_train_loss = 0.0\n",
        "\n",
        "\n",
        "      for step, batch in enumerate(tqdm(data_loader, desc = 'Training Iteration')):\n",
        "        # for i, t in enumerate(batch):\n",
        "        #     print(\"Inside hello\")\n",
        "        #     print(i, \" : \", type(t))\n",
        "        batch = tuple(t.to(DEVICE) for t in batch)\n",
        "        # input_ids, attention_mask, context_input_ids, context_attention_mask, acoustic_input, visual_input, labels = batch\n",
        "        input_ids, attention_mask,  acoustic_input, visual_input, labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        # print(\"Input ids shape : \", input_ids.shape)\n",
        "        # print(\"Input ids shape : \", input_ids.shape)\n",
        "        outputs = model(input_ids = input_ids,\n",
        "                        attention_mask = attention_mask,\n",
        "                        # context_input_ids = context_input_ids,\n",
        "                        # context_attention_mask = context_attention_mask,\n",
        "                        acoustic_input = acoustic_input,\n",
        "                        visual_input = visual_input,\n",
        "                        labels = labels)\n",
        "\n",
        "        loss = outputs['loss']\n",
        "        epoch_train_loss += loss.item()\n",
        "\n",
        "        # print(\"Batch wise loss : \", epoch_train_loss)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "      print(\"Epoch train loss : \", epoch_train_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0H6Hk2w62xu"
      },
      "outputs": [],
      "source": [
        "def valid_epoch(model, data_loader):\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  gold = []\n",
        "\n",
        "  valid_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for step, batch in enumerate(tqdm(data_loader)):\n",
        "      batch = tuple(t.to(DEVICE) for t in batch)\n",
        "      # input_ids, attention_mask, context_input_ids, context_attention_mask, acoustic_input, visual_input, labels = batch\n",
        "      input_ids, attention_mask,  acoustic_input, visual_input, labels = batch\n",
        "\n",
        "      outputs = model(input_ids = input_ids,\n",
        "                            attention_mask = attention_mask,\n",
        "                            # context_input_ids = context_input_ids,\n",
        "                            # context_attention_mask = context_attention_mask,\n",
        "                            acoustic_input = acoustic_input,\n",
        "                            visual_input = visual_input,\n",
        "                            labels = labels)\n",
        "\n",
        "      logits = outputs['logits']\n",
        "      loss = outputs['loss']\n",
        "\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "\n",
        "\n",
        "      pred = logits.argmax(dim = -1)\n",
        "\n",
        "      predictions.extend(pred.tolist())\n",
        "      gold.extend(labels.tolist())\n",
        "\n",
        "  return valid_loss, predictions, gold\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93nGID9-Imr-"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def test_epoch(model, data_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    gold = []\n",
        "\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(tqdm(data_loader)):\n",
        "            batch = tuple(t.to(DEVICE) for t in batch)\n",
        "            # input_ids, attention_mask, context_input_ids, context_attention_mask, acoustic_input, visual_input, labels = batch\n",
        "            input_ids, attention_mask,  acoustic_input, visual_input, labels = batch\n",
        "\n",
        "            outputs = model(input_ids = input_ids,\n",
        "                            attention_mask = attention_mask,\n",
        "                            # context_input_ids = context_input_ids,\n",
        "                            # context_attention_mask = context_attention_mask,\n",
        "\n",
        "                            acoustic_input = acoustic_input,\n",
        "                            visual_input = visual_input,\n",
        "                            labels = labels)\n",
        "\n",
        "            logits = outputs['logits']\n",
        "\n",
        "            pred = logits.argmax(dim = -1)\n",
        "\n",
        "            predictions.extend(pred.tolist())\n",
        "\n",
        "            gold.extend(labels.tolist())\n",
        "\n",
        "            correct += int((pred == labels).sum())\n",
        "\n",
        "    return correct/len(data_loader.dataset), predictions, gold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5QauBgKBtYS"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "  def __init__(self, patience, min_delta):\n",
        "    self.patience = patience\n",
        "    self.min_delta = min_delta\n",
        "    self.counter = 0\n",
        "    self.min_validation = np.inf\n",
        "\n",
        "  def early_stop(self, valid_loss):\n",
        "    if valid_loss < self.min_validation:\n",
        "      self.min_validation = valid_loss\n",
        "      self.counter = 0\n",
        "    elif valid_loss > (self.min_validation + self.min_delta):\n",
        "      self.counter += 1\n",
        "      if self.counter >= self.patience:\n",
        "        return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dHmyfM2Cc9b"
      },
      "outputs": [],
      "source": [
        "early_stopper = EarlyStopping(patience = 15, min_delta = 0.2)\n",
        "early_stopper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0HB4PIc6ixU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_and_validation(model, train_loader, valid_loader):\n",
        "  # lowest_loss = 1e6\n",
        "  best_f1 = 0.0\n",
        "  for epoch in range(30):\n",
        "    print(\"\\n=============Epoch : \", epoch)\n",
        "    train_epoch(model, train_loader)\n",
        "    valid_loss, valid_pred, valid_gold = valid_epoch(model, valid_loader)\n",
        "\n",
        "    if early_stopper.early_stop(valid_loss):\n",
        "      break\n",
        "\n",
        "    print(\"Length of predictions : \", len(valid_pred))\n",
        "    print(\"Length of gold : \", len(valid_gold))\n",
        "    print(\"Valid loss : \", valid_loss)\n",
        "    print(\"\\n Valid Accuracy : \", accuracy_score(valid_gold, valid_pred))\n",
        "    print(\"\\n Valid Precision : \", precision_score(valid_gold, valid_pred, average = 'weighted'))\n",
        "    print(\"\\n Valid Recall : \", recall_score(valid_gold, valid_pred, average = 'weighted'))\n",
        "    print(\"\\nValid F1 score : \", f1_score(valid_gold, valid_pred, average = 'weighted'))\n",
        "\n",
        "\n",
        "    curr_f1 = f1_score(valid_gold, valid_pred, average = 'weighted')\n",
        "\n",
        "    curr_loss = valid_loss\n",
        "    # if((curr_f1 > best_f1) and (epoch>=4)):\n",
        "    if(curr_f1 > best_f1):\n",
        "    # if(curr_loss < lowest_loss):\n",
        "      best_f1 = curr_f1\n",
        "      # print(\"Valid pred : \", valid_pred)\n",
        "      # print('valid_gold : ', valid_gold)\n",
        "      torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/video_first/best_model_epoch_'+str(epoch)+'_best_f1_'+str(int(best_f1*100))+'_foldNum_'+str(foldNum)+'.pt')\n",
        "\n",
        "      print(\"model saved\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JMzMS-JAYMuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMJL4HbkEBh2"
      },
      "outputs": [],
      "source": [
        "# train_and_validation(model, train_loader, test_loader)\n",
        "train_and_validation(model, train_loader, valid_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3o5p6PC96ZJQ"
      },
      "outputs": [],
      "source": [
        "# best_model_epoch_12_best_f1_72_foldNum_0.pt\n",
        "# best_model_epoch_9_best_f1_69_foldNum_1.pt\n",
        "# best_model_epoch_9_best_f1_79_foldNum_2.pt\n",
        "# best_model_epoch_9_best_f1_73_foldNum_3.pt\n",
        "# best_model_epoch_7_best_f1_75_foldNum_4.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DO0Qsq9ufvhi"
      },
      "outputs": [],
      "source": [
        "# path = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/stratify/best_model_epoch_11_best_f1_78_foldNum_0.pt'\n",
        "\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/video_first/best_model_epoch_8_best_f1_81_foldNum_0.pt'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAZqaFeUAiMW"
      },
      "outputs": [],
      "source": [
        "# PATH_0 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_12_f1_76.pt'\n",
        "# PATH_1 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_13_f1_78_foldNum_1.pt'\n",
        "# PATH_2 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_9_f1_64_foldNum_2.pt'\n",
        "# PATH_3 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_5_f1_73_foldNum_3.pt'\n",
        "# PATH_4 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_4_f1_74_foldNum_4.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU9qavVgEwJ0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# PATH_0 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_6_f1_50_foldNum_0.pt'\n",
        "# PATH_1 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_4_f1_70_foldNum_1.pt'\n",
        "# PATH_2 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_11_f1_63_foldNum_2.pt'\n",
        "# PATH_3 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_4_f1_44_foldNum_3.pt'\n",
        "# PATH_4 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_4_f1_62_foldNum_4.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PMNVLL3BM-Y"
      },
      "outputs": [],
      "source": [
        "foldNum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_J_qDFj5_ie3"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgEpzAt-_mSQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "acc, test_pred, test_gold = test_epoch(model, test_loader)\n",
        "\n",
        "print(acc)\n",
        "\n",
        "print(\"\\nAccuracy : \", accuracy_score(test_gold, test_pred))\n",
        "print(\"\\nPrecision : \", precision_score(test_gold, test_pred, average = 'weighted'))\n",
        "print(\"\\nRecall : \", recall_score(test_gold, test_pred, average = 'weighted'))\n",
        "print(\"\\nF1 score : \", f1_score(test_gold, test_pred, average = 'weighted'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50ctOavcBUOt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTd5vAY4HOp6"
      },
      "outputs": [],
      "source": [
        "# valid_loss, valid_pred, valid_gold = valid_epoch(model, valid_loader)\n",
        "\n",
        "# # print(acc)\n",
        "\n",
        "# print(\"\\nAccuracy : \", accuracy_score(valid_gold, valid_pred))\n",
        "# print(\"\\nPrecision : \", precision_score(valid_gold, valid_pred, average = 'weighted'))\n",
        "# print(\"\\nRecall : \", recall_score(valid_gold, valid_pred, average = 'weighted'))\n",
        "# print(\"\\nF1 score : \", f1_score(valid_gold, valid_pred, average = 'weighted'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eD8Mc-spvSz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3K5movSJPIf"
      },
      "outputs": [],
      "source": [
        "# valid_loss, valid_pred, valid_gold = valid_epoch(model, valid_loader)\n",
        "\n",
        "# # print(acc)\n",
        "\n",
        "# print(\"\\nAccuracy : \", accuracy_score(valid_gold, valid_pred))\n",
        "# print(\"\\nPrecision : \", precision_score(valid_gold, valid_pred))\n",
        "# print(\"\\nRecall : \", recall_score(valid_gold, valid_pred))\n",
        "# print(\"\\nF1 score : \", f1_score(valid_gold, valid_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWNb08EkJPu0"
      },
      "outputs": [],
      "source": [
        "# valid_loss, valid_pred, valid_gold = valid_epoch(model, valid_loader)\n",
        "\n",
        "# # print(acc)\n",
        "\n",
        "# print(\"\\nAccuracy : \", accuracy_score(valid_gold, valid_pred))\n",
        "# print(\"\\nPrecision : \", precision_score(valid_gold, valid_pred, average = 'micro'))\n",
        "# print(\"\\nRecall : \", recall_score(valid_gold, valid_pred, average = 'micro'))\n",
        "# print(\"\\nF1 score : \", f1_score(valid_gold, valid_pred, average = 'micro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ixy_k2L9GgEV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3XQZOkxJ4eP"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# acc, test_pred, test_gold = test_epoch(model, test_loader)\n",
        "\n",
        "# # print(acc)\n",
        "\n",
        "# print(\"\\nAccuracy : \", accuracy_score(test_gold, test_pred))\n",
        "# print(\"\\nPrecision : \", precision_score(test_gold, test_pred))\n",
        "# print(\"\\nRecall : \", recall_score(test_gold, test_pred))\n",
        "# print(\"\\nF1 score : \", f1_score(test_gold, test_pred))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jryD1kV-J5MG"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# acc, test_pred, test_gold = test_epoch(model, test_loader)\n",
        "\n",
        "# # print(acc)\n",
        "\n",
        "# print(\"\\nAccuracy : \", accuracy_score(test_gold, test_pred))\n",
        "# print(\"\\nPrecision : \", precision_score(test_gold, test_pred, average = 'micro'))\n",
        "# print(\"\\nRecall : \", recall_score(test_gold, test_pred, average = 'micro'))\n",
        "# print(\"\\nF1 score : \", f1_score(test_gold, test_pred, average = 'micro'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "993-qFHhZxPp"
      },
      "outputs": [],
      "source": [
        "# test_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPaJ3qk0isrD"
      },
      "outputs": [],
      "source": [
        "# test_gold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94i6uFJtivrG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "gpuClass": "premium"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}