{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "gpuClass": "premium"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mvLg3mQz5lV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "iiHTSMzt2FHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==2.10.0"
      ],
      "metadata": {
        "id": "XKlHxNvt2yr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install wandb\n"
      ],
      "metadata": {
        "id": "XOVp4tVIhj-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "iv0YiT_pjIzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "# sys.path.insert(0,'/content/drive/MyDrive/Colab Notebooks/32/HKT/global_config.py')\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/32/HKT')"
      ],
      "metadata": {
        "id": "kaIz1INbShp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import global_config"
      ],
      "metadata": {
        "id": "di18tQPzTSDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visual_features_list=list(range(55,91))\n",
        "acoustic_features_list=list(range(0,60))\n",
        "\n",
        "ACOUSTIC_DIM = len(acoustic_features_list)\n",
        "VISUAL_DIM = len(visual_features_list)\n",
        "HCF_DIM=4\n",
        "LANGUAGE_DIM=768\n",
        "\n",
        "VISUAL_DIM_ALL = 91\n",
        "ACOUSTIC_DIM_ALL = 81\n",
        "\n",
        "H_MERGE_SENT = 768\n",
        "DATASET_LOCATION = \"./dataset/\"\n",
        "SEP_TOKEN_ID = 3"
      ],
      "metadata": {
        "id": "F9XxkWkL0ewl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import csv\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import pickle\n",
        "import sys\n",
        "# from global_config import *\n",
        "import numpy as np\n",
        "import wandb\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "from torch.nn import CrossEntropyLoss, L1Loss, BCEWithLogitsLoss\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from transformers import (\n",
        "    AlbertConfig,\n",
        "    AlbertTokenizer,\n",
        "    AlbertForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    BertForNextSentencePrediction,\n",
        "    BertTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "# from models import *\n",
        "from transformers.optimization import AdamW\n",
        "import copy"
      ],
      "metadata": {
        "id": "9uj9QMB2RgCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import csv\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import pickle\n",
        "import sys\n",
        "# from global_config import *\n",
        "import math\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from transformers import (\n",
        "    AlbertModel,\n",
        "    AlbertPreTrainedModel,\n",
        "    AlbertConfig,\n",
        "    load_tf_weights_in_albert,\n",
        ")\n",
        "# from transformers.modeling_albert import AlbertEmbeddings, AlbertLayerGroup\n"
      ],
      "metadata": {
        "id": "7KDSWtr01wou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parser = argparse.ArgumentParser()"
      ],
      "metadata": {
        "id": "8fU9z6FIVb0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# parser.add_argument(\n",
        "#     # \"--model\", type=str, choices=[\"HKT\",\"language_only\", \"acoustic_only\", \"visual_only\",\"hcf_only\"], default=\"HKT\",\n",
        "#     \"--model\", type=str,  default=\"HKT\",\n",
        "# )"
      ],
      "metadata": {
        "id": "B2MVnA78VeSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parser.add_argument(\"--dataset\", type=str, choices=[\"sarcasm\"], default=\"sarcasm\")\n",
        "# parser.add_argument(\"--batch_size\", type=int, default=16)\n",
        "# parser.add_argument(\"--max_seq_length\", type=int, default=85)\n",
        "# parser.add_argument(\"--n_layers\", type=int, default=1)\n",
        "# parser.add_argument(\"--n_heads\", type=int, default=1)\n",
        "# parser.add_argument(\"--cross_n_layers\", type=int, default=1)\n",
        "# parser.add_argument(\"--cross_n_heads\", type=int, default=4)\n",
        "# parser.add_argument(\"--fusion_dim\", type=int, default=172)\n",
        "# parser.add_argument(\"--dropout\", type=float, default=0.2366)\n",
        "# parser.add_argument(\"--epochs\", type=int, default=20)\n",
        "\n",
        "# parser.add_argument(\"--seed\", type=int, default=100)\n",
        "\n",
        "# parser.add_argument(\"--learning_rate\", type=float, default=0.000005)\n",
        "# parser.add_argument(\"--learning_rate_a\", type=float, default=0.003)\n",
        "# parser.add_argument(\"--learning_rate_h\", type=float, default=0.0003)\n",
        "# parser.add_argument(\"--learning_rate_v\", type=float, default=0.003)\n",
        "# parser.add_argument(\"--warmup_ratio\", type=float, default=0.07178)\n",
        "# parser.add_argument(\"--save_weight\", type=str, choices=[\"True\",\"False\"], default=\"False\")"
      ],
      "metadata": {
        "id": "CQPjKogIViS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def return_unk():\n",
        "#     return 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# args, unknown = parser.parse_known_args()\n",
        "# # args = parser.parse_args()"
      ],
      "metadata": {
        "id": "admN-C2nRlzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DEVICE"
      ],
      "metadata": {
        "id": "qBHqbC-WlQdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, d_model, num_layers=1, nhead=1, dropout=0.1, dim_feedforward=128, max_seq_length=5000):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.pos_encoder = nn.Embedding(max_seq_length, d_model)\n",
        "        self.encoder = TransformerEncoder(TransformerLayer(d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout), num_layers=num_layers)\n",
        "        self.decoder = nn.Linear(d_model, 1)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, input, attention_mask=None):\n",
        "        seq_length = input.size()[1]\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input.device)\n",
        "        positions_embedding = self.pos_encoder(position_ids).unsqueeze(0).expand(input.size()) # (seq_length, d_model) => (batch_size, seq_length, d_model)\n",
        "        input = input + positions_embedding\n",
        "        input = self.norm(input)\n",
        "        hidden = self.encoder(input, attention_mask=attention_mask)\n",
        "        out = self.decoder(hidden) # (batch_size, seq_len, hidden_dim)\n",
        "        out = (out[:,0,:], out, hidden) # ([CLS] token embedding, full output, last hidden layer)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, hidden_size, nhead=1, dim_feedforward=128, dropout=0.1):\n",
        "        super(TransformerLayer, self).__init__()\n",
        "        self.self_attention = Attention(hidden_size, nhead, dropout)\n",
        "        self.fc = nn.Sequential(nn.Linear(hidden_size, dim_feedforward), nn.ReLU(), nn.Linear(dim_feedforward, hidden_size))\n",
        "        self.norm1 = nn.LayerNorm(hidden_size)\n",
        "        self.norm2 = nn.LayerNorm(hidden_size)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, attention_mask=None):\n",
        "        src_1 = self.self_attention(src, src, attention_mask=attention_mask)\n",
        "        src = src + self.dropout1(src_1)\n",
        "        src = self.norm1(src)\n",
        "        src_2 = self.fc(src)\n",
        "        src = src + self.dropout2(src_2)\n",
        "        src = self.norm2(src)\n",
        "\n",
        "        return src\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, layer, num_layers):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.layers = _get_clones(layer, num_layers)\n",
        "    def forward(self, src, attention_mask=None):\n",
        "        for layer in self.layers:\n",
        "            new_src = layer(src, attention_mask=attention_mask)\n",
        "            src = src + new_src\n",
        "        return src\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size, num_attention_heads, attention_probs_dropout_prob, ctx_dim=None):\n",
        "        super().__init__()\n",
        "        if hidden_size % num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.attention_head_size = int(hidden_size / num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        # visual_dim = 2048\n",
        "        if ctx_dim is None:\n",
        "            ctx_dim = hidden_size\n",
        "        self.query = nn.Linear(hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(ctx_dim, self.all_head_size)\n",
        "        self.value = nn.Linear(ctx_dim, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, context, attention_mask=None):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(context)\n",
        "        mixed_value_layer = self.value(context)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "\n",
        "        # Apply the attention mask is\n",
        "        if attention_mask is not None:\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        return context_layer\n",
        "\n",
        "\n",
        "class CrossAttentionLayer(nn.Module):\n",
        "    def __init__(self, hidden_size, context_size, nhead=1, dropout=0.1):\n",
        "        super(CrossAttentionLayer, self).__init__()\n",
        "        self.src_cross_attention = Attention(hidden_size, nhead, dropout, ctx_dim=context_size)\n",
        "        self.context_cross_attention = Attention(context_size, nhead, dropout, ctx_dim=hidden_size)\n",
        "        self.self_attention = Attention(hidden_size + context_size, nhead, dropout)\n",
        "        self.fc = nn.Sequential(nn.Linear(hidden_size + context_size, hidden_size + context_size), nn.ReLU())\n",
        "        self.norm1 = nn.LayerNorm(hidden_size + context_size)\n",
        "        self.norm2 = nn.LayerNorm(hidden_size + context_size)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, context, attention_mask=None):\n",
        "        new_src = self.src_cross_attention(src, context, attention_mask=attention_mask)\n",
        "        new_context = self.context_cross_attention(context, src, attention_mask=attention_mask)\n",
        "\n",
        "        cross_src = torch.cat((new_src, new_context), dim=2)\n",
        "\n",
        "        cross_src_1 = self.self_attention(cross_src, cross_src, attention_mask)\n",
        "        cross_src = cross_src + self.dropout1(cross_src_1)\n",
        "        cross_src = self.norm1(cross_src)\n",
        "\n",
        "        cross_src_2 = self.fc(cross_src)\n",
        "        cross_src = cross_src + self.dropout2(cross_src_2)\n",
        "        cross_src = self.norm2(cross_src)\n",
        "\n",
        "        return cross_src\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CrossAttentionEncoder(nn.Module):\n",
        "    def __init__(self, layer, num_layers):\n",
        "        super(CrossAttentionEncoder, self).__init__()\n",
        "        self.layers = _get_clones(layer, num_layers)\n",
        "\n",
        "    def forward(self, src, context, attention_mask=None):\n",
        "        src_dim = src.size()[2]\n",
        "        context_dim = context.size()[2]\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(src, context, attention_mask=attention_mask)\n",
        "            new_src = output[:,:,0:src_dim]\n",
        "            new_context = output[:,:,src_dim:src_dim+context_dim]\n",
        "\n",
        "            src = src + new_src\n",
        "            context = context + new_context\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "#this version use multiple layer of cross attention\n",
        "class HKTMultiLayerCrossAttn(nn.Module):\n",
        "\n",
        "    def __init__(self, text_model, visual_model, acoustic_model,hcf_model, args, dropout=0.1,fusion_dim=128):\n",
        "\n",
        "        super(HKTMultiLayerCrossAttn, self).__init__()\n",
        "        self.newly_added_config=args\n",
        "        self.text_model = text_model\n",
        "        self.visual_model = visual_model\n",
        "        self.acoustic_model = acoustic_model\n",
        "        self.hcf_model = hcf_model\n",
        "\n",
        "        L_AV_layer = CrossAttentionLayer((LANGUAGE_DIM+HCF_DIM), ACOUSTIC_DIM+VISUAL_DIM, nhead=args.cross_n_heads, dropout=args.dropout)\n",
        "        self.L_AV = CrossAttentionEncoder(L_AV_layer, args.cross_n_layers)\n",
        "\n",
        "        total_dim = 2 * (LANGUAGE_DIM+HCF_DIM+ ACOUSTIC_DIM + VISUAL_DIM )\n",
        "\n",
        "        self.fc = nn.Sequential(nn.Linear(total_dim, args.fusion_dim),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Dropout(args.dropout),\n",
        "                                nn.Linear(args.fusion_dim, 1))\n",
        "\n",
        "    def get_params(self):\n",
        "\n",
        "        acoustic_params=list(self.acoustic_model.named_parameters())\n",
        "        visual_params=list(self.visual_model.named_parameters())\n",
        "        hcf_params=list(self.hcf_model.named_parameters())\n",
        "\n",
        "        other_params=list(self.text_model.named_parameters())+list(self.L_AV.named_parameters())+list(self.fc.named_parameters())\n",
        "\n",
        "        return acoustic_params,visual_params,hcf_params,other_params\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, visual, acoustic,hcf, attention_mask=None, token_type_ids=None):\n",
        "\n",
        "        (text_output, _) = self.text_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        (_, _, visual_output) = self.visual_model(visual)\n",
        "        (_, _, acoustic_output) = self.acoustic_model(acoustic)\n",
        "        (_, _, hcf_output) = self.hcf_model(hcf)\n",
        "\n",
        "\n",
        "        text_hcf=torch.cat((text_output,hcf_output),dim=2)\n",
        "\n",
        "        # attention mask conversion\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        av_output=torch.cat((acoustic_output,visual_output),dim=2)\n",
        "\n",
        "        noverbal_text=self.L_AV(text_hcf, av_output, attention_mask=extended_attention_mask)\n",
        "\n",
        "        # Extract embeddings\n",
        "        text_embedding = text_hcf[:,0,:] # [CLS] token\n",
        "        visual_embedding = F.max_pool1d(visual_output.permute(0,2,1).contiguous(), visual_output.shape[1]).squeeze(-1)\n",
        "        acoustic_embedding = F.max_pool1d(acoustic_output.permute(0,2,1).contiguous(),acoustic_output.shape[1]).squeeze(-1)\n",
        "        L_AV_embedding = F.max_pool1d(noverbal_text.permute(0,2,1).contiguous(),noverbal_text.shape[1]).squeeze(-1)\n",
        "\n",
        "\n",
        "        #print(weighted_vad_emb.shape)\n",
        "        fusion = (text_embedding, visual_embedding, acoustic_embedding,L_AV_embedding)\n",
        "        fused_hidden = torch.cat(fusion, dim=1)\n",
        "\n",
        "        out = self.fc(fused_hidden)\n",
        "\n",
        "        return (out, fused_hidden)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class HKT(nn.Module):\n",
        "    def __init__(self, text_model, visual_model, acoustic_model,hcf_model, args, dropout=0.1,fusion_dim=128):\n",
        "        super(HKT, self).__init__()\n",
        "\n",
        "        self.newly_added_config=args\n",
        "        self.text_model = text_model\n",
        "        self.visual_model = visual_model\n",
        "        self.acoustic_model = acoustic_model\n",
        "        self.hcf_model = hcf_model\n",
        "\n",
        "        self.L_AV = CrossAttentionLayer(LANGUAGE_DIM+HCF_DIM, ACOUSTIC_DIM+VISUAL_DIM, nhead=args.cross_n_heads, dropout=args.dropout)\n",
        "\n",
        "        total_dim = 2 * (LANGUAGE_DIM+HCF_DIM + ACOUSTIC_DIM + VISUAL_DIM )\n",
        "\n",
        "        self.fc = nn.Sequential(nn.Linear(total_dim, args.fusion_dim),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Dropout(args.dropout),\n",
        "                                nn.Linear(args.fusion_dim, 1))\n",
        "\n",
        "    def get_params(self):\n",
        "\n",
        "        acoustic_params=list(self.acoustic_model.named_parameters())\n",
        "        visual_params=list(self.visual_model.named_parameters())\n",
        "        hcf_params=list(self.hcf_model.named_parameters())\n",
        "\n",
        "        other_params=list(self.text_model.named_parameters())+list(self.L_AV.named_parameters())+list(self.fc.named_parameters())\n",
        "\n",
        "        return acoustic_params,visual_params,hcf_params,other_params\n",
        "\n",
        "    def forward(self, input_ids, visual, acoustic,hcf, attention_mask=None, token_type_ids=None):\n",
        "        # print('input ids shape : ', input_ids.shape)\n",
        "        # print('attention_mask shape : ', attention_mask.shape)\n",
        "        (text_output, _) = self.text_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        # text_output = self.text_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)['last_hidden_state']\n",
        "        (_, _, visual_output) = self.visual_model(visual)\n",
        "        (_, _, acoustic_output) = self.acoustic_model(acoustic)\n",
        "        (_, _, hcf_output) = self.hcf_model(hcf)\n",
        "\n",
        "        # print('self text model : \\n')\n",
        "        # print(self.text_model)\n",
        "        # print()\n",
        "        # print('text output type : ', type(text_output))\n",
        "        # print('text_output shape : ', text_output.shape)\n",
        "        # print('hcf_output shape : ', hcf_output.shape)\n",
        "        text_hcf=torch.cat((text_output,hcf_output),dim=2)\n",
        "\n",
        "        # attention mask conversion\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        av_output=torch.cat((acoustic_output,visual_output),dim=2)\n",
        "\n",
        "        noverbal_text=self.L_AV(text_hcf, av_output, attention_mask=extended_attention_mask)\n",
        "\n",
        "        # Extract embeddings\n",
        "        text_embedding = text_hcf[:,0,:] # [CLS] token\n",
        "        visual_embedding = F.max_pool1d(visual_output.permute(0,2,1).contiguous(), visual_output.shape[1]).squeeze(-1)\n",
        "        acoustic_embedding = F.max_pool1d(acoustic_output.permute(0,2,1).contiguous(),acoustic_output.shape[1]).squeeze(-1)\n",
        "        L_AV_embedding = F.max_pool1d(noverbal_text.permute(0,2,1).contiguous(),noverbal_text.shape[1]).squeeze(-1)\n",
        "\n",
        "\n",
        "        fusion = (text_embedding, visual_embedding, acoustic_embedding,L_AV_embedding)\n",
        "        fused_hidden = torch.cat(fusion, dim=1)\n",
        "\n",
        "        out = self.fc(fused_hidden)\n",
        "\n",
        "        return (out, fused_hidden)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
      ],
      "metadata": {
        "id": "SvIymQxFtpu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class InputFeatures(object):\n",
        "#     \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "#     def __init__(self, input_ids, input_mask, segment_ids, visual, acoustic,hcf,label_id):\n",
        "#         self.input_ids = input_ids\n",
        "#         self.input_mask = input_mask\n",
        "#         self.segment_ids = segment_ids\n",
        "#         self.visual = visual\n",
        "#         self.acoustic = acoustic\n",
        "#         self.hcf = hcf\n",
        "#         self.label_id = label_id\n",
        "\n",
        "# def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "#     \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "#     pop_count = 0\n",
        "#     while True:\n",
        "#         total_length = len(tokens_a) + len(tokens_b)\n",
        "#         if total_length <= max_length:\n",
        "#             break\n",
        "#         if len(tokens_a) == 0:\n",
        "#             tokens_b.pop()\n",
        "#         else:\n",
        "#             pop_count += 1\n",
        "#             tokens_a.pop(0)\n",
        "#     return pop_count\n",
        "\n",
        "# #albert tokenizer split words in to subwords. \"_\" marker helps to find thos sub words\n",
        "# #our acoustic and visual features are aligned on word level. So we just create copy the same\n",
        "# #visual/acoustic vectors that belong to same word.\n",
        "# def get_inversion(tokens, SPIECE_MARKER=\"â–\"):\n",
        "#     inversion_index = -1\n",
        "#     inversions = []\n",
        "#     for token in tokens:\n",
        "#         if SPIECE_MARKER in token:\n",
        "#             inversion_index += 1\n",
        "#         inversions.append(inversion_index)\n",
        "#     return inversions\n",
        "\n",
        "\n",
        "# def convert_humor_to_features(examples, tokenizer, punchline_only=False):\n",
        "#     features = []\n",
        "\n",
        "#     for (ex_index, example) in enumerate(examples):\n",
        "\n",
        "#         #p denotes punchline, c deontes context\n",
        "#         #hid is the utterance unique id. these id's are provided by the authors of urfunny and mustard\n",
        "#         #label is either 1/0 . 1=humor, 0=not humor\n",
        "#         (\n",
        "#             (p_words, p_visual, p_acoustic, p_hcf),\n",
        "#             (c_words, c_visual, c_acoustic, c_hcf),\n",
        "#             hid,\n",
        "#             label\n",
        "#         ) = example\n",
        "\n",
        "#         text_a = \". \".join(c_words)\n",
        "#         text_b = p_words + \".\"\n",
        "#         tokens_a = tokenizer.tokenize(text_a)\n",
        "#         tokens_b = tokenizer.tokenize(text_b)\n",
        "\n",
        "#         inversions_a = get_inversion(tokens_a)\n",
        "#         inversions_b = get_inversion(tokens_b)\n",
        "\n",
        "#         pop_count = _truncate_seq_pair(tokens_a, tokens_b, args.max_seq_length - 3)\n",
        "\n",
        "#         inversions_a = inversions_a[pop_count:]\n",
        "#         inversions_b = inversions_b[: len(tokens_b)]\n",
        "\n",
        "#         visual_a = []\n",
        "#         acoustic_a = []\n",
        "#         hcf_a=[]\n",
        "#         #our acoustic and visual features are aligned on word level. So we just\n",
        "#         #create copy of the same visual/acoustic vectors that belong to same word.\n",
        "#         #because ber tokenizer split word into subwords\n",
        "#         for inv_id in inversions_a:\n",
        "#             visual_a.append(c_visual[inv_id, :])\n",
        "#             acoustic_a.append(c_acoustic[inv_id, :])\n",
        "#             hcf_a.append(c_hcf[inv_id, :])\n",
        "\n",
        "\n",
        "\n",
        "#         visual_a = np.array(visual_a)\n",
        "#         acoustic_a = np.array(acoustic_a)\n",
        "#         hcf_a = np.array(hcf_a)\n",
        "\n",
        "#         visual_b = []\n",
        "#         acoustic_b = []\n",
        "#         hcf_b = []\n",
        "#         for inv_id in inversions_b:\n",
        "#             visual_b.append(p_visual[inv_id, :])\n",
        "#             acoustic_b.append(p_acoustic[inv_id, :])\n",
        "#             hcf_b.append(p_hcf[inv_id, :])\n",
        "\n",
        "#         visual_b = np.array(visual_b)\n",
        "#         acoustic_b = np.array(acoustic_b)\n",
        "#         hcf_b = np.array(hcf_b)\n",
        "\n",
        "#         tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
        "\n",
        "#         acoustic_zero = np.zeros((1, ACOUSTIC_DIM_ALL))\n",
        "#         if len(tokens_a) == 0:\n",
        "#             acoustic = np.concatenate(\n",
        "#                 (acoustic_zero, acoustic_zero, acoustic_b, acoustic_zero)\n",
        "#             )\n",
        "#         else:\n",
        "#             acoustic = np.concatenate(\n",
        "#                 (acoustic_zero, acoustic_a, acoustic_zero, acoustic_b, acoustic_zero)\n",
        "#             )\n",
        "\n",
        "#         visual_zero = np.zeros((1, VISUAL_DIM_ALL))\n",
        "#         if len(tokens_a) == 0:\n",
        "#             visual = np.concatenate((visual_zero, visual_zero, visual_b, visual_zero))\n",
        "#         else:\n",
        "#             visual = np.concatenate(\n",
        "#                 (visual_zero, visual_a, visual_zero, visual_b, visual_zero)\n",
        "#             )\n",
        "\n",
        "\n",
        "#         hcf_zero = np.zeros((1,4))\n",
        "#         if len(tokens_a) == 0:\n",
        "#             hcf = np.concatenate((hcf_zero, hcf_zero, hcf_b, hcf_zero))\n",
        "#         else:\n",
        "#             hcf = np.concatenate(\n",
        "#                 (hcf_zero, hcf_a, hcf_zero, hcf_b, hcf_zero)\n",
        "\n",
        "#             )\n",
        "\n",
        "#         input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "#         segment_ids = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
        "#         input_mask = [1] * len(input_ids)\n",
        "\n",
        "#         acoustic_padding = np.zeros(\n",
        "#             (args.max_seq_length - len(input_ids), acoustic.shape[1])\n",
        "#         )\n",
        "#         acoustic = np.concatenate((acoustic, acoustic_padding))\n",
        "#         #original urfunny acoustic feature dimension is 81.\n",
        "#         #we found many features are highly correllated. so we removed\n",
        "#         #highly correlated feature to reduce dimension\n",
        "#         acoustic=np.take(acoustic, acoustic_features_list,axis=1)\n",
        "\n",
        "#         visual_padding = np.zeros(\n",
        "#             (args.max_seq_length - len(input_ids), visual.shape[1])\n",
        "#         )\n",
        "#         visual = np.concatenate((visual, visual_padding))\n",
        "#         #original urfunny visual feature dimension is more than 300.\n",
        "#         #we only considred the action unit and face shape parameter features\n",
        "#         visual = np.take(visual, visual_features_list,axis=1)\n",
        "\n",
        "\n",
        "#         hcf_padding= np.zeros(\n",
        "#             (args.max_seq_length - len(input_ids), hcf.shape[1])\n",
        "#         )\n",
        "\n",
        "#         hcf = np.concatenate((hcf, hcf_padding))\n",
        "\n",
        "#         padding = [0] * (args.max_seq_length - len(input_ids))\n",
        "\n",
        "#         input_ids += padding\n",
        "#         input_mask += padding\n",
        "#         segment_ids += padding\n",
        "\n",
        "#         assert len(input_ids) == args.max_seq_length\n",
        "#         assert len(input_mask) == args.max_seq_length\n",
        "#         assert len(segment_ids) == args.max_seq_length\n",
        "#         assert acoustic.shape[0] == args.max_seq_length\n",
        "#         assert visual.shape[0] == args.max_seq_length\n",
        "#         assert hcf.shape[0] == args.max_seq_length\n",
        "\n",
        "#         label_id = float(label)\n",
        "\n",
        "\n",
        "#         features.append(\n",
        "#             InputFeatures(\n",
        "#                 input_ids=input_ids,\n",
        "#                 input_mask=input_mask,\n",
        "#                 segment_ids=segment_ids,\n",
        "#                 visual=visual,\n",
        "#                 acoustic=acoustic,\n",
        "#                 hcf=hcf,\n",
        "#                 label_id=label_id,\n",
        "#             )\n",
        "#         )\n",
        "\n",
        "#     return features\n",
        "\n",
        "\n",
        "\n",
        "# def get_appropriate_dataset(data, tokenizer, parition):\n",
        "\n",
        "\n",
        "#     features = convert_humor_to_features(data, tokenizer)\n",
        "#     all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "#     all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "#     all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
        "#     all_visual = torch.tensor([f.visual for f in features], dtype=torch.float)\n",
        "#     all_acoustic = torch.tensor([f.acoustic for f in features], dtype=torch.float)\n",
        "#     hcf = torch.tensor([f.hcf for f in features], dtype=torch.float)\n",
        "#     all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.float)\n",
        "\n",
        "\n",
        "#     dataset = TensorDataset(\n",
        "#         all_input_ids,\n",
        "#         all_visual,\n",
        "#         all_acoustic,\n",
        "#         all_input_mask,\n",
        "#         all_segment_ids,\n",
        "#         hcf,\n",
        "#         all_label_ids,\n",
        "#     )\n",
        "\n",
        "#     return dataset\n",
        "\n",
        "\n",
        "# def set_up_data_loader():\n",
        "#     if args.dataset==\"humor\":\n",
        "#         data_file = \"ur_funny.pkl\"\n",
        "#     elif args.dataset==\"sarcasm\":\n",
        "#         # data_file = \"mustard.pkl\"\n",
        "#         data_file = \"/content/drive/MyDrive/Colab Notebooks/32/HKT/dataset/our_mustard_split_final.p\"\n",
        "#         # data_file = \"/content/drive/MyDrive/Colab Notebooks/32/HKT/dataset/mustard.pkl\"\n",
        "\n",
        "#     with open(\n",
        "#         # os.path.join(DATASET_LOCATION, data_file),\n",
        "#         data_file,\n",
        "#         \"rb\",\n",
        "#     ) as handle:\n",
        "#         all_data = pickle.load(handle)\n",
        "\n",
        "#     train_data = all_data[\"train\"]\n",
        "#     dev_data = all_data[\"dev\"]\n",
        "#     test_data = all_data[\"test\"]\n",
        "\n",
        "#     # tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")\n",
        "\n",
        "#     train_dataset = get_appropriate_dataset(train_data, tokenizer, \"train\")\n",
        "\n",
        "#     dev_dataset = get_appropriate_dataset(dev_data, tokenizer, \"dev\")\n",
        "#     print('Dev dataset : ', dev_dataset)\n",
        "#     test_dataset = get_appropriate_dataset(test_data, tokenizer, \"test\")\n",
        "\n",
        "#     train_dataloader = DataLoader(\n",
        "#         train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=1\n",
        "#     )\n",
        "\n",
        "#     dev_dataloader = DataLoader(\n",
        "#         dev_dataset, batch_size=args.batch_size, shuffle=True, num_workers=1\n",
        "#     )\n",
        "\n",
        "#     test_dataloader = DataLoader(\n",
        "#         test_dataset, batch_size=args.batch_size, shuffle=True, num_workers=1\n",
        "#     )\n",
        "\n",
        "\n",
        "#     return train_dataloader, dev_dataloader, test_dataloader\n",
        "\n",
        "# def train_epoch(model, train_dataloader, optimizer, scheduler, loss_fct):\n",
        "#     model.train()\n",
        "#     tr_loss = 0\n",
        "#     nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "#     for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
        "\n",
        "#         batch = tuple(t.to(DEVICE) for t in batch)\n",
        "#         (\n",
        "#             input_ids,\n",
        "#             visual,\n",
        "#             acoustic,\n",
        "#             input_mask,\n",
        "#             segment_ids,\n",
        "#             hcf,\n",
        "#             label_ids\n",
        "#         ) = batch\n",
        "\n",
        "#         visual = torch.squeeze(visual, 1)\n",
        "#         acoustic = torch.squeeze(acoustic, 1)\n",
        "\n",
        "#         if args.model == \"language_only\":\n",
        "#             outputs = model(\n",
        "#                 input_ids,\n",
        "#                 token_type_ids=segment_ids,\n",
        "#                 attention_mask=input_mask,\n",
        "#                 labels=None,\n",
        "#             )\n",
        "#         elif args.model == \"acoustic_only\":\n",
        "#             outputs = model(\n",
        "#                 acoustic\n",
        "#             )\n",
        "#         elif args.model == \"visual_only\":\n",
        "#             outputs = model(\n",
        "#                 visual\n",
        "#             )\n",
        "#         elif args.model==\"hcf_only\":\n",
        "#             outputs=model(hcf)\n",
        "\n",
        "#         elif args.model==\"HKT\":\n",
        "#             # print('\\nhello worlld\\n')\n",
        "#             # print('input ids shape : ', input_ids.shape)\n",
        "#             # print('visual shape : ', visual.shape)\n",
        "#             # print('hcf shape : ', hcf.shape)\n",
        "#             # print('segment ids shape : ', segment_ids.shape)\n",
        "#             # print('attention mask shape : ', input_mask.shape)\n",
        "#             outputs = model(input_ids, visual, acoustic,hcf, token_type_ids=segment_ids, attention_mask=input_mask,)\n",
        "\n",
        "\n",
        "\n",
        "#         logits = outputs[0]\n",
        "\n",
        "#         loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
        "\n",
        "#         tr_loss += loss.item()\n",
        "#         nb_tr_examples += input_ids.size(0)\n",
        "#         nb_tr_steps += 1\n",
        "\n",
        "#         loss.backward()\n",
        "\n",
        "#         for o_i in range(len(optimizer)):\n",
        "#             optimizer[o_i].step()\n",
        "#             scheduler[o_i].step()\n",
        "\n",
        "#         model.zero_grad()\n",
        "\n",
        "#     return tr_loss/nb_tr_steps\n",
        "\n",
        "\n",
        "\n",
        "# def eval_epoch(model, dev_dataloader, loss_fct):\n",
        "\n",
        "#     model.eval()\n",
        "#     dev_loss = 0\n",
        "#     nb_dev_examples, nb_dev_steps = 0, 0\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for step, batch in enumerate(tqdm(dev_dataloader, desc=\"Iteration\")):\n",
        "#             batch = tuple(t.to(DEVICE) for t in batch)\n",
        "#             (\n",
        "#                 input_ids,\n",
        "#                 visual,\n",
        "#                 acoustic,\n",
        "#                 input_mask,\n",
        "#                 segment_ids,\n",
        "#                 hcf,\n",
        "#                 label_ids\n",
        "#             ) = batch\n",
        "\n",
        "#             visual = torch.squeeze(visual, 1)\n",
        "#             acoustic = torch.squeeze(acoustic, 1)\n",
        "\n",
        "#             if args.model == \"language_only\":\n",
        "#                 outputs = model(\n",
        "#                     input_ids,\n",
        "#                     token_type_ids=segment_ids,\n",
        "#                     attention_mask=input_mask,\n",
        "#                     labels=None,\n",
        "#                 )\n",
        "#             elif args.model == \"acoustic_only\":\n",
        "#                 outputs = model(\n",
        "#                     acoustic\n",
        "#                 )\n",
        "#             elif args.model == \"visual_only\":\n",
        "#                 outputs = model(\n",
        "#                     visual\n",
        "#                 )\n",
        "#             elif args.model==\"hcf_only\":\n",
        "#                 outputs=model(hcf)\n",
        "\n",
        "#             elif args.model==\"HKT\":\n",
        "#                 outputs = model(input_ids, visual, acoustic,hcf, token_type_ids=segment_ids, attention_mask=input_mask,)\n",
        "\n",
        "\n",
        "#             logits = outputs[0]\n",
        "#             loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
        "\n",
        "#             dev_loss += loss.item()\n",
        "#             nb_dev_examples += input_ids.size(0)\n",
        "#             nb_dev_steps += 1\n",
        "\n",
        "#     return dev_loss/nb_dev_steps\n",
        "\n",
        "# def test_epoch(model, test_data_loader, loss_fct):\n",
        "#     \"\"\" Epoch operation in evaluation phase \"\"\"\n",
        "#     model.eval()\n",
        "\n",
        "#     eval_loss = 0.0\n",
        "#     nb_eval_steps = 0\n",
        "#     preds = []\n",
        "#     all_labels = []\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for step, batch in enumerate(tqdm(test_data_loader, desc=\"Iteration\")):\n",
        "\n",
        "#             batch = tuple(t.to(DEVICE) for t in batch)\n",
        "\n",
        "#             (\n",
        "#                 input_ids,\n",
        "#                 visual,\n",
        "#                 acoustic,\n",
        "#                 input_mask,\n",
        "#                 segment_ids,\n",
        "#                 hcf,\n",
        "#                 label_ids\n",
        "#             ) = batch\n",
        "\n",
        "#             visual = torch.squeeze(visual, 1)\n",
        "#             acoustic = torch.squeeze(acoustic, 1)\n",
        "\n",
        "#             if args.model == \"language_only\":\n",
        "#                 outputs = model(\n",
        "#                     input_ids,\n",
        "#                     token_type_ids=segment_ids,\n",
        "#                     attention_mask=input_mask,\n",
        "#                     labels=None,\n",
        "#                 )\n",
        "#             elif args.model == \"acoustic_only\":\n",
        "#                 outputs = model(\n",
        "#                     acoustic\n",
        "#                 )\n",
        "#             elif args.model == \"visual_only\":\n",
        "#                 outputs = model(\n",
        "#                     visual\n",
        "#                 )\n",
        "#             elif args.model==\"hcf_only\":\n",
        "#                 outputs=model(hcf)\n",
        "\n",
        "#             elif args.model==\"HKT\":\n",
        "#                 # print('\\nhello worlld\\n')\n",
        "#                 # print('input ids shape : ', input_ids.shape)\n",
        "#                 # print('visual shape : ', visual.shape)\n",
        "#                 # print('hcf shape : ', hcf.shape)\n",
        "#                 # print('segment ids shape : ', segment_ids.shape)\n",
        "#                 # print('attention mask shape : ', input_mask.shape)\n",
        "#                 outputs = model(input_ids, visual, acoustic,hcf, token_type_ids=segment_ids, attention_mask=input_mask,)\n",
        "\n",
        "\n",
        "#             logits = outputs[0]\n",
        "\n",
        "\n",
        "#             tmp_eval_loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
        "\n",
        "#             eval_loss += tmp_eval_loss.mean().item()\n",
        "#             nb_eval_steps += 1\n",
        "\n",
        "#             logits = torch.sigmoid(logits)\n",
        "\n",
        "#             if len(preds) == 0:\n",
        "#                 preds=logits.detach().cpu().numpy()\n",
        "#                 all_labels=label_ids.detach().cpu().numpy()\n",
        "#             else:\n",
        "#                 preds= np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "#                 all_labels = np.append(\n",
        "#                     all_labels, label_ids.detach().cpu().numpy(), axis=0\n",
        "#                 )\n",
        "\n",
        "\n",
        "\n",
        "#         eval_loss = eval_loss / nb_eval_steps\n",
        "#         preds = np.squeeze(preds)\n",
        "#         all_labels = np.squeeze(all_labels)\n",
        "\n",
        "#     return preds, all_labels, eval_loss\n",
        "\n",
        "\n",
        "\n",
        "# def test_score_model(model, test_data_loader, loss_fct, exclude_zero=False):\n",
        "\n",
        "#     predictions, y_test, test_loss = test_epoch(model, test_data_loader, loss_fct)\n",
        "\n",
        "#     predictions = predictions.round()\n",
        "\n",
        "#     f_score = f1_score(y_test, predictions, average=\"weighted\")\n",
        "#     accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "#     print(\"Accuracy:\", accuracy,\"F score:\", f_score)\n",
        "#     return accuracy, f_score, test_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def train(\n",
        "#     model,\n",
        "#     train_dataloader,\n",
        "#     dev_dataloader,\n",
        "#     test_dataloader,\n",
        "#     optimizer,\n",
        "#     scheduler,\n",
        "#     loss_fct,\n",
        "# ):\n",
        "\n",
        "#     best_valid_loss = 9e+9\n",
        "#     # run_name = str(wandb.run.id)\n",
        "#     run_name = 'colab'\n",
        "#     valid_losses = []\n",
        "\n",
        "#     n_epochs=args.epochs\n",
        "\n",
        "\n",
        "#     for epoch_i in range(n_epochs):\n",
        "\n",
        "#         train_loss = train_epoch(\n",
        "#             model, train_dataloader, optimizer, scheduler, loss_fct\n",
        "#         )\n",
        "#         valid_loss = eval_epoch(model, dev_dataloader, loss_fct)\n",
        "\n",
        "#         print('\\n')\n",
        "#         valid_losses.append(valid_loss)\n",
        "#         print(\n",
        "#             \"\\nepoch:{}, train_loss : {}, valid_loss : {}\".format(\n",
        "#                 epoch_i, train_loss, valid_loss\n",
        "#             )\n",
        "#         )\n",
        "#         print('\\n')\n",
        "\n",
        "#         valid_accuracy, valid_f_score, valid_loss = test_score_model(\n",
        "#             model, dev_dataloader, loss_fct\n",
        "#         )\n",
        "\n",
        "#         print('Valid accuracy : ', valid_accuracy)\n",
        "#         print('Valid F score : ', valid_f_score)\n",
        "#         print('\\n')\n",
        "\n",
        "#         test_accuracy, test_f_score, test_loss = test_score_model(\n",
        "#             model, test_dataloader, loss_fct\n",
        "#         )\n",
        "\n",
        "#         print('Test accuracy : ', test_accuracy)\n",
        "#         print('Test F score : ', test_f_score)\n",
        "#         if(valid_loss <= best_valid_loss):\n",
        "#             best_valid_loss = valid_loss\n",
        "#             best_valid_test_accuracy = test_accuracy\n",
        "#             best_valid_test_fscore= test_f_score\n",
        "#             print(\"\\nBest model\\n\")\n",
        "#             print(\"best  test accuracy : \", best_valid_test_accuracy)\n",
        "#             print('best  test fscore : ', best_valid_test_fscore)\n",
        "#             torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/32/HKT/saved_model/epoch_'+str(epoch_i)+'_f1_'+str(best_valid_test_fscore))\n",
        "#             print(\"\\n\")\n",
        "\n",
        "#             # if(args.save_weight == \"True\"):\n",
        "#             #     print(\"Best model\\n\")\n",
        "#             #     print(\"best valid test accuracy : \", best_valid_test_accuracy)\n",
        "#             #     print('best valid test fscore : ', best_valid_test_fscore)\n",
        "#                 # torch.save(model.state_dict(),'./best_weights/'+run_name+'.pt')\n",
        "\n",
        "\n",
        "\n",
        "#         #we report test_accuracy of the best valid loss (best_valid_test_accuracy)\n",
        "#         # wandb.log(\n",
        "#         #     {\n",
        "#         #         \"train_loss\": train_loss,\n",
        "#         #         \"valid_loss\": valid_loss,\n",
        "#         #         \"test_loss\": test_loss,\n",
        "#         #         \"best_valid_loss\": best_valid_loss,\n",
        "#         #         \"best_valid_test_accuracy\": best_valid_test_accuracy,\n",
        "#         #         \"best_valid_test_fscore\":best_valid_test_fscore\n",
        "#         #     }\n",
        "#         # )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def get_optimizer_scheduler(params,num_training_steps,learning_rate=1e-5):\n",
        "\n",
        "#     no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "#     optimizer_grouped_parameters = [\n",
        "#         {\n",
        "#             \"params\": [\n",
        "#                 p for n, p in params if not any(nd in n for nd in no_decay)\n",
        "#             ],\n",
        "#             \"weight_decay\": 0.01,\n",
        "#         },\n",
        "#         {\n",
        "#             \"params\": [\n",
        "#                 p for n, p in params if any(nd in n for nd in no_decay)\n",
        "#             ],\n",
        "#             \"weight_decay\": 0.0,\n",
        "#         },\n",
        "#     ]\n",
        "\n",
        "#     optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "#     scheduler = get_linear_schedule_with_warmup(\n",
        "#         optimizer,\n",
        "#         num_warmup_steps=int(num_training_steps * args.warmup_ratio),\n",
        "#         num_training_steps=num_training_steps,\n",
        "#     )\n",
        "\n",
        "#     return optimizer,scheduler\n",
        "\n",
        "# def prep_for_training(num_training_steps):\n",
        "\n",
        "\n",
        "#     if args.model == \"language_only\":\n",
        "#         model = AlbertForSequenceClassification.from_pretrained(\n",
        "#             \"albert-base-v2\", num_labels=1\n",
        "#         )\n",
        "#     elif args.model == \"acoustic_only\":\n",
        "#         model = Transformer(ACOUSTIC_DIM, num_layers=args.n_layers, nhead=args.n_heads, dim_feedforward=args.fc_dim)\n",
        "\n",
        "#     elif args.model == \"visual_only\":\n",
        "#         model = Transformer(VISUAL_DIM, num_layers=args.n_layers, nhead=args.n_heads, dim_feedforward=args.fc_dim)\n",
        "\n",
        "#     elif args.model==\"hcf_only\":\n",
        "#         model=Transformer(HCF_DIM, num_layers=args.n_layers, nhead=args.n_heads, dim_feedforward=args.fc_dim)\n",
        "\n",
        "#     elif args.model == \"HKT\" :\n",
        "#         #HKT model has 4 unimodal encoders. But the language one is ALBERT pretrained model. But other enocders are\n",
        "#         #trained from scratch with low level features. We have found that many times most of the the gardients flows to albert encoders only as it\n",
        "#         #already has rich contextual representation. So in the beginning the gradient flows ignores other encoders which are trained from low level features.\n",
        "#         # We found that if we intitalize the weights of the acoustic, visual and hcf encoders of HKT model from the best unimodal models that we already ran for ablation study then\n",
        "#         #the model converege faster. Other wise it takes very long time to converge.\n",
        "#         if args.dataset==\"humor\":\n",
        "#             visual_model = Transformer(VISUAL_DIM, num_layers=7, nhead=3, dim_feedforward= 128)\n",
        "#             visual_model.load_state_dict(torch.load(\"./model_weights/init/humor/humorVisualTransformer.pt\"))\n",
        "#             acoustic_model = Transformer(ACOUSTIC_DIM, num_layers=8, nhead=3, dim_feedforward = 256)\n",
        "#             acoustic_model.load_state_dict(torch.load(\"./model_weights/init/humor/humorAcousticTransformer.pt\"))\n",
        "#             hcf_model = Transformer(HCF_DIM, num_layers=3, nhead=2, dim_feedforward = 128)\n",
        "#             hcf_model.load_state_dict(torch.load(\"./model_weights/init/humor/humorHCFTransformer.pt\"))\n",
        "\n",
        "#         elif args.dataset==\"sarcasm\":\n",
        "#             visual_model = Transformer(VISUAL_DIM, num_layers=8, nhead=4, dim_feedforward=1024)\n",
        "#             # visual_model.load_state_dict(torch.load(\"./model_weights/init/sarcasm/sarcasmVisualTransformer.pt\"))\n",
        "#             visual_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/32/HKT/model_weights/init/sarcasm/sarcasmVisualTransformer.pt\"))\n",
        "#             acoustic_model = Transformer(ACOUSTIC_DIM, num_layers=1, nhead=3, dim_feedforward=512)\n",
        "#             # acoustic_model.load_state_dict(torch.load(\"./model_weights/init/sarcasm/sarcasmAcousticTransformer.pt\"))\n",
        "#             acoustic_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/32/HKT/model_weights/init/sarcasm/sarcasmAcousticTransformer.pt\"))\n",
        "#             hcf_model = Transformer(HCF_DIM, num_layers=8, nhead=4, dim_feedforward=128)\n",
        "#             # hcf_model.load_state_dict(torch.load(\"./model_weights/init/sarcasm/sarcasmHCFTransformer.pt\"))\n",
        "#             hcf_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/32/HKT/model_weights/init/sarcasm/sarcasmHCFTransformer.pt\"))\n",
        "\n",
        "#         text_model = AlbertModel.from_pretrained('albert-base-v2')\n",
        "\n",
        "#         # print('Text model\\n')\n",
        "#         # print(text_model)\n",
        "#         # print('\\n')\n",
        "#         num_parameters = sum(p.numel() for p in text_model.parameters())\n",
        "#         print('text model Number of parameters : ', num_parameters/1e6)\n",
        "#         model = HKT(text_model, visual_model, acoustic_model,hcf_model, args)\n",
        "\n",
        "#     else:\n",
        "#         raise ValueError(\"Requested model is not available\")\n",
        "\n",
        "#     model.to(DEVICE)\n",
        "\n",
        "#     loss_fct = BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "#     # Prepare optimizer\n",
        "#     # used different learning rates for different componenets.\n",
        "\n",
        "#     if args.model == \"HKT\" :\n",
        "\n",
        "#         acoustic_params,visual_params,hcf_params,other_params = model.get_params()\n",
        "#         optimizer_o,scheduler_o=get_optimizer_scheduler(other_params,num_training_steps,learning_rate=args.learning_rate)\n",
        "#         optimizer_h,scheduler_h=get_optimizer_scheduler(hcf_params,num_training_steps,learning_rate=args.learning_rate_h)\n",
        "#         optimizer_v,scheduler_v=get_optimizer_scheduler(visual_params,num_training_steps,learning_rate=args.learning_rate_v)\n",
        "#         optimizer_a,scheduler_a=get_optimizer_scheduler(acoustic_params,num_training_steps,learning_rate=args.learning_rate_a)\n",
        "\n",
        "#         optimizers=[optimizer_o,optimizer_h,optimizer_v,optimizer_a]\n",
        "#         schedulers=[scheduler_o,scheduler_h,scheduler_v,scheduler_a]\n",
        "\n",
        "#     else:\n",
        "#         params = list(model.named_parameters())\n",
        "\n",
        "#         optimizer_l, scheduler_l = get_optimizer_scheduler(\n",
        "#             params, num_training_steps, learning_rate=args.learning_rate\n",
        "#         )\n",
        "\n",
        "#         optimizers=[optimizer_l]\n",
        "#         schedulers=[scheduler_l]\n",
        "\n",
        "\n",
        "#     return model, optimizers, schedulers,loss_fct\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def set_random_seed(seed):\n",
        "#     \"\"\"\n",
        "#     This function controls the randomness by setting seed in all the libraries we will use.\n",
        "#     \"\"\"\n",
        "#     random.seed(seed)\n",
        "#     os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "#     np.random.seed(seed)\n",
        "#     torch.manual_seed(seed)\n",
        "#     torch.cuda.manual_seed(seed)\n",
        "#     torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "#     torch.backends.cudnn.benchmark = False\n",
        "#     torch.backends.cudnn.enabled = False\n",
        "#     torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "\n",
        "# def main():\n",
        "\n",
        "#     # wandb.init(project=\"HKT\")\n",
        "#     # wandb.config.update(args)\n",
        "\n",
        "#     if(args.seed == -1):\n",
        "#         seed = random.randint(0, 9999)\n",
        "#         print(\"seed\",seed)\n",
        "#     else:\n",
        "#         seed = args.seed\n",
        "\n",
        "#     # wandb.config.update({\"seed\": seed}, allow_val_change=True)\n",
        "\n",
        "#     set_random_seed(seed)\n",
        "\n",
        "#     train_dataloader,dev_dataloader,test_dataloader=set_up_data_loader()\n",
        "#     print(\"Dataset Loaded: \",args.dataset)\n",
        "#     print('\\n')\n",
        "#     num_training_steps = len(train_dataloader) * args.epochs\n",
        "\n",
        "#     model, optimizers, schedulers, loss_fct = prep_for_training(\n",
        "#         num_training_steps\n",
        "#     )\n",
        "#     print(\"Model Loaded: \",args.model)\n",
        "\n",
        "#     print('train loader length : ', len(train_dataloader.dataset))\n",
        "#     print('dev loader length : ', len(dev_dataloader.dataset))\n",
        "#     print('test loader length : ', len(test_dataloader.dataset))\n",
        "#     train(\n",
        "#         model,\n",
        "#         train_dataloader,\n",
        "#         dev_dataloader,\n",
        "#         test_dataloader,\n",
        "#         optimizers,\n",
        "#         schedulers,\n",
        "#         loss_fct,\n",
        "#     )\n",
        "\n",
        "#     # print(\"Full model : \\n\")\n",
        "#     # print(model)\n",
        "\n",
        "#     # z = torch.load(\"/content/drive/MyDrive/Colab Notebooks/32/HKT/model_weights/best/sarcasm/sarcasmHKT.pt\")\n",
        "#     # print(type(z))\n",
        "#     # for name in z:\n",
        "#     #   print(name)\n",
        "\n",
        "#     # model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/32/HKT/model_weights/best/sarcasm/sarcasmHKT.pt\"))\n",
        "#     # model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/32/HKT/model_weights/best/sarcasm/sarcasmAcousticTransformer.pt\"))\n",
        "#     # print(z)\n",
        "#     # test_accuracy, test_f_score, test_loss = test_score_model(\n",
        "#     #         model, test_dataloader, loss_fct\n",
        "#     #     )\n",
        "\n",
        "#     # test_accuracy, test_f_score, test_loss = test_score_model(\n",
        "#     #         model, train_dataloader, loss_fct\n",
        "#     #     )\n",
        "\n",
        "#     # print('Test accuracy : ', test_accuracy)\n",
        "#     # print('Test F score : ', test_f_score)\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ],
      "metadata": {
        "id": "mNpywOtOUYud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer = AutoTokenizer.from_pretrained('albert-base-v2')\n",
        "# model = AlbertModel.from_pretrained(\"albert-base-v2\")\n"
      ],
      "metadata": {
        "id": "YffYRnGKeOrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model"
      ],
      "metadata": {
        "id": "cX_Q6peNuWZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import argparse\n",
        "# import csv\n",
        "# import logging\n",
        "# import os\n",
        "# import random\n",
        "# import pickle\n",
        "# import sys\n",
        "# from global_config import *\n",
        "# import numpy as np\n",
        "\n",
        "# from sklearn.metrics import classification_report\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "# from sklearn.metrics import precision_recall_fscore_support\n",
        "# from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "# from torch.utils.data.distributed import DistributedSampler\n",
        "# from tqdm import tqdm, trange\n",
        "\n",
        "# from torch.nn import CrossEntropyLoss, L1Loss, BCEWithLogitsLoss\n",
        "# from scipy.stats import pearsonr, spearmanr\n",
        "# from sklearn.metrics import matthews_corrcoef\n",
        "# from transformers import (\n",
        "#     AlbertConfig,\n",
        "#     AlbertTokenizer,\n",
        "#     AlbertForSequenceClassification,\n",
        "#     BertForNextSentencePrediction,\n",
        "#     BertTokenizer,\n",
        "#     get_linear_schedule_with_warmup,\n",
        "# )\n",
        "# # from models import *\n",
        "# from transformers.optimization import AdamW"
      ],
      "metadata": {
        "id": "fzStTsbf_Qdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()"
      ],
      "metadata": {
        "id": "UiM1RGMwCwc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def return_unk():\n",
        "    return 0\n",
        "\n",
        "\n",
        "parser.add_argument(\n",
        "    # \"--model\", type=str, choices=[\"language_only\", \"acoustic_only\", \"visual_only\",\"hcf_only\",\"HKT\"], default=\"HKT\",\n",
        "    \"--model\", type=str,  default=\"HKT\"\n",
        ")"
      ],
      "metadata": {
        "id": "qbFRbJIsCzIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# # parser.add_argument(\"--dataset\", type=str, choices=[\"humor\", \"sarcasm\"], default=\"sarcasm\")#humor=UR-FUNNY, sarcasm=MUsTARD\n",
        "parser.add_argument(\"--dataset\", type=str, choices=[\"sarcasm\"], default=\"sarcasm\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vS9ajhTz_TrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser.add_argument(\"--batch_size\", type=int, default=16)\n",
        "parser.add_argument(\"--max_seq_length\", type=int, default=77)\n",
        "parser.add_argument(\"--max_concept_length\", type=int, default=5)\n",
        "parser.add_argument(\"--n_layers\", type=int, default=1)\n",
        "parser.add_argument(\"--n_heads\", type=int, default=1)\n",
        "parser.add_argument(\"--cross_n_layers\", type=int, default=1)\n",
        "parser.add_argument(\"--cross_n_heads\", type=int, default=2)\n",
        "parser.add_argument(\"--fusion_dim\", type=int, default=172)\n",
        "parser.add_argument(\"--dropout\", type=float, default=0.09379)\n",
        "parser.add_argument(\"--seed\", type=int, default=5149)\n",
        "\n"
      ],
      "metadata": {
        "id": "YyGiJIBLTlzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "args, unknown = parser.parse_known_args()"
      ],
      "metadata": {
        "id": "DG1HtWE6LkN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_train_data = []\n",
        "global_dev_data = []\n",
        "global_test_data = []"
      ],
      "metadata": {
        "id": "1husiv0h9Apa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, visual, acoustic,hcf,label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.visual = visual\n",
        "        self.acoustic = acoustic\n",
        "        self.hcf = hcf\n",
        "        self.label_id = label_id\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "    pop_count = 0\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) == 0:\n",
        "            tokens_b.pop()\n",
        "        else:\n",
        "            pop_count += 1\n",
        "            tokens_a.pop(0)\n",
        "    return pop_count\n",
        "\n",
        "\n",
        "def get_inversion(tokens, SPIECE_MARKER=\"â–\"):\n",
        "    inversion_index = -1\n",
        "    inversions = []\n",
        "    for token in tokens:\n",
        "        if SPIECE_MARKER in token:\n",
        "            inversion_index += 1\n",
        "        inversions.append(inversion_index)\n",
        "    return inversions\n",
        "\n",
        "\n",
        "def convert_humor_to_features(examples, tokenizer, punchline_only=False):\n",
        "    features = []\n",
        "\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "\n",
        "        (\n",
        "            (p_words, p_visual, p_acoustic, p_hcf),\n",
        "            (c_words, c_visual, c_acoustic, c_hcf),\n",
        "            hid,\n",
        "            label\n",
        "        ) = example\n",
        "\n",
        "        text_a = \". \".join(c_words)\n",
        "        text_b = p_words + \".\"\n",
        "        tokens_a = tokenizer.tokenize(text_a)\n",
        "        tokens_b = tokenizer.tokenize(text_b)\n",
        "\n",
        "        inversions_a = get_inversion(tokens_a)\n",
        "        inversions_b = get_inversion(tokens_b)\n",
        "\n",
        "        pop_count = _truncate_seq_pair(tokens_a, tokens_b, args.max_seq_length - 3)\n",
        "\n",
        "        inversions_a = inversions_a[pop_count:]\n",
        "        inversions_b = inversions_b[: len(tokens_b)]\n",
        "\n",
        "        visual_a = []\n",
        "        acoustic_a = []\n",
        "        hcf_a=[]\n",
        "\n",
        "        for inv_id in inversions_a:\n",
        "            visual_a.append(c_visual[inv_id, :])\n",
        "            acoustic_a.append(c_acoustic[inv_id, :])\n",
        "            hcf_a.append(c_hcf[inv_id, :])\n",
        "\n",
        "\n",
        "\n",
        "        visual_a = np.array(visual_a)\n",
        "        acoustic_a = np.array(acoustic_a)\n",
        "        hcf_a = np.array(hcf_a)\n",
        "\n",
        "        visual_b = []\n",
        "        acoustic_b = []\n",
        "        hcf_b = []\n",
        "        for inv_id in inversions_b:\n",
        "            visual_b.append(p_visual[inv_id, :])\n",
        "            acoustic_b.append(p_acoustic[inv_id, :])\n",
        "            hcf_b.append(p_hcf[inv_id, :])\n",
        "\n",
        "        visual_b = np.array(visual_b)\n",
        "        acoustic_b = np.array(acoustic_b)\n",
        "        hcf_b = np.array(hcf_b)\n",
        "\n",
        "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
        "\n",
        "        acoustic_zero = np.zeros((1, ACOUSTIC_DIM_ALL))\n",
        "        if len(tokens_a) == 0:\n",
        "            acoustic = np.concatenate(\n",
        "                (acoustic_zero, acoustic_zero, acoustic_b, acoustic_zero)\n",
        "            )\n",
        "        else:\n",
        "            acoustic = np.concatenate(\n",
        "                (acoustic_zero, acoustic_a, acoustic_zero, acoustic_b, acoustic_zero)\n",
        "            )\n",
        "\n",
        "        visual_zero = np.zeros((1, VISUAL_DIM_ALL))\n",
        "        if len(tokens_a) == 0:\n",
        "            visual = np.concatenate((visual_zero, visual_zero, visual_b, visual_zero))\n",
        "        else:\n",
        "            visual = np.concatenate(\n",
        "                (visual_zero, visual_a, visual_zero, visual_b, visual_zero)\n",
        "            )\n",
        "\n",
        "\n",
        "        hcf_zero = np.zeros((1,4))\n",
        "        if len(tokens_a) == 0:\n",
        "            hcf = np.concatenate((hcf_zero, hcf_zero, hcf_b, hcf_zero))\n",
        "        else:\n",
        "            hcf = np.concatenate(\n",
        "                (hcf_zero, hcf_a, hcf_zero, hcf_b, hcf_zero)\n",
        "\n",
        "            )\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        segment_ids = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        acoustic_padding = np.zeros(\n",
        "            (args.max_seq_length - len(input_ids), acoustic.shape[1])\n",
        "        )\n",
        "        acoustic = np.concatenate((acoustic, acoustic_padding))\n",
        "        acoustic=np.take(acoustic, acoustic_features_list,axis=1)\n",
        "\n",
        "        visual_padding = np.zeros(\n",
        "            (args.max_seq_length - len(input_ids), visual.shape[1])\n",
        "        )\n",
        "        visual = np.concatenate((visual, visual_padding))\n",
        "        visual = np.take(visual, visual_features_list,axis=1)\n",
        "\n",
        "\n",
        "        hcf_padding= np.zeros(\n",
        "            (args.max_seq_length - len(input_ids), hcf.shape[1])\n",
        "        )\n",
        "\n",
        "        hcf = np.concatenate((hcf, hcf_padding))\n",
        "\n",
        "        padding = [0] * (args.max_seq_length - len(input_ids))\n",
        "\n",
        "        input_ids += padding\n",
        "        input_mask += padding\n",
        "        segment_ids += padding\n",
        "\n",
        "        assert len(input_ids) == args.max_seq_length\n",
        "        assert len(input_mask) == args.max_seq_length\n",
        "        assert len(segment_ids) == args.max_seq_length\n",
        "        assert acoustic.shape[0] == args.max_seq_length\n",
        "        assert visual.shape[0] == args.max_seq_length\n",
        "        assert hcf.shape[0] == args.max_seq_length\n",
        "\n",
        "        label_id = float(label)\n",
        "\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                input_ids=input_ids,\n",
        "                input_mask=input_mask,\n",
        "                segment_ids=segment_ids,\n",
        "                visual=visual,\n",
        "                acoustic=acoustic,\n",
        "                hcf=hcf,\n",
        "                label_id=label_id,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "\n",
        "def get_appropriate_dataset(data, tokenizer, parition):\n",
        "\n",
        "\n",
        "    features = convert_humor_to_features(data, tokenizer)\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
        "    all_visual = torch.tensor([f.visual for f in features], dtype=torch.float)\n",
        "    all_acoustic = torch.tensor([f.acoustic for f in features], dtype=torch.float)\n",
        "    hcf = torch.tensor([f.hcf for f in features], dtype=torch.float)\n",
        "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.float)\n",
        "\n",
        "\n",
        "    dataset = TensorDataset(\n",
        "        all_input_ids,\n",
        "        all_visual,\n",
        "        all_acoustic,\n",
        "        all_input_mask,\n",
        "        all_segment_ids,\n",
        "        hcf,\n",
        "        all_label_ids,\n",
        "    )\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def set_up_data_loader():\n",
        "    if args.dataset==\"humor\":\n",
        "        data_file = \"ur_funny.pkl\"\n",
        "    elif args.dataset==\"sarcasm\":\n",
        "        # data_file = \"/content/drive/MyDrive/Colab Notebooks/32/HKT/dataset/mustard.pkl\"\n",
        "        data_file = \"/content/drive/MyDrive/Colab Notebooks/32/HKT/dataset/our_mustard_split_final.p\"\n",
        "        # data_file = \"mustard.pkl\"\n",
        "\n",
        "    with open(\n",
        "        # os.path.join(DATASET_LOCATION, args.dataset, data_file),\n",
        "        data_file,\n",
        "        \"rb\",\n",
        "    ) as handle:\n",
        "        all_data = pickle.load(handle)\n",
        "    train_data = all_data[\"train\"]\n",
        "    dev_data = all_data[\"dev\"]\n",
        "    test_data = all_data[\"test\"]\n",
        "\n",
        "    print('Train data : ', len(train_data))\n",
        "    print('Dev data : ', len(dev_data))\n",
        "    print('Test data : ', len(test_data))\n",
        "\n",
        "    tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
        "\n",
        "    train_dataset = get_appropriate_dataset(train_data, tokenizer, \"train\")\n",
        "    dev_dataset = get_appropriate_dataset(dev_data, tokenizer, \"dev\")\n",
        "    test_dataset = get_appropriate_dataset(test_data, tokenizer, \"test\")\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=1\n",
        "    )\n",
        "\n",
        "    dev_dataloader = DataLoader(\n",
        "        dev_dataset, batch_size=args.batch_size, shuffle=False, num_workers=1\n",
        "    )\n",
        "\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=1\n",
        "    )\n",
        "\n",
        "    return (train_dataloader, dev_dataloader, test_dataloader)\n",
        "\n",
        "\n",
        "\n",
        "def get_model():\n",
        "\n",
        "    if args.model == \"HKT\" :\n",
        "\n",
        "        if args.dataset==\"humor\":\n",
        "            visual_model = Transformer(VISUAL_DIM, num_layers=7, nhead=3, dim_feedforward= 128)\n",
        "            acoustic_model = Transformer(ACOUSTIC_DIM, num_layers=8, nhead=3, dim_feedforward = 256)\n",
        "            hcf_model = Transformer(HCF_DIM, num_layers=3, nhead=2, dim_feedforward = 128)\n",
        "            text_model = AlbertModel.from_pretrained('albert-base-v2')\n",
        "            model = HKT(text_model, visual_model, acoustic_model,hcf_model, args)\n",
        "            model.load_state_dict(torch.load(\"./model_weights/best/humor/humorHKT.pt\"))\n",
        "        elif args.dataset==\"sarcasm\":\n",
        "            visual_model = Transformer(VISUAL_DIM, num_layers=8, nhead=4, dim_feedforward=1024)\n",
        "            acoustic_model = Transformer(ACOUSTIC_DIM, num_layers=1, nhead=3, dim_feedforward=512)\n",
        "            hcf_model = Transformer(HCF_DIM, num_layers=8, nhead=4, dim_feedforward=128)\n",
        "            text_model = AlbertModel.from_pretrained(\"albert-base-v2\")\n",
        "            model = HKT(text_model, visual_model, acoustic_model, hcf_model, args)\n",
        "            # model.load_state_dict(torch.load(\"./model_weights/best/sarcasm/sarcasmHKT.pt\"))\n",
        "            # model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/32/HKT/model_weights/best/sarcasm/sarcasmHKT.pt\"))\n",
        "            model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/32/HKT/saved_model/epoch_7_f1_0.6720807726075505\"))\n",
        "\n",
        "\n",
        "\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def test_epoch(model, data_loader, loss_fct):\n",
        "    \"\"\" Epoch operation in evaluation phase \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(\n",
        "            data_loader, mininterval=2, desc=\"  - (Validation)   \", leave=False\n",
        "        ):\n",
        "            batch = tuple(t.to(DEVICE) for t in batch)\n",
        "\n",
        "            (\n",
        "                input_ids,\n",
        "                visual,\n",
        "                acoustic,\n",
        "                input_mask,\n",
        "                segment_ids,\n",
        "                hcf,\n",
        "                label_ids\n",
        "            ) = batch\n",
        "\n",
        "            visual = torch.squeeze(visual, 1)\n",
        "            acoustic = torch.squeeze(acoustic, 1)\n",
        "\n",
        "\n",
        "            if args.model == \"HKT\":\n",
        "                outputs = model(input_ids, visual, acoustic,hcf, token_type_ids=segment_ids, attention_mask=input_mask,)\n",
        "\n",
        "            logits = outputs[0]\n",
        "\n",
        "\n",
        "            tmp_eval_loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
        "\n",
        "            eval_loss += tmp_eval_loss.mean().item()\n",
        "            nb_eval_steps += 1\n",
        "\n",
        "            logits = torch.sigmoid(logits)\n",
        "\n",
        "\n",
        "            if len(preds) == 0:\n",
        "                preds=logits.detach().cpu().numpy()\n",
        "                all_labels=label_ids.detach().cpu().numpy()\n",
        "            else:\n",
        "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "                all_labels = np.append(\n",
        "                    all_labels, label_ids.detach().cpu().numpy(), axis=0\n",
        "                )\n",
        "\n",
        "        eval_loss = eval_loss / nb_eval_steps\n",
        "\n",
        "        preds = np.squeeze(preds)\n",
        "        all_labels = np.squeeze(all_labels)\n",
        "\n",
        "    return preds, all_labels, eval_loss\n",
        "\n",
        "def test_score_model(model, test_data_loader, loss_fct, exclude_zero=False):\n",
        "\n",
        "    predictions, y_test, test_loss = test_epoch(model, test_data_loader, loss_fct)\n",
        "\n",
        "    predictions = predictions.round()\n",
        "\n",
        "    f_score = f1_score(y_test, predictions, average=\"weighted\")\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "    print(\"Accuracy, F score\", accuracy, f_score)\n",
        "    return accuracy, f_score, y_test, predictions\n",
        "\n",
        "\n",
        "def set_random_seed(seed):\n",
        "    \"\"\"\n",
        "    This function controls the randomness by setting seed in all the libraries we will use.\n",
        "    \"\"\"\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.enabled = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def main():\n",
        "\n",
        "   set_random_seed(args.seed)\n",
        "   (\n",
        "        train_data_loader,\n",
        "        dev_data_loader,\n",
        "        test_data_loader,\n",
        "    ) = set_up_data_loader()\n",
        "\n",
        "   model = get_model()\n",
        "   print(\"loaded\")\n",
        "   loss_fct = BCEWithLogitsLoss()\n",
        "   acc, f_score, test_gold, test_pred = test_score_model(model, test_data_loader, loss_fct)\n",
        "   dev_acc, dev_f_score, dev_gold, dev_pred = test_score_model(model, dev_data_loader, loss_fct)\n",
        "  #  acc, f_score = test_score_model(model, train_data_loader, loss_fct)\n",
        "   print('test accuracy : ', acc)\n",
        "   print('test f_score : ', f_score)\n",
        "\n",
        "   print('dev accuracy : ', dev_acc)\n",
        "   print('dev f score : ', dev_f_score)\n",
        "\n",
        "   return test_gold, test_pred\n",
        "if __name__ == \"__main__\":\n",
        "    test_gold, test_pred = main()\n"
      ],
      "metadata": {
        "id": "P92CnHND5Yne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/Colab Notebooks/32/HKT/dataset/our_mustard_split_final.p\", 'rb') as f:\n",
        "  dataset = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "MNI_Y2OP5yUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "id": "zxuVmA0j7fNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = dataset['test']\n",
        "len(test_dataset)"
      ],
      "metadata": {
        "id": "TgER80eD72Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "Q9wZv15dBvL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(test_gold, test_pred)"
      ],
      "metadata": {
        "id": "h249_xAADHjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(len(test_pred)):\n",
        "  if((test_pred[j]==0) and (test_gold[j]==1)):\n",
        "    print(j, \" : \", test_dataset[j][2])\n"
      ],
      "metadata": {
        "id": "ExDZ_dkV8AiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(len(test_pred)):\n",
        "  if((test_pred[j]==1) and (test_gold[j]==0)):\n",
        "    print(j, \" : \", test_dataset[j][2])"
      ],
      "metadata": {
        "id": "z8eXMNjyCShQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(len(test_pred)):\n",
        "  if((test_pred[j]==0) and (test_gold[j]==0)):\n",
        "    print(j, \" : \", test_dataset[j][2])"
      ],
      "metadata": {
        "id": "JtO8l358DWaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(len(test_pred)):\n",
        "  if((test_pred[j]==1) and (test_gold[j]==1)):\n",
        "    print(j, \" : \", test_dataset[j][2])"
      ],
      "metadata": {
        "id": "sVm3OotUF_cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text = \"Replace me by any text you'd like.\"\n",
        "# encoded_input = tokenizer(text, return_tensors='pt')\n"
      ],
      "metadata": {
        "id": "gV9YSOYH4iDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoded_input.keys()"
      ],
      "metadata": {
        "id": "gMdcX31e4kD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output = model(**encoded_input)"
      ],
      "metadata": {
        "id": "fHo3Hlrn4l3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output.keys()"
      ],
      "metadata": {
        "id": "F1Bo8sar4pR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a,b  = model(input_ids = encoded_input['input_ids'], attention_mask = encoded_input['attention_mask'], token_type_ids = encoded_input['token_type_ids'])"
      ],
      "metadata": {
        "id": "sVxKiyM84qPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a"
      ],
      "metadata": {
        "id": "j0RQRHYt5BzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# b"
      ],
      "metadata": {
        "id": "bXlT3Djb5JVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oceRZyU15Ony"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}